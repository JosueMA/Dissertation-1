\documentclass{article}
\usepackage[left=1.00in, right=1.00in, top=1.00in, bottom=1.00in]{geometry}
\usepackage{amsmath}

\usepackage[american]{babel}
\usepackage{csquotes}
\usepackage[backend=biber, style=apa]{biblatex}
\DeclareLanguageMapping{american}{american-apa}
\bibliography{../references.bib}

\begin{document}
\SweaveOpts{concordance = TRUE, echo = FALSE, align = center, height = 3}

\author{Daniel C. Furr}
\date{\today}
\title{Bayesian approaches to cross-validation for person-explanatory models}
\maketitle

<<>>=
library(reshape2, quietly = TRUE)
library(ggplot2, quietly = TRUE)
library(edstan, quietly = TRUE)
library(xtable)

numword <- function(x, cap = FALSE) {
  words <- c("one", "two", "three", "four", "five", "six", "seven",
             "eight", "nine", "ten", "eleven", "twelve", "thirteen",
             "fourteen", "fifteen", "sixteen", "seventeen", "eighteen",
             "nineteen", "twenty")
  if(x > length(words)) {
    return(x)
  } else if(cap) {
    word <- words[x]
    capped <- paste0(toupper(substr(word, 1, 1)), substr(word, 2, nchar(word)))
    return(capped)
  } else {
    return(words[x])
  }
}

# Function for printing numbers
f <- function(x, digits = 2) {
  formatC(x, digits = digits, big.mark = ",", format = "f")
}

# Function for printing list of numbers in text
and <- function(x) {
  l <- length(x)
  if(l == 1) {
    x
  } else if(l == 2) {
    paste(x, collapse = " and ")
  } else if(l > 2) {
    part <- paste(x[-l], collapse = ", ")
    paste(part, x[l], sep = ", and ")
  }
}

my_theme <- theme_bw() +
  theme(text = element_text(family = "serif"),
        strip.background = element_rect(fill = NA, color = NA, size = .5),
        legend.key = element_rect(fill = NA, color = NA))

startup_vars <- ls()
@


\section{Introduction}

Model comparison using indices motivated by cross-validation requires consideration of how new observations would arise.
For the case of independent observations, new observations come about in a straightforward way, that being simply a new collection of exchangeable units.
For the case of clustered observations, new observations may come from within the existing clusters or instead from within new clusters.
\textcite{spiegelhalter2002bayesian} described this distinction as the focus of the model. A model may be focused on the direct parameters associated with clusters, often referred to as latent variables.
Focus on the direct parameters implies that a new data collection would entail a new sample of units from the existing set of clusters.
Alternatively, the focus of the model may be the hyperparameters for the distribution of the direct parameters. In this case, the implied new data collection involves a new sample of clusters, which of course also provide previously unobserved units.

A researcher may be interested in obtaining estimates of the out-of-sample prediction accuracy, either to assess a single model or to compare several models. The deviance information criterion \parencite[DIC;][]{spiegelhalter2002bayesian}, widely applicable information criterion \parencite[WAIC;][]{watanabe2010asymptotic}, and Pareto-smoothed importance sampling estimates of leave-one-out cross-validation \parencite[PSIS-LOO;][]{vehtari2016pareto} are methods of obtaining such estimates. Each of these depend on evaluating the likelihood given posterior draws from Markov chain Monte Carlo (MCMC) simulation. The usual way of specifying Bayesian models in programs like BUGS, JAGS, or Stan bases the likelihood on the direct parameters, and so the ``default'' implementation of information criteria results in an inference with the focus being on the direct parameters. As such, the estimates of predictive accuracy are for predictions for new units arising from the existing set of clusters.

For assessing predictive accuracy with a focus on the hyperparameters, or in other words for predictions involving a new sample of clusters, it is necessary to obtain cluster-level likelihoods that are \emph{marginal} over the direct parameters. Such likelihoods are common in the frequentist tradition; generalized structural equation models and generalized linear mixed models are usually fit using marginal maximum likelihood estimation, for example. In this chapter, I advocate for obtaining posterior draws of \emph{marginal} likelihoods after the usual MCMC simulation for use with DIC, WAIC, or PSIS-LOO when a prediction inference involving new clusters in needed.


\section{A simple hierarchical Bayesian model}

The posterior for a simple hierarchical Bayesian model is
\begin{equation}
  p(\omega, \psi, \zeta | y) \propto
    \prod_{j=1}^J \prod_{i=1}^I p(y_{ij} | \omega, \zeta_j)
    \prod_{j=1}^J p(\zeta_j | \psi)
  p(\omega, \psi)
,\end{equation}
where $y_{ij} \in y$ is the response for observation $i$ ($i = 1 \ldots I$) in cluster $j$ ($j = 1 \dots J$), $\omega$ is a vector of parameters common across clusters, $\zeta_j$ is a cluster-specific parameter, and $\psi$ is a hyperparameter for the prior distribution of $\zeta_j$. Further, let $\zeta$ represent the vector containing all $\zeta_j$. The likelihood $p(y_{ij} | \omega, \zeta_j)$ is conditional on $\zeta_j$ and therefore does not directly involve $\psi$. The prior distributions include the hierarchical prior for $\zeta$ and the joint prior for $\omega$ and $\psi$, and the latter may be rewritten as $p(\omega) p(\psi)$ if independent priors are assigned.

For a focus on $\{ \omega, \psi \}$, a marginal, cluster-level likelihood may be obtained in order to make predictive inferences regarding data from new clusters. Otherwise, for a focus on $\{ \omega, \zeta \}$, the usual unit-level likelihood may be used for prediction inferences for new data from the original clusters. I will refer to this likelihood as the ``conditional'' likelihood as it conditions on $\zeta$.

Any hierarchical prior distribution may be assumed for $\zeta_j$, and depending on the type of distribution, $\psi$ may be a vector representing multiple parameters associated with the chosen distribution. Further, $\zeta_j$ may be a vector of several cluster-specific parameters. However, the adaptive quadrature method proposed in Section~\ref{sec:adaptive-quadrature} requires a scalar $\zeta_j$ having a normal prior, $\zeta_j \sim \mathrm{N}(0, \psi^2)$, where $\psi$ is a standard deviation. Here, $\zeta_j$ may be thought of as a residual, and then $\omega$ determines the mean structure of $y$ while $\psi$ represents the standard deviation of the residuals. As an aside, the proposed adaptive quadrature method could be generalized to a multivariate normal distribution to accommodate $\zeta_j$ being a vector.


\section{Information criteria for hierarchical Bayesian models}

% 1 IC is based on predictive accuracy of a model
% 2 marginal versus conditional

In this section, the notation of \textcite{vehtari2016practical} regarding out-of-sample pointwise predictive accuracy is modified for the specific case of hierarchical models.
For the conditional approach, predictions are conditional on $\zeta_j$, while for the marginal approach $\zeta_j$ is integrated out to obtain predictions.
The notation of \textcite{vehtari2016practical} implicitly takes the conditional approach, which is made explicit here by the inclusion of the $c$ subscript on the relevant quantities.


\subsection{Conditional forms of out-of-sample predictive accuracy}

\subsubsection{Target quantity for the conditional case}

Inference regarding predictive performance of the model for new data relies on the posterior predictive distribution \parencite{rubin1984bayesianly, marshall2007identifying}. The posterior predictive distribution for a new observation $\tilde y_{i'j}$ is
\begin{equation} \label{eq:3-ppd-new-obs}
  p(\tilde y_{i'j} | y) =
  \iiint
    p(\tilde y_{i'j} | \omega, \zeta_j)
    p(\zeta_j | \omega, \psi, y)
    p(\omega, \psi | y)
  ~\mathrm{d} \omega \mathrm{d} \psi \mathrm{d} \zeta_j
,\end{equation}
where $i'$ is a new unit within a previously existing cluster $j$ and
$p(\tilde y_{i'j} | \omega, \zeta_j)$
is similar to
$p(y_{ij} | \omega, \zeta_j)$
but for unobserved (hypothetical) $\tilde y_{i'j}$.
Implicitly, $p(\tilde y_{i'j} | y)$ in the above also depends on the choice of model, not just the data.
The expected log pointwise predictive density for a new dataset, in which new observations arise from the existing clusters, is
\begin{equation}
  \mathrm{elpd}_c =
  \sum_{j=1}^J \sum_{i=1}^I
  \int
    p_t(\tilde y_{i'j})
      \log p(\tilde y_{i'j} | y)
  ~\mathrm{d} \tilde y_{i'j}
,\end{equation}
where $p_t$ is the true (unknown) data generating distribution.
The $c$ subscript in $\mathrm{elpd}_c$ denotes that it is based on the likelihood conditional on $\zeta_j$.
Information criteria and cross-validation are means of approximating the expected log pointwise predictive density.


\subsubsection{Conditional widely applicable information criteria}

WAIC is a form of Bayesian information criterion that requires only the log pointwise predictive density, which is:
\begin{equation}
  \mathrm{lpd}_c =
  \sum_{j=1}^J \sum_{i=1}^I
  \log \iiint
    p(y_{ij} | \omega, \zeta_j)
    p(\zeta_j | \omega, \psi, y)
    p(\omega, \psi | y)
  ~\mathrm{d}\omega \mathrm{d}\psi \mathrm{d}\zeta_j
.\end{equation}
The $\mathrm{lpd}_c$ is the log of the full data likelihood integrated over the posterior for all parameters.
An estimate for it is obtained from the draws of MCMC simulation as
\begin{equation}
  \widehat{\mathrm{lpd}}_c =
  \sum_{j=1}^J \sum_{i=1}^I \log
  \left (
    \frac{1}{S} \sum_{s=1}^S p(y_{ij} | \omega^s, \zeta_j^s)
  \right )
,\end{equation}
where $\omega^s$ and $\zeta_j^s$ are the $s$-th posterior draw from MCMC simulation, $s = 1 \ldots S$.
The $\widehat{\mathrm{lpd}}_c$ is a naively optimistic, in-sample estimate of $\mathrm{elpd}_c$, given that the same data are used both to fit the model and obtain $\widehat{\mathrm{lpd}}_c$. Similar to many forms of information criteria, WAIC adds a ``penalty'' to a naive estimate like $\widehat{\mathrm{lpd}}_c$ to obtain an estimate of expected fit to new data. This penalty, which approximates the ``optimism'' of $\mathrm{lpd}_c$, may also be referred to as the estimated effective number of parameters. For WAIC, this estimated effective number of parameters is
\begin{equation} \label{eq:3-waic-penalty}
  \hat p_{\mathrm{WAIC},c} =
  \sum_{j=1}^J \sum_{i=1}^I V_{s=1}^S \log p(y_{ij} | \omega^s, \zeta_j^s)
\end{equation}
where $V_{s=1}^S$ represents the sample variance across the $S$ posterior draws. Then the expected log pointwise predictive density for WAIC is
\begin{equation}
  \widehat \mathrm{elpd}_{\mathrm{WAIC},c} =
  \widehat{\mathrm{lpd}}_c - \hat p_{\mathrm{WAIC},c}
.\end{equation}
The value normally reported for WAIC is on the deviance scale,
\begin{equation}
  \mathrm{WAIC}_c =
  -2 \widehat{\mathrm{elpd}}_{\mathrm{WAIC}, c}
.\end{equation}

WAIC is asymptotically equal to Bayesian cross-validation with the deviance as the loss function \parencite[][p. 2]{vehtari2016practical}. It is unreliable when the variance of $\log p(y_{ij} | \omega^s, \zeta_j^s)$ (from Equation~\ref{eq:3-waic-penalty}) exceeds .4 for a given observation \parencite[][p. 11]{vehtari2016practical}.
The penalty in WAIC may be viewed as an approximation to the number of unconstrained parameters \parencite[][p. 1003]{gelman2014understanding}.
An alternative formula for the WAIC effective number of parameters based on the mean of $p(y_{ij} | \omega^s, \zeta_j^s)$ is available, but is less numerically stable \parencite[][p. 1002]{gelman2014understanding}.


\subsubsection{Conditional approximate leave-one-out cross-validation}

The Bayesian leave-one-out expected log pointwise predictive density is
\begin{equation}
  \mathrm{elpd}_{\mathrm{LOO},c} =
  \sum_{j=1}^J \sum_{i=1}^I \log p(y_{ij} | y_{-i,j})
\end{equation}
where $y_{-i,j}$ is all observations except for the $i$-th observation in cluster $j$ and
\begin{equation}
  p(y_{ij} | y_{-i,j}) =
  \iiint
    p(y_{ij} | \omega, \zeta_j)
    p(\zeta_j | \psi, y_{-i,j})
    p(\omega, \psi | y_{-i,j})
  ~\mathrm{d} \omega \mathrm{d} \psi \mathrm{d} \zeta_j
,\end{equation}
which bears resemblance to the posterior predictive distribution for a new response. An estimate of $\mathrm{elpd}_{\mathrm{LOO},c}$ may be obtained from the posterior draws as
\begin{equation} \label{eq:importance-sampling}
  \widehat{\mathrm{elpd}}_{\operatorname{PSIS-LOO},c} =
  \sum_{j=1}^J \sum_{i=1}^I \log
    \left ( \frac
      {\sum_{s=1}^S w_{ij}^s p(y_{ij} | \omega^s, \zeta_j^s)}
      {\sum_{s=1}^S w_{ij}^s}
    \right )
,\end{equation}
where $w_{ij}^s$ is a weight specific to the observation and posterior draw.
\textcite{vehtari2016pareto} introduced Pareto smoothed importance sampling weights, which are calculated separately for every observation as follows: first, raw importance ratios are calculated as
$p(y_{ij} | \omega^s, \zeta_j^s)^{-1}$ for each posterior draw;
second, the generalized Pareto distribution is fit to the 20\% largest raw importance ratios; third, the 20\% largest raw importance ratios are replaced by the expected values of the order statistics of the fitted generalized Pareto distribution; and fourth, the weights are truncated at
$S^\frac{3}{4} \bar w_{ij}$, where $\bar w_{ij}$ is the average of the $S$ smoothed weights.
The calculation of the raw importance weights was developed by \textcite{gelfand1992model}, the truncation of the weights was developed by \textcite{ionides2008truncated}, and the Pareto smoothing was developed by \textcite{vehtari2016pareto}.
On the deviance scale,
\begin{equation}
  \operatorname{PSIS-LOO}_c =
  -2 \widehat{\mathrm{elpd}}_{\operatorname{PSIS-LOO},c}
.\end{equation}
Because $\operatorname{PSIS-LOO}_c$ is calculated from a single fit of a model on one dataset, it may be considered a form of information criterion. Lastly, an estimate of the effective number of parameters associated with PSIS-LOO may be obtained after the fact:
\begin{equation}
  \hat p_{\operatorname{PSIS-LOO},c} =
  \widehat{\mathrm{elpd}}_{\operatorname{PSIS-LOO},c} - \widehat{\mathrm{lpd}}_c
.\end{equation}

PSIS-LOO, like WAIC, is an approximation to $\mathrm{elpd}$ \parencite[][p. 3]{vehtari2016practical} that may be computed from the $\mathrm{lpd}$. PSIS-LOO is more robust than WAIC in cases with weak priors or influential observations \parencite[][p. 2]{vehtari2016practical}, and so it may be used when the requirements related to the penalty term in WAIC are not met. Still, it becomes unreliable when the estimated shape parameter for the generalized Pareto distribution exceeds 1 for a given observation \parencite[][p. 11]{vehtari2016practical}.


\subsubsection{Conditional deviance information criteria}

%, and it differs from $\mathrm{lpd}_{\mathrm{DIC},c}$ in that it involves the log of an integrated probability rather than an integrated log probability.

% WAIC is a form of Bayesian information criterion that does not rely on point estimates for the parameters and

% In contrast to DIC, WAIC is based on the full posterior and so is a fully Bayesian alternative that is invariant to reparameterization \parencite[][p. 2]{vehtari2016practical}.

While conditional WAIC and PSIS-LOO approximate $\mathrm{elpd}_c$, the target quantity for DIC differs. The target is
\begin{equation}
  \mathrm{elpd}_{\mathrm{DIC},c} =
  \sum_{j=1}^J \sum_{i=1}^I \int
    p_\mathrm{t}(\tilde y_{i'j}) p(\tilde y_{i'j} | \bar\omega, \bar\zeta_j)
  ~d\tilde y_{i'j}
,\end{equation}
where $\bar\omega$ and $\bar\zeta_j$ represent the posterior means for their respective parameters. While $\mathrm{elpd}_c$, presented earlier, involves the full posterior predictive distribution $p(\tilde y_{i'j} | y)$, $\mathrm{elpd}_{\mathrm{DIC},c}$ instead relies on the predictive distribution $p(\tilde y_{i'j} | \bar\omega, \bar\zeta_j)$, which is based on point estimates. The use of point estimates differentiates DIC from WAIC and PSIS-LOO.

The naive, in-sample equivalent of $\mathrm{elpd}_{\mathrm{DIC},c}$ is the log-likelihood of the observed $y$ evaluated at the posterior mean, which is
\begin{equation} \label{eq:3-best-lpdc}
  \mathrm{lpd}_{\mathrm{DIC},c}^* =
    \sum_{j=1}^J \sum_{i=1}^I
    \log p(y_{ij} | \bar \omega, \bar \zeta_j)
.\end{equation}
Estimates for $\bar \omega$ and $\bar \zeta_j$ are plugged into the above to obtain
$\widehat{\mathrm{lpd}}_{\mathrm{DIC},c}^*$,
the estimated log-likelihood evaluated at the posterior mean.
Obtaining the estimated effective number of parameters requires this quantity as well as the mean of the log-likelihood taken over the posterior, which is
\begin{equation}
  \mathrm{lpd}_{\mathrm{DIC},c} =
  \sum_{j=1}^J \sum_{i=1}^I
  \iiint \log
    p(y_{ij} | \omega, \zeta_j)
    p(\zeta_j | \omega, \psi, y)
    p(\omega, \psi | y)
  ~\mathrm{d}\omega \mathrm{d}\psi \mathrm{d}\zeta_j
.\end{equation}
This $\mathrm{lpd}_{\mathrm{DIC},c}$ differs from $\mathrm{lpd}_c$ in that it is the log-likelihood integrated over the posterior rather than the log of the likelihood integrated over the posterior.
It is estimated in MCMC simulation as
\begin{equation}
  \widehat{\mathrm{lpd}}_{\mathrm{DIC},c} =
  \sum_{j=1}^J \sum_{i=1}^I
  \left (
    \frac{1}{S} \log \sum_{s=1}^S p(y_{ij} | \omega^s, \zeta_j^s)
  \right )
,\end{equation}
where $\omega^s$ and $\zeta_j^s$ represent parameter values at posterior draw $s$, $s = 1 \cdots S$.
Then the estimated effective number of parameters is
\begin{equation}
  \hat p_{\mathrm{DIC},c} =
  2 (\widehat{\mathrm{lpd}}_{\mathrm{DIC},c}^* -
    \widehat{\mathrm{lpd}}_{\mathrm{DIC},c})
.\end{equation}
The approximation for the expected log pointwise predictive density is
\begin{equation}
  \widehat{\mathrm{elpd}}_{\mathrm{DIC}, c}
  = \widehat{\mathrm{lpd}}_{\mathrm{DIC},c}^* - \hat p_{\mathrm{DIC},c}
,\end{equation}
and the value reported for DIC is usually on the deviance scale:
\begin{equation}
  \mathrm{DIC}_c =
  -2 \widehat{\mathrm{elpd}}_{\mathrm{DIC}, c}
.\end{equation}
This conditional DIC corresponds to the $\mathrm{DIC}_7$ of \textcite{celeux2006deviance}.

The reliance on point estimates in calculating $\widehat{\mathrm{lpd}}_c^*$ results in DIC not being invariant to reparameterization \parencite[][p. 4]{spiegelhalter2014deviance}.
For example, results for DIC will differ depending on whether a parameter like $\psi$ represents a standard deviation or a variance.
Further, owing to the reliance on point estimates, the posterior distribution must be reasonably summarized by its mean \parencite[][p. 1015]{gelman2014understanding}, and it is possible for $\hat p_{\mathrm{DIC},c}$ to be negative if the posterior mean is far from the mode \parencite[][p. 172]{BDA3}.
An alternative for $\hat p_{\mathrm{DIC},c}$ based on the variance of $p(y_{ij} | \omega^s, \zeta_j^s)$ is guaranteed to be positive, but is less numerically stable \parencite[][p. 173]{BDA3}.
Lastly, $\mathrm{DIC}_c$ will only be a good approximation of $-2\mathrm{elpd}_{\mathrm{DIC},c}$ when $\hat p_{\mathrm{DIC},c}$ is much less than the number of units \parencite[][p. 535]{plummer2008penalized}, which may not be the case for $\mathrm{DIC}_c$ given the cluster-specific parameters.

% An alternative way of writing $\widehat{\mathrm{elpd}}_{\mathrm{DIC}, c}$ is
% \begin{equation}
%   \widehat{\mathrm{elpd}}_{\mathrm{DIC}, c}
%   = \widehat{\mathrm{lpd}}_{\mathrm{DIC},c} - \frac{1}{2}\hat p_{\mathrm{DIC},c}
% ,\end{equation}
% which frames the quantity in terms of the mean posterior log-likelihood. However, $\widehat{\mathrm{lpd}}_{\mathrm{DIC},c}$ involves integrating the log-likelihood over the posterior, while $p(\tilde y_{i'j} | y)$ does not include the logarithm, a subtle but important difference.


\subsection{Marginal forms of out-of-sample predictive accuracy}

\subsubsection{Target quantity for the marginal case}

Marginal likelihoods may be used with information criteria instead of the ``default'' conditional likelihoods. In this section, marginal equivalents of the conditional quantities in the previous section are described. The marginal likelihood, which integrates out $\zeta_j$, is
\begin{equation} \label{eq:3-marginal-likelihood}
  p(y_{j} | \omega, \psi) =
  \int
    p(\zeta_{j} | \omega, \psi)
    \prod_{i=1}^I p(y_{ij} | \omega, \zeta_{j})
  ~\mathrm{d} \zeta_{j}
,\end{equation}
where $y_{j}$ is the vector of responses for cluster $j$, and $p(\zeta_{j} | \psi)$ is the prior distribution of $\zeta_j$ given $\psi$. This prior is not directly influenced by the data, in contrast to the posterior $p(\zeta_j | y_j, \omega, \psi)$. Though the conditional likelihood is normally used to specify the Bayesian model in software for MCMC, the marginal likelihood may be calculated after MCMC simulation, which is the approach taken here.

The predictive distribution for $\tilde y_{j'}$, which is a new response vector arising from a new cluster $j'$, is
\begin{equation}
  p(\tilde y_{j'} | y) =
  \iint
    p(\tilde y_{j'} | \omega, \psi)
    p(\omega, \psi | y)
  ~\mathrm{d} \omega \mathrm{d} \psi
,\end{equation}
where
$p(\tilde y_{j'} | \omega, \psi)$
is similar to
$p(y_{j} | \omega, \psi)$
(in Equation~\ref{eq:3-marginal-likelihood}) but for unobserved $\tilde y_{j'}$. The density $p(\tilde y_{j'} | y)$ may be referred to as a mixed predictive distribution \parencite{Gelman1996, marshall2007identifying}; it involves the posterior for $\omega$ and $\psi$ but the prior $p(\tilde \zeta_{j'}, \psi)$ for $\tilde \zeta_{j'}$.
The expected log pointwise predictive density for a new dataset, containing a new sample of clusters, is
\begin{equation}
  \mathrm{elpd}_m =
  \sum_{j=1}^J
  \int
    p_t(\tilde y_{j'})
      \log p(\tilde y_{j'} |  y)
  ~\mathrm{d} \tilde y_{j'}
,\end{equation}
where $p_t$ again is the true data generating distribution. The $m$ subscript indicates that $\mathrm{elpd}_m$ results from the marginal likelihood. It is important to note that here the meaning of ``point'' is redefined to refer to a cluster rather than a single unit within a cluster.


\subsubsection{Marginal widely applicable information criteria}

Marginal WAIC is calculated in much the same way as the conditional version by substituting $p(y_{j} | \omega^s, \psi^s)$ for $p(y_{ij} | \omega^s, \zeta_j^s)$ in the calculations and defining the points to be clusters.
For completeness, the modified equations are presented.
The marginal form for the log pointwise predictive density is
\begin{equation}
  \mathrm{lpd}_m =
  \sum_{j=1}^J
  \log \iint
    p(y_{j} | \omega, \psi)
    p(\omega, \psi | y)
  ~\mathrm{d} \omega \mathrm{d} \psi
\end{equation}
and may estimated from the draws of MCMC simulation as
\begin{equation}
  \widehat{\mathrm{lpd}}_m =
  \sum_{j=1}^J
  \log \left [
    \frac{1}{S} \sum_{s=1}^S
      p(y_{j} | \omega^s, \psi^s)
      p(\omega^s, \psi^s | y)
  \right ]
,\end{equation}
where
$p(y_{j} | \omega^s, \psi^s)$ is similar to
$p(y_{j} | \omega, \psi)$
in Equation~\ref{eq:3-marginal-likelihood} but for a given posterior sample $s$. It is expected that $\mathrm{lpd}_m$ will be less than $\mathrm{lpd}_c$ \parencite{trevisani2003inequalities}.
The effective number of parameters for marginal WAIC is
\begin{equation}
  \hat p_{\mathrm{WAIC},m} =
  \sum_{j=1}^J V_{s=1}^S \log p(y_{j} | \omega^s, \psi^s)
,\end{equation}
the expected log pointwise predictive density is
\begin{equation}
  \widehat{\mathrm{elpd}}_{\mathrm{WAIC}, m} =
  \widehat{\mathrm{lpd}}_m - \hat p_{\mathrm{WAIC},m}
,\end{equation}
and the final value on the deviance scale is
\begin{equation}
  \mathrm{WAIC}_m =
  -2 \widehat{\mathrm{elpd}}_{\mathrm{WAIC},m}
.\end{equation}

The integrated WAIC of \textcite{li2015approximating} bears some relation to this marginal WAIC. Integrated WAIC was developed for the case in which there is a vector of direct parameters ($\zeta_j$) for each observation. The data in this case are not clustered. \textcite{li2015approximating} use Monte Carlo sampling to approximate the integration over the $\zeta_j$ vector. On one hand, integrated WAIC is less general than the marginal WAIC proposed in this chapter in that it does not handle clustered data. On the other hand, it is more general in that it allows for cluster-specific parameters vectors that are correlated between clusters.


\subsubsection{Marginal approximate leave-one-out cross-validation}

The marginal Bayesian leave-one-out expected log pointwise predictive density is
\begin{equation}
  \mathrm{elpd}_{\mathrm{LOO},m} = \sum_{j=1}^J \log p(y_{j} | y_{-j})
\end{equation}
where $y_{-j}$ is the response vectors of all clusters except for the $j$-th cluster and
\begin{equation}
  p(y_{j} | y_{-j}) =
  \iint
    p(y_{j} | \omega, \psi)
    p(\omega, \psi | y_{-j})
  ~\mathrm{d} \omega \mathrm{d} \psi
,\end{equation}
which bears resemblance to the mixed predictive distribution for $\tilde y_{j'}$. An estimate of $\mathrm{elpd}_{\mathrm{LOO},m}$ may be obtained from the posterior draws as
\begin{equation}
  \widehat{\mathrm{elpd}}_{\operatorname{PSIS-LOO},m} =
  \sum_{j=1}^J \log
    \left ( \frac
      {\sum_{s=1}^S w_{j}^s p(y_{j} | \omega^s, \psi^s)}
      {\sum_{s=1}^S w_{j}^s}
    \right )
.\end{equation}
The raw importance ratios are obtained as $p(y_{j} | \omega^s, \psi^s)^{-1}$, and these are adjusted by smoothing and truncating as before to obtain weights $w_{j}^s$.
On the deviance scale,
\begin{equation}
  \operatorname{PSIS-LOO}_m =
  -2 \widehat{\mathrm{elpd}}_{\operatorname{PSIS-LOO},m}
.\end{equation}


\subsubsection{Marginal deviance information criteria}

For marginal DIC, the target quantity is
\begin{equation}
  \mathrm{elpd}_{\mathrm{DIC},m} =
  \sum_{j=1}^J \int \log
    p_\mathrm{t}(\tilde y_{j'}) p(\tilde y_{j'} | \bar\omega, \bar\psi)
  ~d\tilde y_{j'}
.\end{equation}
The marginal log-likelihood evaluated at the posterior means of the parameters is
\begin{equation} \label{eq:2-best-lpdm}
  \mathrm{lpd}_m^* =
    \sum_{j=1}^J
    \log p(y_{j} | \bar \omega, \bar \psi)
,\end{equation}
and plugging in the estimated posterior means for $\bar \omega$ and $\bar \psi$ yields $\widehat{\mathrm{lpd}}_m^*$.
The log-likelihood integrated over the posterior is
\begin{equation}
  \mathrm{lpd}_{\mathrm{DIC},m} =
  \sum_{j=1}^J
  \iint \log
    p(y_{j} | \omega, \psi)
    p(\omega, \psi | y)
  ~\mathrm{d} \omega \mathrm{d} \psi
,\end{equation}
which is estimated in MCMC simulation as
\begin{equation}
  \widehat{\mathrm{lpd}}_{\mathrm{DIC},m} =
  \sum_{j=1}^J
  \frac{1}{S} \sum_{s=1}^S
  \left [
    \log p(y_{j} | \omega^s, \psi^s)
    p(\omega^s, \psi^s | y)
  \right ]
.\end{equation}
The estimated effective number of parameters for marginal DIC is
\begin{equation}
  \hat p_{\mathrm{DIC},m} =
  2(\widehat{\mathrm{lpd}}_m^* - \widehat{\mathrm{lpd}_m})
,\end{equation}
the expected log pointwise predictive density is
\begin{equation}
  \widehat{\mathrm{elpd}}_{\mathrm{DIC}, m} =
  \widehat{\mathrm{lpd}}_m^* - \hat p_{\mathrm{DIC},m}
,\end{equation}
and the final value on the deviance scale is
\begin{equation}
  \mathrm{DIC}_m =
  -2 \widehat{\mathrm{elpd}}_{\mathrm{DIC},m}
.\end{equation}
This marginal DIC corresponds to the $\mathrm{DIC}_1$ of \textcite{celeux2006deviance}. It has not been used much in the literature, partly due to the difficulty in integrating out the cluster-specific parameters.


\section{Adaptive Gaussian quadrature for marginal likelihoods}
\label{sec:adaptive-quadrature}

For models with normally distributed $y_{ij}$, obtaining $\widehat{\mathrm{lpd}}_m$ by way of Equation~\ref{eq:3-analytic} provides an exact and computationally efficient result. For cases where an analytical form for the integration is unavailable, such as logistic models, Gaussian quadrature may be used to perform numerical integration. (Both methods depend on the integration being performed over a normal prior distribution.) \textcite{rabe2002reliable} applied the adaptive quadrature scheme developed by \textcite{naylor1982applications} to generalized linear mixed models. In this chapter, that approach is extended to the individual posterior draws from Markov chain Monte Carlo simulation.

The proposed adaptive quadrature method relies on $M$ standard Gaussian quadrature node locations $G_{\mathrm{std},m}$ and weights $W_{\mathrm{std},m}$, $m = 1 \cdots M$, as well as the posterior mean and standard deviation of each $\zeta_j$. The posterior mean of $\zeta_j$ is
\begin{equation}
  \hat \mu_j =
  \hat E(\zeta_j | y) =
  \frac{1}{S} \sum_{s=1}^S \zeta_j^s
,\end{equation}
and the posterior standard deviation is
\begin{equation}
  \hat \tau_j =
  \sqrt{\widehat\mathrm{var}(\zeta_j | y_j) } =
  \sqrt{V_{s=1}^S \zeta_j^s}
.\end{equation}
The posterior means and standard deviations are marginal over $\omega$ and $\psi$, whereas adaptive quadrature for maximum likelihood estimation would use conditional quantities. The adaptive quadrature node locations are
\begin{equation}
  \mathrm{G}_{jm} = \hat \mu_j +
                    \hat \tau_j \times
                    \mathrm{G}_{\mathrm{std},m}
,\end{equation}
and their weights are
\begin{equation}
  \mathrm{W}_{jm}^s = \sqrt{2\pi} \times
        \hat \tau_j \times
        \exp \left ( \frac{\mathrm{G}_{jm}^2}{2} \right ) \times
        \phi \left ( \mathrm{G}_{jm}; 0, \psi^{2,s} \right ) \times
        W_{\mathrm{std},m}
.\end{equation}
The adaptive quadrature node locations will differ between clusters, while the weights will differ between both clusters and MCMC iterations because they depend on $\psi^s$.
The marginal likelihood for cluster $j$ at posterior draw $s$ is approximated as
\begin{equation}
  p(y_{j} | \omega^s, \psi^s) \approx
  \sum_{m=1}^M \left [
    \mathrm{W}_{jm}^s \prod_{i=1}^I p(y_{ij} | \omega^s, \mathrm{G}_{jm})
  \right ]
,\end{equation}
where $p(y_{ij} | \omega^s, \mathrm{G}_{jm})$ is similar to the conditional likelihood $p(y_{ij} | \omega^s, \zeta_j^s)$ except that $\mathrm{G}_{jm}$ is substituted for $\zeta_j^s$.

%The marginal likelihood at a given MCMC iteration $s$ is approximated as a weighted sum, this marginal likelihood is averaged across draws, and then the log of the result is summed across clusters:
% \begin{equation}
%   \widehat{\mathrm{lpd}}_m =
%   \sum_{j=1}^J
%   \log \frac{1}{S} \sum_{s=1}^S \sum_{m=1}^M
%   \left [
%     \mathrm{W}_{jm}^s
%     \prod_{i=1}^I
%       \phi \left ( y_{ij}; x_j' \beta + \mathrm{G}_{jm}, \sigma^{2,s} \right )
%   \right ]
% .\end{equation}

\section{Circular block bootstrap for estimating Monte Carlo error}

Straightforward expressions exist for estimating the Monte Carlo error for means or variances of functions of parameters but not for more complicated quantities like DIC, WAIC, and PSIS-LOO. Instead, the moving block bootstrap \parencite{kunsch1989jackknife, liu1992moving}, a bootstrap technique for autocorrelated data, may be used to estimate the Monte Carlo error for these quantities. The moving block bootstrap may be used in this context by concatenating the independent MCMC chains into a single chain having length $S$. Then blocks of consecutive draws are drawn with replacement and concatenated into a new chain of the same length, $S$. A quantity of interest, in this case information criterion, is recorded for the new chain. The process is repeated a large number of times, and the standard deviation of the results provides a bootstrap estimate of the Monte Carlo error. This approach may further be improved by using the circular block bootstrap \parencite{politis1992circular}, which joins the ends of the chain, forming a circle. In this way, a sampled block may wrap around from the last observations to the first observations, solving the problem in the moving block bootstrap of under sampling the early and late observations.

Sampling from the original draws in blocks preserves the autocorrelation structure when forming a bootstrap chain, except at the ``seams'' where the blocks are stitched together. Some care is needed in selecting the size of the blocks to maintain the autocorrelation structure while also obtaining sufficiently different block bootstrap samples.
Data-dependent means of selecting a block length have been proposed
\parencite{hall1995blocking, buhlmann1999block,
           politis2004automatic, patton2009correction},
but these methods are not used in this chapter. Instead, a simulation is conducted in which a wide range of block sizes are chosen with the circular block bootstrap in order to study how the  results may depend on block size. As an aside, both \textcite{hall1995blocking} and \textcite{buhlmann1999block} suggest a block size of $S^\frac{1}{3}$ as a starting point.

% For inferences regarding variance, \textcite{hall1995blocking} suggest starting with a block size of $S^\frac{1}{3}$, where $S$ is the number of time series observations, or in this case, the number of posterior draws, and then using an algorithm of their design for refining this initial choice. Their procedure involves computing the bootstrap estimates many times, effectively bootstrapping the bootstrap procedure, and so is impractical for a very long series, as is the case here.
%Other procedures have been developed to selecting a block size \parencite[for example,][]{politis2004automatic, patton2009correction}, but these are similarly impractical for data of this type.

% However, given that the $S$ tends to be very large in MCMC compared with the lag at which autocorrelation becomes negligible, applications of this technique to MCMC draws may be more robust than for the cases considered by \textcite{hall1995blocking}. I propose obtaining Monte Carlo error estimates using a block size of $S^\frac{1}{3}$ and then obtaining the same for a larger and smaller block size as a robustness check. If the results do not differ much between block sizes, then the initial choice of $S^\frac{1}{3}$ may be accepted.


\section{Simulation study of adaptive quadrature and circular block bootstrap}

\subsection{Data and model}

<<sim_start>>=
load("Simulation/simulation.Rdata")
@

Data are generated and analyzed using a linear random intercept model because for this model marginal likelihoods may be obtained by exact integration, which will serve as a benchmark to test the adaptive Gaussian quadrature approximation. The model is
\begin{equation}
  y_{ij} | x_j, \beta, \zeta_j, \sigma^2 \sim
  \mathrm{N}(x_j'\beta + \zeta_j, \sigma^2)
\end{equation}
\begin{equation}
  \zeta_j \sim \mathrm{N}(0, \psi^2)
\end{equation}
\begin{equation}
  \beta \sim \mathrm{N}(0, 4)
\end{equation}
\begin{equation}
  \sigma \sim \mathrm{Exp}(.1)
\end{equation}
\begin{equation}
  \psi \sim \mathrm{Exp}(.1)
,\end{equation}
% \begin{equation}
%   y_{ij} = x_j'\beta + \zeta_j + \epsilon_{ij}
% ,\end{equation}
where $i = 1 \ldots I$ indexes observations within cluster $j$, $j = 1 \ldots J$.
Further,
\begin{equation}
  x_j'\beta = \beta_0 + \beta_1 x_{1j} + \beta_2 x_{2j} + \beta_3 x_{3j} +
  \beta_4 x_{1j} x_{2j} + \beta_5 x_{2j} x_{3j}
.\end{equation}
% The priors are
% $\beta \sim \mathrm{U}(-\infty, \infty)$,
% $\epsilon_{ij} \sim \mathrm{N}(0, \sigma^2)$
% $\zeta_j \sim \mathrm{N}(0, \psi^2)$,
% $\sigma \sim \mathrm{Exp}(.1)$, and
% $\psi \sim \mathrm{Exp}(.1)$.
The generating parameters are:
$\sigma = \Sexpr{sim_sigma}$,
$\psi = \Sexpr{sim_psi}$, and
$\beta = \{\Sexpr{paste(sim_beta, collapse=",")}\}$.
One dataset is simulated for each cluster size
$I \in \{\Sexpr{paste(sim_I,collapse=",")}\}$.
Each dataset has $J = \Sexpr{sim_J}$ clusters, and the covariates in $x_j$ are random draws from a standard normal distribution.

All MCMC simulations within the simulation study use \Sexpr{n_chains} chains of \Sexpr{n_iter} iterations. The first \Sexpr{n_warmup} iterations of each chain are discarded, leaving a total of \Sexpr{n_posterior} posterior draws across the chains. Convergence is monitored using the $\hat R$ statistic of \textcite{gelman1992inference}; when $\hat R < 1.1$ for each parameter and for the log posterior, convergence may be inferred.

The linear random intercept model is a special case of the general model described previously. Let $\omega = \{\beta, \sigma\}$, and then $\omega$, $\zeta_j$, and $\psi$ directly correspond to the parameters of the general model. The likelihood for the linear random intercept model is
\begin{equation} \label{eq:3-conditional-likelihood}
  p(y_{ij} | x_j, \omega, \zeta_j) =
  \phi(y_{ij}; x_j'\beta + \zeta_j, \sigma^2)
,\end{equation}
where $\phi$ is the normal density function. The conditional log pointwise predictive density is
\begin{equation}
  \widehat{\mathrm{lpd}}_c =
  \sum_{j=1}^J \sum_{i=1}^I
    \log \frac{1}{S} \sum_{s=1}^S
    \phi(y_{ij}; x_j'\beta^s + \zeta_j^s, \sigma^{s,2})
.\end{equation}
The simulation focuses on the marginal likelihood, which has a simple form because of the normally distributed $y_{ij}$ and $\zeta_j$:
\begin{equation}
  p(y_{j} | x_j, \omega, \psi) =
  \Phi \left(y_j; x_j'\beta, \Omega \right)
,\end{equation}
where $\Phi$ is the multivariate normal density function and $\Omega$ is an $I$-by-$I$ covariance matrix with elements on the diagonal equal to $\psi^2 + \sigma^2$ and elements on the off-diagonal equal to $\psi^2$. Then marginal log pointwise predictive density is
\begin{equation} \label{eq:3-analytic}
  \widehat{\mathrm{lpd}}_m =
  \sum_{j=1}^J
    \log \frac{1}{S} \sum_{s=1}^S \Phi(y_j; x_j'\omega^s, \Omega^s)
.\end{equation}
Marginal information criteria may be calculated from this $\widehat{\mathrm{lpd}}_m$ without resorting to a quadrature approximation.


\subsection{Conditional and marginal information criteria estimates}

<<sim_count_parameters>>=
# No output
p_c <- length(sim_beta) + sim_J + 1
p_m <- length(sim_beta) + 2
@

Information criteria for the \Sexpr{numword(length(sim_I))} simulated datasets are presented in Table~\ref{tab:3-sim-ic}. The multivariate normal density (as in Equation~\ref{eq:3-analytic}) is used to evaluate the marginal likelihood. Values for WAIC and PSIS-LOO ($-2\widehat{\mathrm{elpd}}$) are very close to one another in both the conditional and marginal cases, while those for DIC are slightly lower.

The effective number of parameters $\hat p$ may be compared against the number of parameters in focus. In the conditional focus, there are \Sexpr{p_c} parameters (contained in $\{\beta, \zeta, \sigma\}$), but $\hat p_c$ is less than this count in all cases. The reason for this discrepancy is that the prior on $\zeta$ is informative and as such partially constrains $\zeta$, reducing the effective number of parameters. As cluster size increases, $\hat p_c$ decreases, reflecting the increase of data available for estimating each $\zeta_j$ and the resulting decline in influence of the prior. In the marginal focus, there are \Sexpr{p_m} parameters (contained in $\{\beta, \sigma, \psi\}$), and $\hat p_m$ approximately matches this count.

\begin{table}[htb]
\centering
<<sim_ic_table, results = tex>>=
df_mvn <- subset(df_combine, nagq == 0 & variable %in% c("dev", "p"))
sort_key <- c(DIC = 1, WAIC = 2, "PSIS-LOO" = 3)
df_mvn$sort_by_ic <- sort_key[df_mvn$IC]
df_mvn <- dcast(df_mvn, I + sort_by_ic + IC ~ focus + variable)
df_mvn$sort_by_ic <- NULL
names(df_mvn) <- c("$I$", "Criterion", "$-2\\widehat{\\mathrm{elpd}}_c$",
                   "$\\hat p_c$", "$-2\\widehat{\\mathrm{elpd}}_m$",
                   "$\\hat p_m$")

df_q <- subset(df_quad_compare, variable == "dev")
n <- nrow(df_q)
dif_last_agq <- c(NA, with(df_q, agq[2:n] - agq[1:(n-1)]))
df_q$dif_last_agq <- dif_last_agq
df_q <- melt(df_q, id.vars = c("I", "IC", "focus", "nagq"),
           measure.vars = c("dif_mvn", "dif_nagq"), na.rm = TRUE)
df_q$sort_by_ic <- sort_key[df_q$IC]
df_q <- dcast(subset(df_q, abs(df_q$value) < .01),
              I + sort_by_ic + IC ~ variable, min, value.var = "nagq")
# Following warning is a known bug and can be ignored:
# In .fun(.value[0], ...) : no non-missing arguments to min; returning Inf
df_q$sort_by_ic <- NULL
names(df_q) <- c("$I$", "Criterion", "Absolute", "Relative")

df_both <- cbind(df_mvn[,1:4], NA, df_mvn[,5:6], NA, df_q[,-1:-2])
xtab_both <- xtable(df_both,
                    display = c("d","d","s","f","f", "s", "f", "f", "s","d","d"))
add_hlines <- which(1:nrow(xtab_both) %% length(sim_I) == 0)
add_row <- list(list(-1),
                paste("\\hline",
                      "& & \\multicolumn{2}{c}{Conditional}",
                      "& & \\multicolumn{2}{c}{Marginal}",
                      "& & \\multicolumn{2}{c}{Minimum N. nodes} \\\\",
                      "\\cline{3-4} \\cline{6-7} \\cline{9-10} \n"))
print(xtab_both, include.rownames = FALSE, floating = FALSE,
      hline.after = c(0, add_hlines), add.to.row = add_row,
      sanitize.text.function = function(x) x,
      sanitize.colnames.function = function(x) ifelse(x == "NA", "", x))
@
\caption[Conditional and marginal DIC, WAIC, and PSIS-LOO for the simulated datasets using the multivariate normal density]
{Conditional and marginal DIC, WAIC, and PSIS-LOO for the simulated datasets using the multivariate normal density. In the conditional focus, there are a total of
\Sexpr{p_c}
model parameters, whereas there are
\Sexpr{p_m}
in the marginal focus. Shown on far right are the minimum numbers of adaptive quadrature nodes (among those considered) needed to obtain an absolute and relative error less than .01. Absolute error refers to the absolute difference between the adaptive quadrature approximation and exact results. Relative error refers to the difference between the adaptive quadrature approximation and the same approximation using one-third fewer nodes.}
\label{tab:3-sim-ic}
\end{table}


\subsection{Adaptive quadrature approximation}

Marginal DIC, WAIC, and PSIS-LOO are calculated on the same \Sexpr{numword(length(sim_I))} simulated datasets using \Sexpr{and(sim_nodes)} adaptive quadrature nodes. Each number of nodes is 50\% greater than the preceding number, rounded to the nearest odd number. With an odd number of nodes, one node is placed on the mean of the distribution, which does not happen for an even number. The absolute difference between approximate results from adaptive quadrature and exact results from the multivariate normal density are shown in Figure~\ref{fig:3-compare}. In this chapter, an absolute difference less than .01 is judged to be a sufficiently close approximation (shown as horizontal dashed lines), which is somewhat arbitrary but should be conservative unless the information criteria values are very close. The figure indicates that more nodes are required as cluster size increases and that DIC requires more nodes than WAIC or PSIS-LOO.

\begin{figure}[tb]
\begin{center}
<<sim_agq_plot, fig = TRUE, height = 3.5>>=
df_qc <- subset(df_quad_compare, variable == "dev")
key <- c("DIC" = "DIC[m]",
         "WAIC" = "WAIC[m]",
         "PSIS-LOO" = "PSIS-LOO[m]")
df_qc$Criterion <- factor(key[df_qc$IC], key)
df_qc$I <- factor(df_qc$I, rev(unique(df_qc$I)))
legend_title <- "Cluster\nsize"
ggplot(df_qc) +
  aes(x = nagq, y = abs(dif_mvn), color = as.factor(I), pch = as.factor(I)) +
  geom_hline(yintercept = .01, lty = "dashed") +
  geom_line() +
  geom_point() +
  scale_x_continuous(breaks = sim_nodes) +
  scale_y_log10() +
  facet_wrap(~Criterion, labeller = label_parsed) +
  xlab(expression(paste("Number of adaptive Gaussian quadrature nodes (",
                         italic(M), ")"))) +
  ylab("Absolute error of approximation") +
  labs(color = legend_title, pch = legend_title) +
  my_theme +
  theme(panel.grid.minor.x = element_blank()) # +
  # coord_cartesian(ylim = c(1e-8, max(abs(df_qc$dif_mvn))))
@
\end{center}
\caption[Differences in marginal information criteria between calculations using adaptive quadrature and the multivariate normal density function]
{Differences in marginal information criteria ($-2 \times \widehat{\mathrm{elpd}}_m$) between calculations using adaptive quadrature (approximate method) and the multivariate normal density function (exact method). The $y$-axis uses a log scale. The dashed line is drawn at .01.}
\label{fig:3-compare}
\end{figure}

<<sim_agq_text>>=
# For in text, what number of nodes results in minimal change
df <- subset(df_quad_compare, variable == "dev")
n <- nrow(df)
dif_last_agq <- c(NA, with(df, agq[2:n] - agq[1:(n-1)]))
df$dif_last_agq <- dif_last_agq
which_compare <- aggregate(dif_last_agq ~ I + IC,
                           data = df,
                           function(x) which(abs(x) < .01)[1])
which_compare$nodes <- sim_nodes[which_compare$dif_last_agq]
df_difficult <- subset(df_quad_compare, grepl("(best|mean)_lpd", variable) &
                         I == max(sim_I) &
                         nagq == min(sim_nodes))
best_lpd <- df_difficult$dif_mvn[df_difficult$variable == "best_lpd"]
mean_lpd <- df_difficult$dif_mvn[df_difficult$variable == "mean_lpd"]
@

Marginal DIC required more nodes than WAIC or PSIS-LOO to obtain the desired accuracy, which appears to be due to the difficulty in obtaining accurate values for $\widehat{\mathrm{lpd}}_m^*$ (Equation~\ref{eq:2-best-lpdm}) with adaptive quadrature. To illustrate, consider the most difficult simulation condition, which involved using \Sexpr{min(sim_nodes)} nodes to estimate marginal quantities for cluster sizes of $I=\Sexpr{max(sim_I)}$. In this condition, the difference between the adaptive quadrature approximation for $\widehat{\mathrm{lpd}}_m^*$ and the exact value was \Sexpr{f(best_lpd, 2)}, while the corresponding difference for $\widehat{\mathrm{lpd}}_m$ was \Sexpr{f(mean_lpd, 2)}.
Given that adaptive quadrature calculations for $p(y_{j} | \omega^s, \psi^s)$ and $p(y_{j} | \bar \omega, \bar \psi)$ are identical, the disparity in accuracy is likely due to the fact that $p(y_{j} | \omega^s, \psi^s)$ is averaged over many posterior draws to obtain $\widehat{\mathrm{lpd}}_m$, but no such averaging is involved in using $p(y_{j} | \bar \omega, \bar \psi)$ to obtain $\widehat{\mathrm{lpd}}_m^*$. It may be that errors resulting from adaptive quadrature cancel out, at least partially, when they are averaged over posterior draws.

In real applications, adaptive quadrature will only be used when an exact calculation is not available, and so a sufficient number of nodes cannot generally be determined by comparison to an exact calculation.
In this case, results may be obtained and compared for different numbers of nodes to calculate a \emph{relative} error of approximation. The relative error of approximation may be calculated for $M$ nodes by finding the absolute value of difference between the information criteria result using $M$ nodes and using $\frac{2}{3}M$, which corresponds to the node counts used in this study. In this chapter, an approximation using $M$ nodes is judged sufficient when the relative error of approximation is less than .01, which again is somewhat arbitrary but is expected to be conservative. Table~\ref{tab:3-sim-ic} provides the minimum number of nodes needed to obtain the desired absolute (that is, in comparison to the exact result) and relative precision. The sufficient number of nodes as judged by relative precision tends to be the next higher number than that indicated by absolute precision. In this way, the simulation results support the conservativeness of this approach in testing the number of nodes used.


\subsection{Circular block bootstrap}

<<sim_bruteforce_load>>=
load("Simulation/bootstrap.Rdata")
@

Focusing on WAIC, the circular block bootstrap will be used to obtain standard error estimates. Before that, however, a ``brute force'' approach is used as a reliable means of studying the variability of WAIC that arises from Monte Carlo error. To this end, \Sexpr{length(sim_bs_l)} independent sets of MCMC chains are simulated, and for each the conditional and marginal versions of $\mathrm{lpd}_{\mathrm{WAIC}}$, $p_{\mathrm{WAIC}}$, and $\mathrm{WAIC}$ are estimated. The same simulated dataset is used throughout, having \Sexpr{sim_J} clusters each having \Sexpr{sim_bs_I} units. The standard deviation of the brute force results may serve as a benchmark against which to compare the bootstrap standard error estimates.
The means and standard deviations related to the brute force replications are presented in Table~\ref{tab:3-sim-bf}. Conditional WAIC ($\mathrm{WAIC}_c$) is substantially more variable than marginal WAIC ($\mathrm{WAIC}_m$), as shown by its larger standard deviation. For either the conditional or marginal case, most of the variability appears tied to $p_{\mathrm{WAIC}}$ rather than $\mathrm{lpd}_{\mathrm{WAIC}}$, which has a relatively low standard deviation across replications.

\begin{table}[htb]
\centering
<<sim_bruteforce_table, results = tex>>=
est_names <- c("lpd_waic", "p_waic", "waic")
df_brute <- melt(df_bootstrap, id.vars = c("l", "focus"),
                 measure.vars = est_names)

df_mean <- dcast(df_brute, variable ~ focus, mean)
names(df_mean)[2:3] <- paste0(names(df_mean)[2:3], "_mean")
df_sd <- dcast(df_brute, variable ~ focus, sd)
names(df_sd)[2:3] <- paste0(names(df_sd)[2:3], "_sd")
df_brute <- merge(df_mean, df_sd)
df_brute <- cbind(df_brute[,c(1,3,5)], NA, df_brute[,c(2,4)])
names(df_brute) <- c("", "Mean", "SD", "", "Mean", "SD")

key <- c("lpd_waic" = "$\\mathrm{lpd}_\\mathrm{WAIC}$",
         "p_waic" = "$\\hat p_\\mathrm{WAIC}$",
         "waic" = "$\\mathrm{WAIC}$")
df_brute[,1] <- key[df_brute[,1]]

xtab_bf <- xtable(df_brute, display = c("s","s","f","f","s","f","f"))
add_row <- list(list(-1),
                paste("\\hline",
                      "& \\multicolumn{2}{c}{Conditional} & ",
                      "& \\multicolumn{2}{c}{Marginal} \\\\",
                      "\\cline{2-3} \\cline{5-6} \n"))
print(xtab_bf, include.rownames = FALSE, floating = FALSE,
      hline.after = c(0, 3), add.to.row = add_row,
      sanitize.text.function = function(x) x,
      sanitize.colnames.function = function(x) x)
@
\caption[Means and standard deviations for the ``brute force'' WAIC results]
{Means and standard deviations for the ``brute force'' WAIC results for the simulation. Results are from \Sexpr{length(sim_bs_l)} independent sets of MCMC chains using the same simulated dataset with cluster size $I=\Sexpr{sim_I[1]}$.}
\label{tab:3-sim-bf}
\end{table}

Next, the circular block bootstrap is applied once each to the \Sexpr{length(sim_bs_l)} independent sets of MCMC chains. Block sizes ranging from \Sexpr{min(sim_bs_l)} to \Sexpr{max(sim_bs_l)} are used, and the results are presented in Figure~\ref{fig:3-bootstrap}. The bootstrap substantially underestimates the Monte Carlo error for both conditional and marginal WAIC. While Monte Carlo error estimates for $\mathrm{lpd}_\mathrm{WAIC}$ and $p_\mathrm{WAIC}$ are both too small using the bootstrap, the discrepancy for $p_\mathrm{WAIC}$ is much worse. Further, block size does not appear to have an effect on estimated Monte Carlo error, possibly because autocorrelations with Stan are typically low. This bootstrap scheme may also be applied to DIC and PSIS-LOO, but there is no reason to expect better performance.

\begin{figure}[tb]
\begin{center}
<<sim_bootstrap_plot, fig = TRUE>>=
df_bf <- melt(df_bootstrap, id.vars = c("l", "focus", "IC"),
              measure.vars = c("lpd_waic", "p_waic", "waic"))
df_bf <- dcast(df_bf, focus + IC + variable ~ ., sd)
names(df_bf)[names(df_bf) == "."] <- "bruteforce_sd"

mce_names <- paste0("mce_", est_names)
df_bs <- melt(df_bootstrap, id.vars = c("l", "focus"), measure.vars = mce_names)
names(df_bs)[names(df_bs) == "value"] <- "bootstrap_est"
df_bs$variable <- sub("mce_", "", df_bs$variable)
df_bs <- merge(df_bs, df_bf)

df_bs$variable <- paste0(df_bs$variable, "_",
                         ifelse(df_bs$focus == "Marginal", "m", "c"))
key <- c("lpd_waic_c" = "lpd[list(WAIC,c)]",
         "p_waic_c" = "p[list(WAIC,c)]",
         "waic_c" = "WAIC[c]",
         "lpd_waic_m" = "lpd[list(WAIC,m)]",
         "p_waic_m" = "p[list(WAIC,m)]",
         "waic_m" = "WAIC[m]")
df_bs$variable <- factor(key[df_bs$variable], key)

ggplot(df_bs) +
  aes(x = l, y = bootstrap_est) +
  geom_hline(aes(yintercept = bruteforce_sd), linetype = "dashed") +
  geom_point(alpha = .2) +
  geom_smooth(method = "loess", se = FALSE) +
  facet_wrap(~ variable, nrow = 2, scales = "free_y", labeller = label_parsed) +
  xlab("Block size") +
  ylab("Bootstrap standard error estimate") +
  my_theme
@
\end{center}
\caption[Circular block bootstrap standard error estimates for WAIC by block size for the simulation]
{Circular block bootstrap standard error estimates for WAIC by block size for the simulation. Points are bootstrap estimates from \Sexpr{length(sim_bs_l)} independent sets of MCMC chains using the same simulated dataset, and the solid line is a loess curve fit to the points. The dashed line is the ``brute force'' standard error estimate. The $y$-axes vary.}
\label{fig:3-bootstrap}
\end{figure}


\section{Applied example}

\subsection{Data and models}

<<application_start>>=
rm(list = ls()[! ls() %in% startup_vars])
load("Application/application.Rdata")
load("Application/brute_force.Rdata")
@

A latent regression Rasch model is fit to a dataset on verbal aggression \parencite{Vansteelandt2000} that consists of $J = 316$ persons and $I = 24$ items.
Participants were instructed to imagine four frustrating scenarios, and for each they responded to items regarding whether they would react by cursing, scolding, and shouting. They also responded to parallel items regarding whether they would \emph{want} to engage in the three behaviors, resulting in a total six items per scenario (cursing/scolding/shouting $\times$ doing/wanting). An example item is, ``A bus fails to stop for me. I would want to curse.'' The response options for all items were ``yes'', ``perhaps'', and ``no.'' The items have been dichotomized for this example by combining ``yes'' and ``perhaps'' responses. Two person-related covariates are included: the respondent's trait anger score \parencite{spielberger1988state}, which is a raw score from a separate measure taking values ranging from 11 to 39 in the data, and an indicator for whether the respondent is male, which takes the values 0 and 1.

The model is
% \begin{equation} \label{eq:4-latregrasch}
%   \Pr(y_{ij} = 1 | w_j, \lambda, \zeta_j, \delta_i) =
%   \mathrm{logit}^{-1}(w_j' \lambda + \zeta_j - \delta_i),
% \end{equation}
\begin{equation} \label{eq:4-latregrasch}
  y_{ij} | w_j, \lambda, \zeta_j, \delta_i \sim
  \mathrm{Bernoulli}\left (
    \mathrm{logit}^{-1}(w_j' \lambda + \zeta_j - \delta_i)
  \right )
\end{equation}
\begin{equation}
  \delta_1 \ldots \delta_{I-1} \sim \mathrm{N}(0, 9)
\end{equation}
\begin{equation}
  \zeta_j \sim \mathrm{N}(0, \sigma^2)
\end{equation}
\begin{equation}
  \sigma \sim \mathrm{Exp}(.1)
\end{equation}
\begin{equation}
  \lambda \sim t_1(0, 1)
,\end{equation}
where $y_{ij} = 1$ if the response for person $j$ to item $i$ is correct and $y_{ij} = 0$ otherwise, $w_j$ is a vector of person-related covariates, $\lambda$ is a vector of latent regression coefficients, $\zeta_j$ is a person residual, and $\delta_i$ is an item difficulty parameter. One element of $w_j$ is one for the intercept, and the last item difficulty is constrained, $\delta_I = -\sum_{i}^{(I-1)} \delta_i$.
The priors for $\lambda$ match those recommended by \textcite{gelman2008weakly} for logistic regression. First, the covariates ($w_j$) are transformed. Continuous covariates are mean-centered and then rescaled to have a standard deviation of .5. Binary covariates are also mean-centered and then are rescaled by dividing by the difference between their maximum and minimum values, which results in a range of 1. A constant supplied for the model intercept is left to equal 1. With these transformations, the same prior is applied to all coefficients,
$\lambda \sim t_1(0, 1)$,
where $t_1$ is the Student's $t$ distribution with one degree of freedom. A transformation may be applied to $\lambda$ to find what the regression coefficients would be on the original scale of the covariates.

Focus is placed on $\zeta_j$ for the conditional approach, which yields a prediction inference involving new responses from the same persons (and items). The marginal approach, perhaps more realistically, places focus on $\sigma$, implying a prediction inference involving new responses from a new sample of persons.
Five competing models are considered, differing only in what person covariates are included: Model 1 includes no covariates, Model 2 has the trait anger score, Model 3 has the indicator for male, Model 4 has both covariates, and Model 5 has both covariates and their interaction. All models include an intercept term.


\subsection{Results}

The five models are estimated with Stan using \Sexpr{n_chains} chains of \Sexpr{f(n_iter, 0)} draws with the first \Sexpr{f(n_warmup, 0)} draws of each discarded, resulting in a total of
\Sexpr{f(n_posterior,0)}
kept posterior draws. The larger number of posterior draws is chosen here due to the anticipated Monte Carlo errors, but such a large number is not ordinarily necessary for estimating, for example, the posterior means and standard deviations for parameters.
%The $\hat R$ convergence statistic was \Sexpr{#f(max_rhat,2)} or less for all parameters and the log posterior across models.
Marginal DIC, WAIC, and PSIS-LOO are computed using adaptive quadrature to integrate out $\zeta$. Arbitrarily focusing on Model~4, the process described in the previous section was used to determine that \Sexpr{n_agq} nodes are sufficient to obtain an accurate approximation.
All evaluations of the marginal likelihood that follow use this number of nodes.

In order to obtain an indication of the variability of results owing to Monte Carlo error, $\Sexpr{n_repeats}$ independent sets of MCMC chains are run for each model. The circular block bootstrap is not used for this purpose, as it was found to substantially underestimate the Monte Carlo error.
Figure~\ref{fig:3-effn} provides the estimated effective number of parameters ($\hat p$) for each model and focus, as well as the count of parameters associated with each focus.
For conditional information criteria, the $\hat p$ are substantially less than the counts of parameters, owing mainly to the fact that each $\zeta_j$, with its hierarchical prior, contributes less than one to the effective number of parameters.
On the other hand, $\hat p$ for marginal information criteria are close to the counts of model parameters, and for WAIC and PSIS-LOO it tends to be slightly larger than the count of parameters.

\begin{figure}[htb]
\centering
<<parameters, fig = TRUE>>=
df <- df_models

# Add IC type to values in "variable" column when not included
df$variable <- as.character(df$variable)
append_dic <- grepl("^(best\\_lpd|mean\\_lpd)$", df$variable)
df$variable[append_dic] <- paste0(df$variable[append_dic], "_dic")
append_waic <- grepl("^p\\_[0-9]+$", df$variable)
df$variable[append_waic] <- paste0(df$variable[append_waic], "_waic")
append_looic <- grepl("^pk\\_[0-9]+$", df$variable)
df$variable[append_looic] <- paste0(df$variable[append_looic], "_looic")

df$Criterion <- toupper(sub("^.+\\_([a-z]+)$", "\\1", df$variable))
df$Criterion <- toupper(df$Criterion)
df$Criterion <- sub("(LOO|LOOIC)", "PSIS-LOO", df$Criterion)
df$Criterion <- factor(df$Criterion, c("DIC", "WAIC", "PSIS-LOO", "Count"))

df$variable <- sub("\\_(dic|waic|loo|looic)$", "", df$variable)
df$variable <- sub("^(dic|waic|loo|looic)$", "estimate", df$variable)

marginal_pars <- c(25, 26, 26, 27, 28)
conditional_pars <- c(25, 26, 26, 27, 28) - 1 + 316
df_par_counts <- data.frame(focus = rep(c("Marginal", "Conditional"), each = 5),
                            model = rep(1:5, times = 2),
                            count_pars = c(marginal_pars[1:5], conditional_pars[1:5]))
df <- merge(df, df_par_counts)

ggplot(subset(df, variable == "p")) +
  aes(y = value, x = model) +
  geom_jitter(height = 0, width = .1, alpha = .25) +
  geom_segment(aes(y = count_pars, yend = count_pars,
                   x = model-.4, xend = model+.4)) +
  facet_grid(focus ~ Criterion, scale = "free_y") +
  labs(y = "Estimated effective number of parameters", x = "Model") +
  my_theme +
  theme(panel.grid.minor.x = element_blank())
@
\caption[Estimated effective number of parameters for the five latent regression Rasch models]
{Estimated effective number of parameters ($\hat p$) for the five latent regression Rasch models. Points represent the results of the $\Sexpr{n_repeats}$ independent MCMC simulations per model. A small amount of horizontal jitter is added to the points. The horizontal lines represent the counts of parameters associated with each model and focus. The $y$-axes vary by focus.}
\label{fig:3-effn}
\end{figure}

Figure~\ref{fig:3-example-ic} provides the estimates for the information criteria values themselves ($-2 \widehat \mathrm{elpd}$). The different conditional information criteria differ from each other for any given model, though they seem to show a similar pattern between models. The high degree of Monte Carlo error in the conditional focus renders differentiating the predictive performance of the models difficult. In the marginal focus the amount of Monte Carlo error is less but still poses a degree of difficulty in making close comparisons. Models~1 and 2 clearly provided poorer predictions in comparison to the others using marginal information criteria, and there is some evidence supporting Model~4 as the best among the candidates.

\begin{figure}
\begin{center}
<<mcerror_plot, fig = TRUE>>=
ggplot(subset(df, variable == "estimate")) +
  aes(y = value, x = as.factor(model)) +
  geom_jitter(height = 0, width = .1, alpha = .25) +
  facet_grid(focus ~ Criterion, scale = "free_y") +
  labs(y = "Information criteria value", x = "Model") +
  my_theme +
  theme(panel.grid.minor.x = element_blank())
@
\end{center}
\caption[Information criteria values for the five latent regression Rasch models]
{Information criteria values ($-2 \widehat \mathrm{elpd}$) for the five latent regression Rasch models. Points represent the results of the $\Sexpr{n_repeats}$ independent MCMC simulations per model. A small amount of horizontal jitter is added to the points. The $y$-axes vary by focus.}
\label{fig:3-example-ic}
\end{figure}

As discussed in their respective sections, WAIC and PSIS-LOO are associated with criteria to support the reliability of their estimates. For WAIC any given point should contribute less than .4 to $\hat p$, and for PSIS-LOO a point should not have a Pareto shape parameter greater than one. As mentioned earlier, ``point'' is defined either as a unit or a cluster depending on whether the focus is conditional or marginal, respectively.  Table~\ref{tab:3-diagnostics} provides the counts of problematic observations by model, averaging over the \Sexpr{n_repeats} repeated MCMC simulations. Conditional WAIC has a small number problematic observations with each model, while marginal WAIC does not exhibit any such issues. No problematic observations are found for either form of PSIS-LOO.

\begin{table}[htb]
\centering
<<diagnostics_table, results = tex>>=
# pk_05 includes counts for pk_10
df_diagnostic <- aggregate(value ~ focus + Criterion + variable + model,
                           subset(df, variable %in% c("p_04", "pk_10")), sum)
df_diagnostic$mean <- df_diagnostic$value / n_repeats
df_diagnostic <- dcast(df_diagnostic, model ~ focus + Criterion, value.var = "mean")
df_diagnostic$blank <- ""
df_diagnostic <- df_diagnostic[, c(1:3, 6, 4:5)]
names(df_diagnostic) <- c("Model", "WAIC", "PSIS-LOO", "", "WAIC", "PSIS-LOO")
xtab_diagnostic <- xtable(df_diagnostic, digits = 1,
                          display = c("d","d","f","f","f","f","f"))
add_row <- list(list(-1),
                paste("\\hline",
                      "& \\multicolumn{2}{c}{Conditional} & ",
                      "& \\multicolumn{2}{c}{Marginal} \\\\",
                      "\\cline{2-3} \\cline{5-6} \n"))
print(xtab_diagnostic, include.rownames = FALSE, floating = FALSE,
      hline.after = c(0, 5),
      add.to.row = add_row,
      sanitize.text.function = function(x) x,
      sanitize.colnames.function = function(x) x)
@
\caption[Counts of problematic observations for WAIC and PSIS-LOO by model]
{Counts of problematic observations for WAIC and PSIS-LOO by model, averaged over the \Sexpr{n_repeats} repeated MCMC simulations. For WAIC, this is the count of observations that contribute more than .4 to $\hat p$, and for PSIS-LOO this is the number of observations having a Pareto shape parameter greater than one. In the conditional focus, observations are defined at the unit level, whereas they are defined at the cluster level for the marginal focus.}
\label{tab:3-diagnostics}
\end{table}


\section{Discussion}

The choice of conditional or marginal focus should depend on the prediction inference to be made, but the marginal approach was found to have some advantages over the conditional approach. Marginal information criteria were found to have less Monte Carlo error and to be more robust in terms of pointwise diagnostics for WAIC and PSIS-LOO. Marginal information criteria are easily obtained for linear models when the cluster-specific parameters to be integrated out are assigned a normal prior. For non-linear models, analysis revealed adaptive Gaussian quadrature to be a viable means of obtaining the necessary marginal likelihoods. The methods described in this chapter may be extended to models with cluster-specific vectors of parameters having a multivariate normal prior.

The preceding analyses also demonstrated the existence of non-ignorable Monte Carlo error in both marginal and conditional WAIC, PSIS-LOO, and DIC, though the issue is substantially worse for the conditional information criteria. Caution is therefore advised when using these methods for model comparison. Disappointingly, the circular block bootstrap did not provide reasonable estimates for Monte Carlo error of information criteria. The approach taken in the applied example of simply rerunning the MCMC simulation many times is cumbersome and merely suggestive of the amount of Monte Carlo error, but it could provide accurate results if the time and computing power is available to conduct a larger number of replications.

The conditional information criteria are more easily obtained than the marginal, as the conditional information criteria depend on quantities easily generated from MCMC software. In fact, BUGS and JAGS provide conditional DIC by default, perhaps accounting for the popularity of DIC.
Researchers may rely on these defaults without an awareness of the marginal alternatives, running the risk of obtaining inappropriate prediction inferences. Further, consideration is not usually given to the degree of Monte Carlo error associated with information criteria, which as demonstrated in the applied example may be substantial even with a large number of weakly correlated posterior draws. In short, the naive application of these techniques leaves a great deal of room for obtaining misleading results.


% \newpage
%
% \section{Notes}
%
% \subsection{Information criteria}
%
% \textcite{spiegelhalter2002bayesian}: ``[I]n hierarchical modelling we cannot uniquely define a `likelihood or `model complexity' without specifying the level of the hierarchy that is the focus of the modelling exercise (Gelfand and Trevisani, 2002)'' (p. 585). ``However,the foregoing discussion suggests that such measures of complexity may not be unique and will depend on the number of parameters in focus. Furthermore, the inclusion of a prior distribution induces a dependence between parameters that is likely to reduce the effective dimensionality, although the degree of reduction may depend on the data that are available'' (p. 585). Section 9.2 (p. 612) briefly discusses invariance to reparmeterization, ``focus'', and integrating out parameters not in the focus.
%
% \textcite{trevisani2003inequalities}: Show that marginal posterior likelihoods are expected to be smaller than conditional (``first stage'') posterior likelihoods, whatever the data and priors.
%
% \textcite{celeux2006deviance}: Describe different versions of DIC for ``missing data'' models: ``observed'' DICs ignore latent variables; ``complete'' DICs are based on joint distribution of response and latent variables; and ``conditional'' DICs resemble those in this chapter. Their $\mathrm{DIC}_7$ seems to correspond to my conditional DIC, and their $\mathrm{DIC}_1$ seems to correspond to my marginal DIC. They refer to both of these as conditional (p. 10).
%
% \textcite{plummer2008penalized}: ``If DIC is regarded as an approximation to the penalized plug-in deviance, then this perspective imposes some important restrictions on its use. A necessary assumption for the penalized plug-in deviance, and hence DIC, is that the data can be broken down into components that are conditionally independent given the parameters in focus. A second important assumption is that the effective number of parameters $p_D$ must be small in relation to the sample size n. When this assumption does not hold, $2p_D$ is a poor approximation to the optimism of the plug-in deviance, and DIC under-penalizes complex models'' (p. 535, discussion section). Second assumption is a problem for conditional DIC. ``The asymptotic justification of DIC using a cross-validation argument is a Bayesian analogue of the result of Stone (1977) on the equivalence of cross-validation and the AIC. Indeed, in the discussion of Spiegelhalter and others (2002), Stone (2002) emphasized the importance of the independence assumption for DIC. The poor empirical performance of DIC compared to crossvalidation when the assumption $p_D \ll n$ does not hold was highlighted by Vehtari and Lampinen (2002)'' (p. 535, discussion section). Otherwise, a lot of stuff on mixture models.
%
% \textcite{spiegelhalter2014deviance}: DIC is not consistent--``the stated aim of DIC is in optimizing short-term predictions of a particular type, and not in trying to identify the `true' model'' (p. 4). DIC uses ``plug-in'' predictions rather than the full predictive distribution, which prevents invariance to reparameterization (p. 4). ``DIC, as with other prediction-based criteria, starts with a particular posterior predictive target based on replicates conditional on certain elements of the design remaining fixed--when we assume a parametric model this replication structure defines the `focus' of the analysis. As implemented in BUGS, DIC takes the lowest level parameters as the focus, but this is only to make the technique computationally feasible'' (p.4). van der Linde (2005, 2012), Plummer (2008) and Ando (2012) have recommended increasing the DIC penalty (p. 5).
%
% \textcite{gelman2014understanding}: ``As is well known in hierarchical modeling (see, e.g., Spiegelhalter et al. 2002; Gelman et al. 2003), the line separating prior distribution from likelihood is somewhat arbitrary and is related to the question of what aspects of the data will be changed in hypothetical replications. In a hierarchical model ... we can imagine replicating new data in existing groups ... or new data in new groups'' (p. 1000). They go on to connect this with PPMC. For WAIC, ``More generally, the adjustment [penalty] can be thought of as an approximation to the number of `unconstrained' parameters in the model'' (p.1003). ``WAIC has the desirable property of averaging over the posterior distribution rather than conditioning on a point estimate... AIC and DIC estimate the performance of the plugin predictive density...'' (p. 1003). ``Bayesian LOO-CV has been proven to asymptotically equal to WAIC (Watanabe 2010)'' (p. 1004). ``Formulas such as AIC, DIC, and WAIC fail in various examples: AIC does not work in settings with strong prior information, DIC gives nonsensical results when the posterior distribution is not well summarized by its mean, and WAIC relies on a data partition that would cause difficulties with structured models such as for spatial or network data. Cross-validation is appealing but can be computationally expensive and also is not always well defined in dependent data settings'' (p. 1015).
%
% \textcite{li2015approximating}: Develop Bayesian cross-validation for models with dependent latent variables;
%
% \textcite{vehtari2016practical}: Pareto smoothing is used in PSIS-LOO because for plain importance sampling the ``resulting estimate is noisy, as the variance of the importance weights can be large or even infinite (Peruggia, 1997, Epifani et al., 2008)'' (p. 1). ``WAIC is fully Bayesian in that it uses the entire posterior distribution, and it is asymptotically equal to Bayesian cross-validation. Unlike DIC, WAIC is invariant to parametrization and also works for singular models'' (p. 2). ``Although WAIC is asymptotically equal to LOO, we demonstrate that PSIS-LOO is more robust in finite case with weak priors or influential observations'' (p. 2). Mention that $y_i$ could be all observations for cluster $i$ rather than single observation, but that PSIS-LOO and WAIC may fail in these cases (p. 8). They don't mention marginalizing the likelihood in this context. Standard errors for PSIS-LOO and WAIC are discussed (p. 19). ``In practice we expect Monte Carlo standard errors to not be so interesting because we would hope to have enough simulations that the computations are stable, but it could make sense to look at them just to check that they are low enough to be negligible compared to sampling error (which scales like $1=n$ rather than $1=S$)'' (p. 20). ``Cross-validation and WAIC should not be used to select a single model among a large number of models due to a selection induced bias as demonstrated, for example, by Piironen and Vehtari (2016)'' (p. 20).
%
%
% \subsection{Monte Carlo error}
%
% \textcite{zhu2000comparing}: Applied three variations on the delta method for assessing the Monte Carlo errors for DIC estimates. All of them underestimated the MC error--in comparison to a brute force method of refitting the model many times to obtain the variance of DIC estimates.
%
% \textcite{mignani2001markov}: Applied block bootstrap to MCMC.
%
% \textcite{politis1994stationary}: Reference for stationary bootstrap.
%
% \textcite{lahiri1999theoretical}: Found that fixed block sizes perform better in terms of mean-squared-error than random block sizes. Later refuted.
%
% \textcite{politis2004automatic} and \textcite{patton2009correction} provide calculations for block sizes in stationary and circular bootstrapping. I guess. Both are very difficult to follow. \textcite{lahiri2007nonparametric} provide a plug-in rule for selecting optimal block lengths, though this requires applying the jackknife on the boostrapped samples.


\printbibliography

\end{document}

