\documentclass{article}
\usepackage[left=1.00in, right=1.00in, top=1.00in, bottom=1.00in]{geometry}

\usepackage[american]{babel}
\usepackage{csquotes}
\usepackage[backend=biber, style=apa]{biblatex}
\DeclareLanguageMapping{american}{american-apa}
\bibliography{../../../Documents/References/references.bib}

\begin{document}
\SweaveOpts{concordance = TRUE, echo = FALSE, height = 3.5}

\author{Daniel C. Furr}
\date{\today}
\title{Frequentist approaches to cross-validation for item-explanatory models}
\maketitle


<<setup>>=
library(ggplot2)
library(xtable)
library(grid)
library(gridExtra)

load("simulation part 2.Rdata")

numword <- function(x, cap = FALSE) {
  words <- c("one", "two", "three", "four", "five", "six", "seven",
             "eight", "nine", "ten", "eleven", "twelve", "thirteen",
             "fourteen", "fifteen", "sixteen", "seventeen", "eighteen",
             "nineteen", "twenty")
  if(x > length(words)) {
    return(x)
  } else if(cap) {
    word <- words[x]
    capped <- paste0(toupper(substr(word, 1, 1)), substr(word, 2, nchar(word)))
    return(capped)
  } else {
    return(words[x])
  }
}

# Function for printing numbers
f <- function(x, digits = 2) {
  formatC(x, digits = digits, big.mark = ",", format = "f")
}

# Function for printing list of numbers in text
and <- function(x, conjunction = "and") {

  l <- length(x)
  if(l == 1) {
    x
  } else if(l == 2) {
    paste(x, collapse = paste0(" ", conjunction, " "))
  } else if(l > 2) {
    part <- paste(x[-l], collapse = ", ")
    paste(part, x[l], sep = paste0(", ", conjunction, " "))
  }
}

x_lab_over_Rsq <- expression(paste("Proportion of explained item variance (",
                                   italic(R)^2, "), ", italic(I) == 32))
x_lab_over_I <- expression(paste("Number of items (", italic(I), "), ",
                                 italic(R)^2 == .6))

my_theme <- theme_bw() +
  theme(text = element_text(family = "serif"),
        strip.background = element_rect(fill = NA, color = NA, size = .5),
        legend.key = element_rect(fill = NA, color = NA))

grid_arrange_shared_legend <- function(..., ncol = length(list(...)), nrow = 1,
                                       position = c("bottom", "right")) {

  plots <- list(...)
  position <- match.arg(position)
  g <- ggplotGrob(plots[[1]] + theme(legend.position = position))$grobs
  legend <- g[[which(sapply(g, function(x) x$name) == "guide-box")]]
  lheight <- sum(legend$height)
  lwidth <- sum(legend$width)
  gl <- lapply(plots, function(x) x + theme(legend.position="none"))
  gl <- c(gl, ncol = ncol, nrow = nrow)

  combined <- switch(position,
                     "bottom" =
                       arrangeGrob(do.call(arrangeGrob, gl), legend, ncol = 1,
                         heights = unit.c(unit(1, "npc") - lheight, lheight)),
                     "right" = arrangeGrob(do.call(arrangeGrob, gl), legend,
                         ncol = 2, widths = unit.c(unit(1, "npc") - lwidth, lwidth                      )))
  # grid.newpage()
  grid.draw(combined)

}
@


\section{Introduction}

% Item explanatory models and prediction
Several models have been developed that account for the factors associated with item difficulty: the linear logistic test model \parencite[LLTM;][]{fischer1973linear}, the linear logistic test model with error \parencite[LLTM-E][]{Janssen2004}, the 2PL-constrained model \parencite{embretson1999generating}, the additive multilevel item structure model \parencite{cho2014additive}, and others. Such models are described as item explanatory models by \textcite{Wilson2004} and lend themselves to the prediction of item difficulties for new items. Predictions regarding new items are useful in automatic item generation, especially in regards to adaptive testing when the goal is to generate an optimally informative item during administration \parencite[see for example,][]{embretson1999generating}. Such a prediction is sometimes referred to as ``precalibration'' \parencite[see for example,][]{gierl2013automatic}. Even when item generation is not automatic, the ability to anticipate the difficulty of new items may be useful in the development of new test forms.

% Prediction-based model selection
The best set of predictors for item difficulty may not be known a priori, and in this case a model selection strategy may be employed to select a preferred model for the prediction of new item difficulties from a set of candidate models. A model selection strategy requires a choice of a score function to evaluate the prediction error, and the deviance ($-2$ times the log-likelihood) is a natural choice. Holdout validation, cross-validation, or information criteria may be used to select a preferred model on the basis of the score function. In holdout validation, a model is estimated on one dataset and then evaluated in a second dataset using the score function. Cross-validation is similar but involves splitting the data multiple times and aggregating the results across splits. The Akaike Information Criterion \parencite[AIC;][]{akaike1974new} is asymptotically equivalent to cross-validation \parencite{stone1977asymptotic} but requires only a single fit of the model. In holdout validation, the estimated prediction error is conditional on the fitted model, whereas cross-validation and AIC approximate the expected prediction error, with the expectation taken over possible training datasets \parencite[][chapter~7]{Hastie2009}.

% Prediction is a good basis of model selection in general
Predictive utility is a good basis for model selection in general even if the goal is not actually prediction per se. In particular, if it is believed that none of the candidate models are true, then the model that best predicts new  data may be justified as the best available approximation to the unknown true model. For example, a researcher may be interested in identifying the factors associated with item difficulty in order to develop or evaluate the theory pertaining to the latent construct if interest. In this way, the purpose of modeling item difficulty may be explanation rather than prediction, but predictive utility may still form the basis for selecting a preferred model. It must be noted that a true model, even if it were available, may not be the most predictive model when it is estimated with finite data, owing to potentially high variance in the parameter estimates. However, this possibility need not be troubling because it is more realistic to consider models as being approximations to a complex reality than as being true data generating mechanisms. Further, it is realistic and appropriate that additional information, in the form of increased amounts of data, should affect judgments about which model best approximates the unknown true model.

% Prediction and clustered data
Obtaining predictions for new data requires consideration of how new observations would arise.
For the case of independent observations, new observations come about in a straightforward way.
For the case of clustered observations, new observations may come from within the existing clusters or instead from within new clusters.
Item response data are more complicated still as the observations are cross-classified, or in other words, responses are clustered simultaneously within persons and items.
As a result, new item responses could arise from any combination of new or same persons and new or same items.
In most applications, predictions are made for the response variable, but with clustered data prediction may also be made for the clusters themselves.
For item response data, this could mean predictions for item difficulties or for person abilities.
This chapter focuses on the prediction of new item difficulties, though the present discussion emphasizes that prediction has the potential to take many forms with item response data.

% With models for clustered data, predictions may be made for new responses or for (the latent variable values of) new clusters. Predictions for responses may be for new responses either from the existing set of clusters or from new clusters. Item response models are a more complicated case than this, given that items and persons are crossed, resulting in two overlapping sets of clusters. For such data, predictions could be made regarding the latent trait for new persons (based on person-related covariates) or for the difficulty of new items (based on item-related covariates). Prediction of new responses may arise from any combination of the same or new persons and the same or new items. This chapter focuses on the prediction of new item clusters.

% Model and likelihood choices
Generalized linear mixed models are commonly used in the analysis of clustered data.
In this framework, models are built from ``fixed'' and ``random'' effects.
Fixed effects are much like standard regression coefficients and are directly estimated.
Fixed effects are usually constant across clusters, while random effects are cluster-specific.
Random effects are not estimated directly, but instead estimation focuses on the parameters of their assumed joint distribution.
In this way, the clusters are treated as sampled from a distribution, implying that a new data collection would entail a new set of clusters.
An alternative modeling strategy is to treat cluster-specific effects as fixed effects, in which case the clusters are fixed rather than random, implying that new observations would arising from the existing set of clusters.

% Model and likelihood choices for IRT
Rasch family item response models, including the LLTM but not the LLTM-E, are readily specified as generalized linear mixed models \parencite{Rijmen2003}.
%and are commonly estimated using marginal maximum likelihood \parencite{Bock1981}.
Items are customarily modeled as fixed effects, for example as item-specific parameters for the Rasch model or as a weighted sum of covariates for the LLTM, and persons are modeled as random effects, perhaps with fixed effects for the mean structure of the person ability distribution \parencite[for example,][]{zwinderman1991generalized, adams1997multilevel}. In this way, common item response models imply that persons are random, and so would be different in new data, and that the items would be the same in new data.
It follows that such a modeling strategy would be well-suited to the selection of a preferred model for predicting person abilities but not for predicting item difficulties.
In particular, information criteria may perform poorly for the LLTM given that they are based on a likelihood that treats the items as fixed, while holdout validation or cross-validation may perform more reasonably.

% The LLTM-E2S as a solution
The LLTM-E treats both the persons and items as random, which is reflected in the likelihood for it. For this reason, information criteria used with the LLTM-E should exhibit correct behavior, unlike for the LLTM. However, given that the model is simultaneously marginal in regard to persons, and the persons and items are crossed, it is infeasible to estimate using marginal maximum likelihood. I propose a two-stage estimation method for the LLTM-E, called the LLTM-E2S, that appropriately treats the items as random.

% This chapter
In this chapter, simulation study results for several model selection strategies are reported for the LLTM and LLTM-E2E. It is expected that holdout validation using a new set of items will yield better item predictions than holdout validation repeating the same set of items. For the LLTM, model selection results using AIC are expected to resemble results from holdout validation with the same items, given that the likelihood for the LLTM treats items as fixed. For the LLTM-E2S, model selection results using AIC are expected to resemble results from holdout validation with new items, given that its likelihood treats items as random.


\section{Models}


\subsection{The linear logistic test model with error}

The data generating model in the simulation study is the LLTM-E:
\begin{equation}
	\Pr(y_{ij} = 1 | x_i, \theta_j) =
	\mathrm{logit}^{-1} \left [ \theta_j - (x_i'\beta + \epsilon_i) \right ]
\end{equation}
\begin{equation}
	\theta_j \sim \mathrm{N}(0, \sigma^2)
\end{equation}
\begin{equation}
	\epsilon_i \sim \mathrm{N}(0, \tau^2)
,\end{equation}
where $y_{ij} = 1$ if person $j$ ($j = 1, \cdots, J$) responded to item $i$ ($i = 1, \cdots, I$) correctly and $y_{ij} = 0$ otherwise.
Latent ability is denoted by $\theta_j$, which follows a normal distribution.
The quantity $x_i'\beta + \epsilon_i$ is a latent regression of item difficulty in which $x_i$ is a vector of item covariates, $\beta$ is a vector of regression coefficients, and $\epsilon_i$ is a residual.
The residual is necessary because it is unrealistic that the item covariates would perfectly account for item difficulty.
Further, $x_i$ is a row from a matrix of item covariates $X$, which will in general include a column with all elements equal to one for the model intercept.
The model may be understood as a generalization of the Rasch model \parencite{Rasch1960a} that decomposes the Rasch difficulty parameters into a structural part ($x_i'\beta$) and residual part ($\epsilon_i$).
Omitting the residual part from the LLTM-E yields the standard LLTM.

Fitting the model using marginal maximum likelihood estimation is infeasible, given the need to integrate over the vectors $\theta$ and $\epsilon$ simultaneously when calculating the marginal likelihood. Maximizing the likelihood is equivalent to minimizing the deviance, which is $-2$ times the log likelihood. The deviance for the LLTM-E is
\begin{equation} \label{eq:lltme-likelihood}
	\mathrm{dev}(y | \hat \omega_m(y)) = -2 \log
		\int \cdots \int \left [
			\prod_{i=1}^I \prod_{j=1}^J
			\Pr(y_{ij} | \hat \beta, \epsilon_i, \theta_j)
			\phi(\epsilon_i ; 0, \hat \tau^2)
			\phi(\theta_j ; 0, \hat \sigma^2)
		\right ] ~\mathrm{d} \epsilon \mathrm{d} \theta
,\end{equation}
where $\phi$ is the normal density function.
The bracketed expression does not simplify and at best may be reduced from $I+J$ to $I+1$ dimensional integrals \parencite{goldstein1987multilevel, rasbash1994efficient}.
In the above equation, $\hat \omega_m(y)$ is shorthand for all estimated parameters ($\hat \beta$, $\hat \sigma$, and $\hat \tau$) for model $m$, which are estimated from data $\{x,y\}$, and the hats on parameters denote marginal maximum likelihood estimates. In the notation $\hat \omega_m(y)$, $x$ is omitted for convenience but would be appropriate to include for completeness.
Also, for the moment it may seem redundant to indicate that the parameter estimates arise from $y$ in the notation $\hat \omega_m(y)$, but this notation will become useful later.


\subsection{The linear logistic test model}

A common model for studying the effects of item covariates, the LLTM \parencite[][]{fischer1973linear}, omits the item residual $\epsilon_i$:
\begin{equation} \label{eq:2-lltm}
	\Pr(y_{ip} = 1 | x_i, \theta_j) =
	\mathrm{logit}^{-1} \left [ \theta_j - x_i'\beta \right ]
\end{equation}
\begin{equation}
	\theta_j \sim \mathrm{N}(0, \sigma^2)
.\end{equation}
Otherwise, the model is the same as the LLTM-E.
The likelihood for the LLTM is marginal over persons but not items. Expressing the likelihood in terms of deviance,
\begin{equation} \label{eq:lltm-likelihood}
	\mathrm{dev}(y | \hat \omega_m(y)) =
	-2 \sum_{j=1}^J \log
	\int
		\left [ \prod_{i=1}^I \Pr(y_{ij} | \hat \beta, \theta_j) \right ]
		\phi(\theta_j ; 0, \hat \sigma^2)
	~\mathrm{d} \theta_j
,\end{equation}
where $\hat \omega_m(y)$ again represents all estimated parameters, this time only $\hat \beta$ and $\hat \sigma$.
Only a one-dimensional integration is involved, and the LLTM is readily fit using marginal maximum likelihood estimation \parencite{bock1981marginal}.
While no closed-form solution exists for the integration due to the logit link function (Equation~\ref{eq:2-lltm}), it is easily approximated by adaptive quadrature \parencite{pinheiro1995approximations, rabe2005maximum}.
As mentioned earlier, the LLTM may be expressed as a generalized linear mixed model and is readily fit in standard software in addition to more specialized software for item response theory models.

\textcite[][p. 232]{Fischer1997} recommended testing the goodness of fit of the LLTM by conducting a likelihood ratio test comparing the LLTM to the Rasch model, suggesting that the LLTM was to be interpreted only if it is not rejected. However, as he admitted, the LLTM will generally be rejected, leaving the researcher with two options: either refrain from studying the sources of item difficulty or interpret the LLTM anyway. The danger in the second option, which he did not identify, is that standard errors for the item predictors will be inappropriately small, often substantially so, when the predictions $x_i' \beta$ fail to replicate the ``complete'' item difficulties $x_i' \beta + \epsilon_i$. This problem directly parallels the situation in multilevel modeling in which a non-hierarchical model is fit to clustered data, and then the omission of cluster-level residuals leads to an overstatement of the amount of information available to estimate coefficients of cluster-level covariates.


\subsection{Two-stage estimation of the  linear logistic test model with error}

To avoid the high-dimensional integral in Equation~\ref{eq:lltme-likelihood}, I propose a two-stage estimation of the LLTM-E, which I will refer to as the LLTM-E2S.
In the first stage, the Rasch model is fit to the data. The Rasch model is
\begin{equation}
	\Pr(y_{ij} = 1 | \delta_i, \theta_j) =
	\mathrm{logit}^{-1} \left [ \theta_j - \delta_i \right ]
\end{equation}
\begin{equation}
	\theta_j \sim \mathrm{N}(0, \sigma^2)
,\end{equation}
where $\delta_i$ is an item-specific difficulty parameter.  Point estimates and standard errors for each $\hat \delta_i$ are obtained by marginal maximum likelihood estimation, minimizing a deviance similar to that in Equation~\ref{eq:lltm-likelihood}. These results are compiled into a constructed dataset of $I$ observations that includes the difficulty estimates, standard errors, and predictors for each item.

In the second stage, the $\hat \delta_i$ are regressed on the item covariates using the constructed data set. This second stage model is
\begin{equation}
	\hat \delta_i = x_i'\beta + u_i + \epsilon_i
\end{equation}
\begin{equation}
	u_i \sim \mathrm{N}(0, \widehat \mathrm{var}(\hat \delta_i))
\end{equation}
\begin{equation}
	\epsilon_i \sim \mathrm{N}(0, \tau^2)
,\end{equation}
where $u_i$ is a residual related to uncertainty in the estimated $\hat \delta_i$, and $\epsilon_i$ is the usual residual in linear regression.
The residual $u_i$ has known variance $\widehat \mathrm{var}(\hat \delta_i)$, which is the square of the estimated standard error for $\hat \delta_i$ obtained in the first stage.
The variance for $\epsilon_i$, $\tau^2$, is a model parameter to be estimated.
This is a random-effects meta-regression model \parencite[see for example][]{raudenbush1985empirical}.
For the LLTM-E2S, let $\hat \omega_m(y)$ represent the set of parameter estimates from the second stage model.
Then the deviance to be minimized in the second stage is
\begin{equation} \label{eq:2-lltme2s-deviance}
	\mathrm{dev}(y | \hat \omega_m(y)) = \sum_{i=1}^I -2 \log
	% \left [
	  \phi(\hat \delta_i ; x_i'\hat\beta, \widehat \mathrm{var}(\hat\delta_i) + \hat\tau^2)
	% \right ]
,\end{equation}
where $\hat \delta_i$ are estimates carried over from the first step, and $\hat\beta$ and $\hat\tau$ are estimates obtained in the second step.
This deviance is suitable only for the selection of an item difficulty model.
Two-stage estimation has been used elsewhere. For example, \textcite{borjas1994two} estimate a probit model with dummy variables for group effects and then regress the estimated group effects on group-level covariates. In this way, their two-stage estimation is similar to the LLTM-E2S except that it is for non-cross-classified hierarchical models.
%Two-stage estimation has been used elsewhere, for example in the regression of fixed-effects estimates of latent variables \parencite{borjas1994two} and in averaging over cluster-specific regressions \parencite{korn1979methods}.
%\emph{((Connect Borjas to this chapter, as the two are similar. Describe Korn more sepecifically.))}


\section{Model selection strategies}


\subsection{Holdout validation}

In holdout validation, a large dataset is split into three parts: the \emph{training}, \emph{validation}, and \emph{evaluation} subsets.
(The evaluation subset may also be referred as the test subset.)
%The fit of models will be evaluated in terms of deviance, which is minus twice the log likelihood, and which may also be referred to as prediction error.
In this process, parameter estimates are obtained for a model by first fitting it to the training subset, and then the fitted model is used to evaluate the score function in the validation subset.
These steps are repeated for each candidate model
In this chapter the deviance is used as the score function, and so models are both estimated and evaluated using the deviance.
The model with the lowest deviance in the validation subset is selected as the best model and then evaluated a second time in the evaluation subset.
The use of a validation subset addresses the bias that would arise from both fitting and evaluating the model on the training subset.
The use of an evaluation subset addresses the bias that would arise from selecting and evaluating a model using the validation subset alone.

This chapter extends the usual holdout validation scheme by considering what elements differ or persist between the three data subsets.
For the case of selecting a model that best predicts item difficulties, the relevant detail is whether the subsets include the same or different items.
Let $y^\mathrm{t}$ be the responses from the training subset.
A validation subset may include the same items as the training subset, and the responses from such a validation subset will be denoted $y^\mathrm{s}$.
Alternatively, a validation subset might include a new set of items, and the responses associated with that training subset will be denoted $y^\mathrm{n}$.
Also, let $y^\mathrm{e}$ be the responses from the evaluation subset, which in this chapter will always contain a set of items distinct from those in the other subsets.
Each of $y^\mathrm{t}$, $y^\mathrm{s}$, $y^\mathrm{n}$, and $y^\mathrm{e}$ are assumed to arise from separate, random samples of persons.
Further, each of $y^\mathrm{t}$, $y^\mathrm{s}$, $y^\mathrm{n}$, and $y^\mathrm{e}$ is associated with item covariates $x^\mathrm{t}$, $x^\mathrm{s}$, $x^\mathrm{n}$, and $x^\mathrm{e}$ respectively, though in general this chapter will omit the item covariates from notation.

A candidate model is selected based on
	$\mathrm{dev}(y^\mathrm{n} | \hat \omega_m(y^\mathrm{t}))$ or
	$\mathrm{dev}(y^\mathrm{s} | \hat \omega_m(y^\mathrm{t}))$,
	depending on the form of the validation subset.
Let $m^*$ represent the model selected from this process.
The deviance for it in the evaluation subset is
	$\mathrm{dev}(y^\mathrm{e} | \hat \omega_{m^*}(y^\mathrm{t}))$,
  where $m^*$ may differ depending on which type of validation subset was used.
In this chapter, the evaluation subset always contains new items; that is, items that are different from those featured in the training and validation subsets.

The estimated prediction error (deviance) in holdout validation is conditional on the particular training data used. This is clear in the notation
	$\mathrm{dev}(y^\mathrm{e} | \hat \omega_{m^*}(y^\mathrm{t}))$,
in which the deviance of the chosen model in the evaluation subset is conditional on parameter estimates obtained from the training subset.
This fact distinguishes holdout validation from the cross-validation methods discussed in the next section.

In summary, two approaches to holdout validation for item prediction are considered in this chapter: one in which the validation subset features the same items as the training subset and one in which the validation subset features new items. Whether the LLTM or LLTM-E2S is used, holdout validation with the same items is expected to perform poorly because the training and validation subsets will be similar, particularly in regards to the items. It may be expected to choose overly complex models too often, as idiosyncrasies related to the items will be repeated in the two subset. Holdout validation with new items is expected to choose the  model with the correct set of item predictors, or when the amount of information in the data is low, a simpler model. The LLTM-E2S may be more successful in this regard than the LLTM, given that its second stage deviance is more targeted toward item prediction than the deviance for the LLTM.


\subsection{Cross-validation and AIC}

If the available data are not abundant enough to support holdout validation, single dataset methods for model selection may be considered instead.
In $k$-fold cross-validation, the data are split into $K$ (approximately) equally sized partitions, most often $K=5$ or 10.
A model is fit to all data \emph{not} in fold $k$, and then the fitted model is evaluated using the score function on the data in fold $k$.
This process is performed for every fold, and the resulting deviances are summed over the folds.
\textcite{Hastie2009} provide a thorough description of $k$-fold cross-validation.
When the observations are clustered, cross-validation may or may not keep the clusters intact, depending on the whether the desired inference requires new clusters.
For item response data, in which item and person clusters are crossed, either the person clusters or the item clusters may be kept intact.
When the goal is to compare models that explain the difficulty of items, folds should be constructed keeping the items clusters intact while breaking up the person clusters.
On the other hand, if the purpose is to compare models with different sets of person covariates, folds should instead keep the person clusters intact.
Asymptotically, the model selected by cross-validation performs as well as the candidate model that minimizes the loss function with respect to the true probability distribution, and this is sometimes referred to as the oracle property \parencite{van2003unified}.

% For item response data, the data may be partitioned into folds defined by persons or by items.
% To compare models that explain the difficulty of items, it is necessary to construct folds based on items.
% On the other hand, comparing models with different sets of person covariates \parencite[see for example][]{Adams1997b} would require folds based on persons.

For models for item prediction, a possibility is to assign each item to its own fold, which may be referred to as leave-one-cluster-out cross-validation (LOCO-CV). Then
\begin{equation}
	\mathrm{LOCO\textnormal{-}CV}_m =
	\sum_{i=1}^I \mathrm{dev}(y_i | \hat \omega_m( y_{-i}))
\end{equation}
where $y_i$ indicates all responses for item $i$, and $y_{-i}$ indicates all responses not associated with item $i$.
The LLTM-E2S is particularly suited to performing LOCO-CV, given that the first stage (fitting the Rasch model) need only be carried out once.
Then the second stage is performed $I$ times per candidate model, leaving one item out and then obtaining a prediction for the left out item difficulty.
These predictions are substituted for $x_i'\hat\beta$ in Equation~\ref{eq:2-lltme2s-deviance} to obtain $\mathrm{LOCO\textnormal{-}CV}_m$.
LOCO-CV could be performed for the LLTM, but it is time-consuming when compared to the LOCO-CV with the LLTM-E2S. For this reason, the simulations that follow only use the LLTM-E2S when performing LOCO-CV.

The Akaike Information Criterion \parencite[AIC;][]{akaike1974new} requires only a single fit of the model and has the form
\begin{equation} \label{eq:aic}
	\mathrm{AIC}_m = \mathrm{dev}(y | \hat \omega_m(y)) + 2q_m
,\end{equation}
where $q_m$ is the count of parameters in model $m$.
AIC is an approximation related to the Kullback-Leibler distance, which is a measure of the information lost when a model is used to approximate the true data generating distribution. Calculating the Kullback-Leibler distance would require knowing the true data generating distribution, but it is possible to approximate the expected \emph{relative} distance
\begin{equation}
	\mathrm{ERD}_m =
	\mathrm{E}_{y^\mathrm{v}} \mathrm{E}_{y^\mathrm{t}}
	  [\mathrm{L}(y^\mathrm{v} |\hat \omega_m(y^\mathrm{t}))]
,\end{equation}
where $\mathrm{L}$ is the log-likelihood rather than deviance, is tractable.
In the above equation, $y^\mathrm{t}$ and $y^\mathrm{v}$ are (hypothetical) independent datasets and the expectations are taken over the true data generating distribution for $y^\mathrm{t}$ and $y^\mathrm{v}$. The difference between the Kullback-Leibler distance and expected relative distance is an unknown constant that is a function only of the true data generating distribution. This constant will be the same for all candidate models because it does not depend on the models. AIC is an approximation to the expected relative distance multiplied by negative two, putting it on the scale of deviance. For models without random effects, AIC is asymptotically equivalent to ``leave-one-observation-out'' cross-validation \parencite{stone1977asymptotic}, and for models with random effects it is asymptotically equivalent to LOCO-CV \parencite{fang2011asymptotic}, at least for linear mixed effects models.

\textcite{Kuha2004} describes two requirements for AIC to be a good approximation of the expected relative distance. First, the sample size is assumed to be large. Corrections for small samples exist but must be derived for every model type. Second, the candidate models are assumed to be true. This is a result of the derivation of AIC; the AIC penalty (two times the number of parameters) is a property of the true distribution. For untrue models, the penalty is biased but has zero variance. ``Other, less biased, estimates for the same quantity exist, but their variances must also be larger. Thus, the constant estimate used in [AIC], besides being trivial to calculate, is likely to have a lower mean squared error than alternatives in many models in which its assumptions are at least roughly satisfied'' \parencite[][p. 208]{Kuha2004}.

\textcite{Vaida2005} demonstrate that, for linear mixed effects models, ``marginal'' AIC (as in Equation~\ref{eq:aic}) assumes that (hypothetical) new datasets would entail a different set of clusters than the original data. Also in the context of linear mixed effects models, \textcite{Greven2010} show that marginal AIC is not asymptotically unbiased, favoring models with fewer random effects, but suggest this may not be a problem in choosing between models that merely have differing fixed effects. In addition, \textcite{Vaida2005} develop a conditional AIC for inferences pertaining to new datasets that would have the same, fixed set of clusters, and this work has been extended by others \parencite{Liang2008, Greven2010, yu2012conditional, yu2013information, saefken2014unifying}. However,  conditional AIC is not suitable for this application.

For linear regression models, a corrected form of AIC is available which adjusts for the finite sample by modifying the penalty \parencite{sugiura1978further, hurvich1989regression}. I apply this corrected AIC to the LLTM-E2S:
\begin{equation} \label{eq:aic-c}
	\mathrm{AIC}_{c,m} = \mathrm{dev}(y | \hat \omega_m(y)) +
	2(q_m+1) \frac{I}{I-q_m-2}
.\end{equation}
The appropriateness of the corrected AIC here is uncertain, as the second stage LLTM-E2S model includes the estimated variances for $\hat \delta$ and as such is not a simple linear regression model. In fact, \textcite[][p. 208]{Kuha2004} notes that a disadvantage of corrected AIC is that the particular adjustments will differ across types of models. Nonetheless, the corrected AIC is expected to be more accurate than standard AIC and so will be used in the simulation study. Corrected AIC has been used with meta-regression in other contexts \parencite[see for example,][]{knowles2009elevated, jones2009environmental, chen2012meta}, and auth. Also, \textcite{Vaida2005} provide a similar correction for linear mixed effects models, but this correction is not used with the LLTM because the correction may not apply to logistic models and also because it would have almost no impact, as the sample size under the LLTM is large ($I \times P$).

As the deviance for the LLTM is marginal only over persons and not over items, or in other words treats persons but not items as random, AIC is expected to perform similarly to holdout validation with the same items for the LLTM. By extension, it is unlikely to be effective for selecting models for item prediction. For the LLTM-E2S, $\mathrm{AIC}_c$ is expected to perform similarly to holdout validation with new items, given that the LLTM-E deviance is marginal in regards to items. Results for LOCO-CV with the LLTM-E2S are expected to more closely match those of holdout validation with new items than $\mathrm{AIC}_c$, given that it does not rely on assumptions regarding the appropriate penalty.


\subsection{BIC and likelihood ratio testing}

For completeness, two other model selection strategies are considered despite the fact that they are not motivated by prediction. First, the Bayesian Information Criterion \parencite[BIC;][]{schwarz1978estimating} is
\begin{equation}
	\mathrm{BIC}_m = \mathrm{dev}(y | \hat \omega_m(y)) + q_m \log N
,\end{equation}
where $N$ is the count of observations. For the LLTM approach $N = I \times P$, while for the two stage approach $N = I$. The penalty for BIC is based on an approximation to the Bayes factor for an assumed multivariate normal prior distribution with means equal to the parameter estimates and a covariance matrix that is as informative as one observation \parencite[][p. 196]{Kuha2004}. The model with the lowest $\mathrm{BIC}_m$ is preferred.

Second, the likelihood ratio test is based on hypothesis testing and is suitable only for comparing nested models. Let $\Delta_\mathrm{dev}$ be the difference in deviance between two models, and let $\Delta_q$ be the difference in the number of parameters. Then the likelihood ratio test statistic is
$\chi^2(\Delta_q) = \Delta_\mathrm{dev}$,
and most often a p-value less than .05 is deemed statistically significant. If the likelihood ratio test provides a statistically significant result, the simpler model is rejected in favor of the more complex one. When multiple comparisons are needed, the comparisons may be made in ordered pairs. For example, the simplest model may be compared against the second simplest, and if the likelihood ratio test rejects the simplest model, then the second simplest is then compared against the third simplest, and so on. As such, the researcher selects the simplest unrejected model in the end.


\section{Simulation}


\subsection{Simulation study design}

In each replication of the simulation, the LLTM-E is used to generate a training  subset, a same items validation subset, a new items validation subset, and an evaluation subset. The two forms of holdout validation (using the same items versus new items) are performed for competing models using both the LLTM and LLTM-E2S. In this way the simulation is a $2 \times 2$ (holdout validation type by modeling strategy) design.
The simulation replications track which model is selected and the score function values,
	$\mathrm{dev}(y^\mathrm{e} | \hat \omega_{m}(y^\mathrm{t}))$,
for for the competing models.
In addition, model selection is also preformed using AIC, BIC, LOCO-CV, and likelihood ratio testing using only the training subset.

The fixed part of the data generating model for the item difficulties is
\begin{equation}
	x_i'\beta \equiv
	\beta_1 x_{1i} + \beta_2 x_{2i} + \beta_3 x_{3i} + \beta_4 x_{4i} +
	\beta_5 x_{2i} x_{3i}
,\end{equation}
which includes constant $x_{1i} = 1$, covariates $x_{2i}$, $x_{3i}$, $x_{4i}$, and one interaction $x_{2i} x_{3i}$.
The predictors $x_{2i} \cdots x_{4i}$ are independent draws from a standard normal distribution.
A key feature of the generated datasets (and data of this type more generally) is the extent to which the item covariates account for the item difficulties. Let $\upsilon^2$ represent the variance of the structural part of item difficulty ($x_i'\beta$).
Then
\begin{equation}
	R^2 = \frac{\upsilon^2}{\upsilon^2 + \tau^2}
\end{equation}
represents the proportion of item variance accounted for by the item predictors.
For the simulation, let the total item variance be $\tau^2 + \upsilon^2 = 1.25$, and let each of $\beta_2 \cdots \beta_5$ equal the same value.
Then for a given value of $R^2$, then variance of the structural part of item difficulty is $\upsilon = \sqrt{1.25 R^2}$.
Given the above,
$\tau = \sqrt{1.25 - \upsilon^2}$ and
$\beta_2 \cdots \beta_5 = \frac{\upsilon}{2}$.
The remaining parameters are the intercept, which is set to $\beta_1=0$, and the person variance, which is set to $\sigma^2=1.25$.
In short, the simulation is designed such that the proportion of item variance accounted for by the predictors, $R^2$, may be varied while maintaining the same the same total item variance.

In the first of two simulation branches, values for $R^2$ are manipulated, $R^2 \in \{.3, .6, .9\}$, while the number items $I=32$ is held constant.
These generating values are provided in the first part of Table~\ref{tab:2-conditions}.
In the second simulation branch, the number of items is varied, $I \in \{16, 32, 64\}$, while $R^2$ is held at .6, and these generating values are depicted in the second part of Table~\ref{tab:2-conditions}.
The condition in which $R^2=.6$ and $I=32$ is duplicated in the two branches, and so there are really five conditions rather than six.
In all conditions in both branches, the number of persons is $P=500$.
A total of five hundred replications are carried out for each condition.

\begin{table}
\centering
<<conditions, results = tex>>=
conditions <- conditions_df
conditions$b1 <- 0
conditions <- conditions[c(1:3,4,2,5), ]
conditions$Branch <- c("1", "", "", "2", "", "")
conditions <- conditions[, c("Branch", "Rsq", "nitems", "npersons", "b1", "b",
                             "tau", "upsilon", "sigma")]
names(conditions) <- c("Branch", "$R^2$", "$I$", "$P$", "$\\beta_1$",
                       "$\\beta_2 \\cdots \\beta_5$", "$\\tau$", "$\\upsilon$",
                       "$\\sigma$")
xtab_conditions <- xtable(conditions, digits = c(0,0,1,0,0,2,2,2,2,2))
print(xtab_conditions, floating = FALSE, include.rownames = FALSE,
      hline.after = c(-1,0,3,nrow(xtab_conditions)),
      sanitize.colnames.function = function(x) x)
@
\caption[Generating values for parameters across the simulation conditions]
{Generating values for parameters across the simulation conditions. In the first simulation branch, the proportion of explained item variance ($R^2$) is varied while the number of items ($I$) is fixed. In the second branch, $I$ is varied while $R^2$ is fixed. The condition in which $R^2=.6$ and $I=32$ is duplicated in the two branches.}
\label{tab:2-conditions}
\end{table}

Three competing models are subjected to the various model selection strategies.
Model~1 includes the main effects only:
\begin{equation}
	x_i'\beta \equiv
	\beta_1 x_{1i} + \beta_2 x_{2i} + \beta_3 x_{3i} + \beta_4 x_{4i}
.\end{equation}
Model~2 matches the data generating model in terms of $x_i'\beta$.
Model~3 includes an extra interaction:
\begin{equation}
	x_i'\beta \equiv
	\beta_1 x_{1i} + \beta_2 x_{2i} + \beta_3 x_{3i} + \beta_4 x_{4i} +
	\beta_5 x_{2i} x_{3i} + \beta_6 x_{2i} x_{4i}
.\end{equation}
Using the LLTM-E2s with Model~2 matches the data generating model, while using the LLTM with Model~2 does not because the LLTM does not include the item residual.


\subsection{Parameter recovery and standard error estimates}

Parameter recovery is investigated for both the LLTM and LLTM-E2S. Results for the LLTM-E2S are of interest as confirmation that the method works, while results for the LLTM are of interest because the misspecification of the LLTM may lead to biased parameter estimates.
This bias is assessed by the difference between the estimated and generating parameters across simulation replications.
For this purpose, I focus on estimation results $\hat \omega(y^\mathrm{t})$ for Model~2 because the fixed part matches that of the generating model.
Figure~\ref{fig:2-bias} presents estimates of bias (the mean of differences) with 95\% confidence intervals ($\pm 1.96 \frac{\mathrm{sd}}{\sqrt{500}}$).
The results are based on 500 replications.

For the LLTM, there is evidence of downward bias in the coefficient estimates ($\hat\beta_2 \cdots \hat\beta_5$) in all of the simulation conditions, and in relation to the magnitude of these coefficients ($\beta_2 \cdots \beta_5 = .58$), the bias is often substantial.
However, no such problem is seen for the estimated intercept ($\hat \beta_1$).
Because of the absence of item residuals, the LLTM is like a population-average model in regards to the items, which is known to exhibit attenuated coefficients for the logistic case \parencite{ritz2004equivalence}.
The estimate of the person standard deviation ($\hat \sigma$) also exhibits a downward bias that depends on $R^2$ (or by extension, $\tau$).
For the LLTM-E2S, there is no systematic evidence for bias in $\hat \beta$, though there is a downward bias in $\hat \tau$ that is mitigated in the high information conditions.
The bias in $\hat \tau$ may be alleviated by using restricted maximum likelihood estimation or more recent estimators \parencite[see for example,][]{viechtbauer2005bias}, but for simplicity and speed, maximum likelihood estimation is used in the simulation.

\begin{figure}[tb]
\begin{center}
<<bias_plot, fig = TRUE, height = 6>>=
bias_labels <- list()
bias_pch <- numeric()
bias_labels[[1]] <- quote(beta[1])
bias_pch[1] <- 18
for(i in 2:n_betas[2]) {
  bias_labels[[i]] <- bquote(beta[.(i)])
  bias_pch[i] <- 16
}
bias_labels[[n_pars[2]]] <- bquote(sigma)
bias_pch[[n_pars[2]]] <- 15
bias_labels[[n_pars[2]+1]] <- bquote(tau)
bias_pch[[n_pars[2]+1]] <- 17

expand_bias <- range(df_bias[, c("lower", "upper")])

bias_1 <- ggplot(subset(df_bias, over_Rsq)) +
  aes(x = as.factor(Rsq), y = mean, ymin = lower, ymax = upper,
      color = factor(par), pch = factor(par)) +
  geom_hline(yintercept = 0) +
  geom_linerange(position = position_dodge(width = 0.75)) +
  geom_point(size = 2, position = position_dodge(width = 0.75)) +
  facet_grid(. ~ Method) +
  xlab(x_lab_over_Rsq) +
  labs(list(y = "Bias", color = NULL, pch = NULL)) +
  expand_limits(y = expand_bias) +
  my_theme +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank()) +
  scale_shape_manual(values = bias_pch, labels = bias_labels) +
  scale_colour_brewer(palette = "Dark2", labels = bias_labels)

bias_2 <- ggplot(subset(df_bias, over_nitems)) +
  aes(x = as.factor(nitems), y = mean, ymin = lower, ymax = upper,
      color = factor(par), pch = factor(par)) +
  geom_hline(yintercept = 0) +
  geom_linerange(position = position_dodge(width = 0.75)) +
  geom_point(size = 2, position = position_dodge(width = 0.75)) +
  facet_grid(. ~ Method) +
  xlab(x_lab_over_I) +
  labs(list(y = "Bias", color = NULL, pch = NULL)) +
  expand_limits(y = expand_bias) +
  my_theme +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank()) +
  scale_shape_manual(values = bias_pch, labels = bias_labels) +
  scale_colour_brewer(palette = "Dark2", labels = bias_labels)

grid_arrange_shared_legend(bias_1, bias_2, ncol = 1, nrow = 2,
                           position = "right")
@
\end{center}
\caption[Bias (mean of parameter estimates minus generating values) with 95\% confidence intervals for Model~2]
{Bias (mean of parameter estimates minus generating values) with 95\% confidence intervals for Model~2. Results are for 500 simulation replications per condition. The LLTM does not include estimation of $\tau$, and the LLTM-E2S does not include estimation of $\sigma$ in the second stage.}
\label{fig:2-bias}
\end{figure}

The LLTM and LLTM-E2S provide very different standard error estimates.
To illustrate, I focus on $\beta_6$ for Model~3. Because $\beta_6 = 0$ in data generation,
  $\frac{\hat \beta_6}{\mathrm{se}(\hat \beta_6)}$
  should follow a standard normal distribution across simulation     iterations if the standard error estimates are correct.
Figure~\ref{fig:2-qqplot} presents Q-Q plots of the observed
  $\frac{\hat \beta_6}{\mathrm{se}(\hat \beta_6)}$
  against the quantiles of the standard normal distribution.
In all conditions, values for
  $\frac{\hat \beta_6}{\mathrm{se}(\hat \beta_6)}$
  for the LLTM deviate greatly from the expected results from a standard normal distribution and indicate that the estimated standard errors are too small.
In contrast, the LLTM-E2S shows no such problem, conforming to the appropriate distribution.

Clearly the LLTM yields inappropriately small standard errors, which is a result of the omission of item residuals.
For example, the mean standard error for $\hat \beta_6$ was
\Sexpr{f(mean(subset(df_qq, Rsq==.6 & nitems==32 & Method=="LLTM", se)[,1]))}
for the LLTM but
\Sexpr{f(mean(subset(df_qq, Rsq==.6 & nitems==32 & Method=="LLTM-E2S", se)[,1]))}
for the LLTM-E2S in the simulation condition in which $R^2=.6$ and $I=32$.
As mentioned earlier, \textcite[][p. 232]{Fischer1997} recommended conducting a likelihood ratio test comparing the LLTM and Rasch model as a goodness of fit test.
Failure to reject the LLTM would imply that $\tau$ is about zero, and then it may be that the LLTM would yield approximately correct standard errors.
However, this is an improbable scenario as it requires perfect predictors for item difficulty.
% In every replication of this simulation, though, the likelihood ratio test rejected Model~2 when compared to the Rasch model, which includes replications in which the item covariates account for about 90\% of item variance.

\begin{figure}[tb]
\begin{center}
<<qq_plot, fig = TRUE, height = 5>>=
expand_qq <- range(df_qq$z)

qq_1 <- ggplot(subset(df_qq, over_Rsq)) +
  aes(sample = z) +
  geom_point(stat = "qq", size = .5) +
  geom_abline(slope = 1, intercept = 0) +
  xlab("Normal theoretical quantiles") + ylab("Observed quantiles") +
  facet_grid(Rsq + nitems ~ Method, labeller =
             label_bquote(rows = list(italic(R)^2 == .(Rsq),
                                      italic(I) == .(nitems)) )) +
  expand_limits(y = expand_qq) +
  my_theme

qq_2 <- ggplot(subset(df_qq, over_nitems)) +
  aes(sample = z) +
  geom_point(stat = "qq", size = .5) +
  geom_abline(slope = 1, intercept = 0) +
  xlab("Normal theoretical quantiles") + ylab("") +
  facet_grid(Rsq + nitems ~ Method, labeller =
             label_bquote(rows = list(italic(R)^2 == .(Rsq),
                                      italic(I) == .(nitems)) )) +
  expand_limits(y = expand_qq) +
  my_theme

grid.arrange(qq_1, qq_2, nrow = 1)
@
\end{center}
\caption[Q-Q plots for the observed $\frac{\hat \beta_6}{\mathrm{se}(\hat \beta_5)}$ for Model~3 across simulation iterations versus standard normal quantiles]
{Q-Q plots for the observed $\frac{\hat \beta_6}{\mathrm{se}(\hat \beta_5)}$ for Model~3 across simulation iterations versus standard normal quantiles.
On the left are results for simulation replications in which $\tau$ varies, and on the right are results for replications in which the number of items vary. Because $\beta_6 = 0$ in data generation, $\frac{\hat \beta_6}{\mathrm{se}(\hat \beta_6)}$ should follow a standard normal distribution across simulation iterations if the standard errors are correct. The lines have an intercept of zero and slope of one, indicating where the points should lay if the standard error estimates are correct.}
\label{fig:2-qqplot}
\end{figure}


\subsection{Model selection}

In describing model selection results, it is useful to consider the relative amount of information about the regression coefficients contained in the simulated datasets.
Larger values for $R^2$ are associated with with item covariates that are strong predictors, so $R^2 = .9$ is a ``high information'' condition.
Also, when the number of items is large, more precise estimates of $\beta$ are possible, so $I = 64$ is also a high information condition.
Conversely, $R^2 = .3$ and $I = 16$ are ``low information'' conditions.
It is expected that Model~2 should provide the best predictions given that it matches the generating model, though there is no guarantee of this with finite data.
In high information conditions, it is expected that Model~2 will tend to be selected using holdout validation, LOCO-CV, or AIC because enough information should be available to obtain good parameter estimates.
In low information conditions, the simpler Model~1 may be selected more often, as estimates for its fewer parameters may be more stable compared to those in larger models.
This is a matter of bias-variance trade off \parencite[][p. 223]{Hastie2009}, and the expected difference in selection between high and low information conditions is correct behavior as the goal is to identify the most predictive model rather than a true model.
Lastly, there is no condition in which Model~3, the most complex model, should tend to be favored, though random variation in the generated datasets are expected to lead to it being selected some small number of times.

% Selecting true model is not really the criteria.
% It is important to note that judging the selection strategies simply by how often they select the a ``true'' model is somewhat inappropriate for holdout validation, cross-validation, and AIC. The reason is that these methods are designed to select the model that best predicts new data, and the true model is only guaranteed to be the best predictor asymptotically (as sample size approaches infinity). An overly simple model may often yield better predictions in finite data, especially when some aspects of the true model are poorly estimated. This is a result of the the bias-variance trade off \parencite[][p. 223]{Hastie2009}. On the other hand, selecting an overly complex model should be a chance occurrence. For these reasons, the selection results do not (directly) indicate which approaches yield the best predictions.

% HV is similar for LLTM and LLTM-E2S. HV with new items is better than HV with same items. Performance of HV.
Figure~\ref{fig:2-selection-Rsq} provides the model selection results for the simulation conditions in which $R^2$ is varied, and Figure~\ref{fig:2-selection-nitems} shows the same for conditions in which the number of items is varied. Holdout validation using new items performs similarly between the LLTM (left side of both figures) and the LLTM-E2S (right side). In addition, this method selects Model~2 the majority of times in all conditions, though Model~1 is selected with increasing frequency in low information conditions. Holdout validation using the same items also behaves similarly between the LLTM and LLTM-E2S, but for this approach Model~3 is chosen the large majority of times in all simulation conditions. This result is problematic but not surprising; idiosyncrasies that arise from a particular set of item residuals in the training subset are repeated again in the validation subset, and so the two subsets are very similar. In this way the overly complex Model~3 is provided an opportunity to capitalize on chance. Clearly, holdout validation for item prediction must feature datasets with different sets of items in order to be effective.

\begin{figure}[tb]
\begin{center}
<<select_tau_plot, fig = TRUE, height = 7>>=
ggplot(subset(percents_df, over_Rsq)) +
  aes(as.factor(Rsq), selected, fill = factor(model)) +
  geom_bar(stat = "identity", position = "dodge") +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank()) +
  xlab(x_lab_over_Rsq) +
  labs(y = "Percentage of times selected", fill = "Model") +
  facet_grid(Selector ~ Method) +
  my_theme +
  theme(panel.grid.minor.x = element_blank(),
        panel.grid.major.x = element_blank())
@
\end{center}
\caption[Percentages of times each model was chosen for simulation replications in which the proportion of explained item variance is varied]
{Percentages of times each model was chosen by the various selection criteria for simulation replications in which the proportion of explained item variance ($R^2$) is varied. Standard AIC is used for the LLTM, while $\mathrm{AIC}_c$ is used for the LLTM-E2S. LOCO-CV was applied only to the LLTM-E2S.}
\label{fig:2-selection-Rsq}
\end{figure}

\begin{figure}[tb]
\begin{center}
<<select_nitems_plot, fig = TRUE, height = 7>>=
ggplot(subset(percents_df, over_nitems)) +
  aes(factor(nitems), selected, fill = factor(model)) +
  geom_bar(stat = "identity", position = "dodge") +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank()) +
  xlab(x_lab_over_I) +
  labs(list(y = "Percentage of times selected", fill = "Model")) +
  facet_grid(Selector ~ Method) +
  my_theme +
  theme(panel.grid.minor.x = element_blank(),
        panel.grid.major.x = element_blank())

# In addition, calculate AIC_c penalties per parameter
df_aic <- data.frame(n_obs = rep(unique(conditions_df$nitems), times = 3),
                n_pars = rep(n_pars[1:3], each = 3))
df_aic$pen <- aic_c(0, df_aic$n_obs, df_aic$n_pars)
df_aic$per <- df_aic$pen / df_aic$n_pars
@
\end{center}
\caption[Percentages of times each model was chosen for simulation replications in which the number of items is varied]
{Percentages of times each model was chosen by the various selection criteria for simulation replications in which the number of items is varied. Standard AIC is used for the LLTM, while $\mathrm{AIC}_c$ is used for the LLTM-E2S. LOCO-CV was applied only to the LLTM-E2S.}
\label{fig:2-selection-nitems}
\end{figure}

% For LLTM, AIC corresponds to HV with same items. Also, LR and BIC.
Focusing now on the LLTM, AIC performs similarly to holdout validation with the same items in terms of model selection (Figures~\ref{fig:2-selection-Rsq} and \ref{fig:2-selection-nitems} again) and quite differently from holdout validation with new items. This supports the argument that using AIC with the deviance from the LLTM corresponds to an inference involving the same set of items. The likelihood ratio test is more conservative, that is, tends to prefer simpler models, when compared to AIC. Assuming an alpha level of .05, a model would have to have a deviance 3.8 lower than a competing model with one fewer parameter in order to reject the simpler model, compared to a difference in deviance of 2 for AIC. BIC is more conservative still with a penalty ranging from
\Sexpr{and(f(log(range(df_bias$nitems) * 500), 2), "to")}
per parameter, depending on the number of items. The relative conservatism of the likelihood ratio test and BIC are noticeable in the two figures. However, AIC, BIC, and the likelihood ratio test for the LLTM all bear resemblance to holdout validation with the same items and too frequently select the overly complex Model~3.

% For LLTM-E2S, AIC corresponds to HV with new items. Also, LR and BIC.
In contrast, $\mathrm{AIC}_c$ paired with the LLTM-E2S performs similarly to holdout validation with new items in terms of model selection (Figures~\ref{fig:2-selection-Rsq} and \ref{fig:2-selection-nitems} again) and is generally more apt to select Model~2 than holdout validation with new items.
Results for LOCO-CV with the LLTM-E2S are similar to those for AIC, as expected.
For these data and models, $\mathrm{AIC}_c$ applies penalties ranging from
\Sexpr{and(f(range(df_aic$per), 2), "to")}
per parameter;
larger penalties occur with greater numbers of parameters and with smaller numbers of items.
The likelihood ratio test implies a penalty of 3.8 again (assuming the models differ by one parameter) and BIC implies penalties ranging from
\Sexpr{and(f(log(range(df_bias$nitems)), 2), "to")}
per parameter, depending on the number of items.
The penalties suggested by $\mathrm{AIC}_c$ may be higher or lower than for BIC or the likelihood ratio test.
Unlike for the LLTM, $\mathrm{AIC}_c$, BIC, and the likelihood ratio test appear more like holdout validation with new items when paired with the LLTM-E2S, and the three perform as expected in model selection.


\subsection{Implied penalties associated with holdout validation}

Standard AIC features a penalty to
  $\mathrm{dev}(y^\mathrm{t} | \hat \omega_m(y^\mathrm{t}))$
  that is a function of the number of model parameters.
Let this be $d_\mathrm{AIC} = 2q$. In this context, $d_\mathrm{AIC}$ is an asymptotic approximation for
\begin{equation}
	d_{\mathrm{HV}} = \mathrm{E}_{y^\mathrm{e}} \mathrm{E}_{y^\mathrm{t}}
	\left [
		\mathrm{dev}
		  \left (y^\mathrm{e} | \hat \omega_m(y^\mathrm{t}) \right ) -
		\mathrm{dev}
		  \left (y^\mathrm{e} | \hat \omega_m(y^\mathrm{e}) \right )
	\right ]
,\end{equation}
which is the expected difference between the holdout validation deviance and the deviance obtained from both fitting and evaluating the model using the evaluation subset.
In effect, $d_{\mathrm{HV}}$ is the correct but unknown penalty.
An empirical estimate for it may be obtained from the simulation as
\begin{equation}
	\hat d_{\mathrm{HV}} = \frac{1}{R} \sum_{r=1}^{R}
	\left [
		\mathrm{dev}
		  \left (y^{\mathrm{e},r} | \hat \omega_m(y^{\mathrm{t},r}) \right ) -
		\mathrm{dev}
		  \left (y^{\mathrm{e},r} | \hat \omega_m(y^{\mathrm{e},r}) \right )
	\right ]
,\end{equation}
where $r$ indexes the $R$ simulation replications. In a large sample, $d_\mathrm{AIC}$ should approximate $d_{\mathrm{HV}}$ well.
Also, let $d_{\mathrm{AIC}_c}$ be the penalty associated with $\mathrm{AIC}_c$, as in Equation~\ref{eq:aic-c}. This penalty should approximate $d_{\mathrm{HV}}$ well in smaller samples.
Last, the penalty implied by LOCO-CV may be estimated as
\begin{equation}
	\hat d_{\mathrm{CV}} =
	\frac{1}{R} \sum_{r=1}^{R}  \left [
  	\left [ \sum_{i=1}^I
  		\mathrm{dev} \left (
  		  y_i^{\mathrm{t},r} | \hat \omega_m(y_{-i}^{\mathrm{t},r}) \right )
  	  \right ] -
  	\mathrm{dev} \left (
  	  y^{\mathrm{t},r} | \hat \omega_m(y^{\mathrm{t},r})
  	\right )
	\right ]
\end{equation}
which should also approximate $d_{\mathrm{HV}}$ well when the sample is large.

Figure~\ref{fig:2-penalty} displays the different estimated and calculated penalties across simulation conditions.
For the LLTM, the $\hat d_{\mathrm{HV}}$ is much greater than $d_\mathrm{AIC}$ across all simulation conditions.
Clearly the penalty implied by AIC is incorrect for the LLTM, given that that $d_\mathrm{AIC}$ and $\hat d_{\mathrm{HV}}$ bear no resemblance.
Interestingly, $\hat d_{\mathrm{HV}}$ for the LLTM appears to depend on $R^2$, and if $R^2$ were near one (making the LLTM the correct model), it may be that $\hat d_{\mathrm{HV}}$ would approximately equal $d_\mathrm{AIC}$ for the LLTM.
It is clear that $\hat d_{\mathrm{HV}}$ depends on $R^2$, but $\mathrm{AIC}_c$ for the LLTM-E2S fails to capture this phenomenon. When $R^2=.9$ or $I=16$, $\mathrm{AIC}_c$ differs substantially from $\hat d_{\mathrm{HV}}$, but otherwise $\mathrm{AIC}_c$ appears to reasonably approximate $\hat d_{\mathrm{HV}}$. On the other hand, $\hat d_{\mathrm{CV}}$ is quite close to $\hat d_{\mathrm{HV}}$, though less so when $I=16$.

\begin{figure}[tb]
\begin{center}
<<penalty_plot, fig = TRUE, height = 8>>=
p_est_1 <- ggplot(subset(df_pen, over_Rsq)) +
  aes(y = p, x = Rsq, color = Model, pch = Model) +
  geom_line() + geom_point() +
  facet_grid(method~Type, scales = "free_y") +
  xlab(x_lab_over_Rsq) +
  ylab("Estimated penalty") +
  scale_x_continuous(breaks = unique(df_pen$Rsq)) +
  my_theme +
  theme(panel.grid.minor = element_blank())
p_est_2 <- ggplot(subset(df_pen, over_nitems)) +
  aes(y = p, x = nitems, color = Model, pch = Model) +
  geom_line() + geom_point() +
  facet_grid(method~Type, scales = "free_y") +
  xlab(x_lab_over_I) +
  ylab("Estimated penalty") +
  scale_x_continuous(breaks = unique(df_pen$nitems)) +
  my_theme +
  theme(panel.grid.minor = element_blank())
grid_arrange_shared_legend(p_est_1, p_est_2, ncol = 1, nrow = 2,
                           position = "bottom")
@
\end{center}
\caption[Estimated penalties implied by holdout validation with new items, AIC, and LOCO-CV]
{Estimated penalties implied by holdout validation with new items ($\hat d_{\mathrm{HV}}$), AIC ($d_\mathrm{AIC}$ or $d_{\mathrm{AIC}_c}$), and LOCO-CV ($\hat d_{\mathrm{CV}}$) for the LLTM and LLTM-E2S. The $y$-axes vary. Standard AIC is used for the LLTM, while $\mathrm{AIC}_c$ is used for the LLTM-E2S. LOCO-CV was not performed with the LLTM.}
\label{fig:2-penalty}
\end{figure}


\subsection{Comparison of predictive performance}

% Two ways of assessing the accuracy of item predictions may be considered.
The root mean squared error of prediction for model $m$ is
\begin{equation}
	\mathrm{RMSEP}_m =
	\sqrt{\frac{1}{I} \sum_{i=1}^I
	  \left [
	    x_i^{\mathrm{e} \prime}
	    \hat\beta_m(x^{\mathrm{t}}, y^{\mathrm{t}}) -
	    % \delta_i(y^\mathrm{e})
	    \delta_i^\mathrm{e}
	  \right ]^2}
,\end{equation}
where $x_i^{\mathrm{e} \prime}$ is a vector of item predictors associated with the evaluation data, $\hat\beta_m(x^{\mathrm{t}}, y^\mathrm{t})$ is a vector of coefficients for model $m$ estimated on the training subset, and $\delta_i^\mathrm{e}$ is a known item difficulty (fixed plus residual parts) associated with the evaluation subset.
Here the item predictors $x$ are brought into the notation to emphasize that the coefficients are trained with $x^{\mathrm{t}}$ (and $y^\mathrm{t}$), but predictions for $\delta_i^\mathrm{e}$ are made with $x^{\mathrm{e}}$.
The $\mathrm{RMSEP}_m$ is based on the difference between predicted and actual item difficulties for new data.
In a best case scenario,
  $x_i^{\mathrm{e} \prime} \hat\beta_m(x^{\mathrm{t}}, y^{\mathrm{t}})$
  may fully account for the fixed part of $\delta_i^\mathrm{e}$, but it cannot account for the residual part.
In this way, the residual standard deviation ($\tau$) is the best value that may be expected for $\mathrm{RMSEP}_m$.
As $\mathrm{RMSEP}_{m}$ relies on known $\delta_i^\mathrm{e}$, it is only available in a simulation context, though an alternative could be based on estimates $\hat \delta_i^\mathrm{e}$ from the Rasch model.

Figure~\ref{fig:2-rmse-m} presents the mean $\mathrm{RMSEP}_{m}$ for each model, and $\tau$ is indicated by the dashed lines.
In general, Models~2 and 3 both produce predictions that are close to $\tau$, while Model~1 performs more poorly.
The exceptions are the low information conditions, in which case Models~1 and 3 perform similarly.
Further, let $\mathrm{RMSEP}_{m^*}$ be the root mean squared error of prediction for the model chosen by a given selection strategy.
Then Figure~\ref{fig:2-rmse} presents the mean of $\mathrm{RMSEP}_{m^*}$ across simulation replications for several selection strategies.
The differences between selection strategies and between the LLTM and LLTM-E2S makes little difference in $\mathrm{RMSEP}_{m^*}$.
Part of the reason for the similarity is that the differences between competing models in regards to $\mathrm{RMSEP}_{m}$ is small.

\begin{figure}[tb]
\begin{center}
<<rmse_m_plot, fig = TRUE, height=6>>=
df_rmse_m <- merge(df_rmse_m, conditions_df)
df_rmse_m_expand <- range(df_rmse_m[, c("value", "tau")])
rmse_m_1 <- ggplot(subset(df_rmse_m, over_Rsq)) +
  aes(y=value, x = Rsq, color = Model, pch = Model) +
  geom_point() + geom_line() +
  geom_line(mapping = aes(y = tau), show.legend = FALSE,
            col = "black", lty = "dashed") +
  facet_wrap(~Method) +
  xlab(x_lab_over_Rsq) +
  ylab(expression(paste("Mean ", RMSEP[m]))) +
  scale_x_continuous(breaks = unique(df_rmse_m$Rsq)) +
  expand_limits(y = df_rmse_m_expand) +
  my_theme +
  theme(panel.grid.minor.x = element_blank())
rmse_m_2 <- ggplot(subset(df_rmse_m, over_nitems)) +
  aes(y=value, x = nitems, color = Model, pch = Model) +
  geom_point() + geom_line() +
  geom_line(mapping = aes(y = tau), show.legend = FALSE,
            col = "black", lty = "dashed") +
  facet_wrap(~Method)+
  xlab(x_lab_over_I) +
  ylab(expression(paste("Mean ", RMSEP[m]))) +
  scale_x_continuous(breaks = unique(df_rmse_m$nitems)) +
  expand_limits(y = df_rmse_m_expand) +
  my_theme +
  theme(panel.grid.minor.x = element_blank())
grid_arrange_shared_legend(rmse_m_1, rmse_m_2, ncol = 1, nrow = 2,
                           position = "bottom")
@
\end{center}
\caption[Mean for the root mean squared error of prediction for each model across simulation conditions]
{Mean for the root mean squared error of prediction ($\mathrm{RMSEP}_{m}$) for each model across simulation conditions. The dashed line represents the residual item standard deviation ($\tau$), which is the limit of prediction accuracy.}
\label{fig:2-rmse-m}
\end{figure}

\begin{figure}[tb]
\begin{center}
<<rmse_plot, fig = TRUE, height=6>>=
# <<rmse_setup>>
df_rmse <- merge(df_rmse, conditions_df)
df_rmse_expand <- range(df_rmse[, c("value", "tau")])
rmse_1 <- ggplot(subset(df_rmse, over_Rsq)) +
  aes(y=value, x = Rsq, pch = Method, color = Method) +
  geom_point() + geom_line() +
  geom_line(mapping = aes(y = tau), show.legend = FALSE,
            col = "black", lty = "dashed") +
  facet_wrap(~Selector, nrow = 1) +
  xlab(x_lab_over_Rsq) +
  ylab(expression(paste("Mean ", RMSEP[italic(m)^symbol("\52")]))) +
  scale_x_continuous(breaks = unique(df_rmse$Rsq)) +
  expand_limits(y = df_rmse_expand) +
  my_theme +
  theme(panel.grid.minor.x = element_blank(),
        strip.text = element_text(size = 9))
rmse_2 <- ggplot(subset(df_rmse, over_nitems)) +
  aes(y = value, x = nitems, pch = Method, color = Method) +
  geom_point() + geom_line() +
  geom_line(mapping = aes(y = tau), show.legend = FALSE,
            col = "black", lty = "dashed") +
  facet_wrap(~Selector, nrow = 1) +
  xlab(x_lab_over_I) +
  ylab(expression(paste("Mean ", RMSEP[italic(m)^symbol("\52")]))) +
  scale_x_continuous(breaks = unique(df_rmse$nitems)) +
  expand_limits(y = df_rmse_expand) +
  my_theme +
  theme(panel.grid.minor.x = element_blank(),
        strip.text = element_text(size = 9))
grid_arrange_shared_legend(rmse_1, rmse_2, ncol = 1, nrow = 2,
                           position = "bottom")
@
\end{center}
\caption[The mean for the mean root mean squared error of prediction for the selected model]
{The mean for the mean root mean squared error of prediction for the selected model ($\mathrm{RMSE}_{m*}$) across simulation replications for the various selection strategies. The dashed line represents the residual item standard deviation ($\tau$), which is the limit of prediction accuracy. Standard AIC is used for the LLTM, while $\mathrm{AIC}_c$ is used for the LLTM-E2S. LOCO-CV was applied only to the LLTM-E2S.}
\label{fig:2-rmse}
\end{figure}


\section{Discussion}

Some recommendations may be made based on the simulation study. First, the LLTM should not be used because it yields biased parameter estimates and incorrect standard error estimates. Further, model selection strategies like AIC, BIC, and the likelihood ratio test behave inappropriately with the LLTM. The preceding findings occurred even when the unrealistic assumptions of the LLTM were approximately met ($R^2 = .9$). Instead, the LLTM-E2S is recommended for the accuracy of its parameter and standard error estimates. It does exhibit biased estimates for $\tau$ with marginal maximum likelihood estimation, but if this is concerning then restricted maximum likelihood may be used instead. Second, LOCO-CV is recommended for model selection over AIC or $\mathrm{AIC}_c$ with the LLTM-E2S. The simulation indicated that correct penalties for the LLTM-E2S depend on $R^2$, or by extension, $\tau$, and neither AIC or $\mathrm{AIC}_c$ address this phenomenon. Despite its use in the literature, $\mathrm{AIC}_c$ appears to be inappropriate for meta-regression models.

% The LLTM-E2S was found to produce accurate estimates and standard errors for coefficients associated with item predictors. In contrast, the LLTM yielded biased estimates for the coefficients and misleading standard errors. Holdout validation with new items, cross-validation, and $\mathrm{AIC}_c$ performed in reasonable ways in regards to model selection for the LLTM-E. Holdout validation with new items also performed reasonably for the LLTM in this regard, but AIC did not. The penalty implied by AIC is clearly inappropriate for the LLTM, and $\mathrm{AIC}_c$ for the LLTM-E2S is partially supported.

Seemingly contradictory findings arose from the simulation study; choices of modeling strategies and selection strategies had clear implications for model selection, but this did not lead to substantial differences in predictive accuracy. Though Model~2 made the best predictions on average across all conditions, it often performed only slightly better than Model~3. As the simulation demonstrated, competing models may have similar predictive utility, and furthermore predictive accuracy is limited by the residual standard deviation of item difficulty.

If the purpose of model selection is to choose a best model for generating predictions (with the specific parameter estimates from the available data), then holdout validation with new items is the recommended strategy as holdout validation is the only approach that is conditional on the parameter estimates. In this way, inferences regarding predictive utility are based on the specific prediction parameter estimates that would be used. In contrast, single dataset approximations like AIC and LOCO-CV rely on expectations over hypothetical data. This may still be useful if the goal is to merely identify a preferred model with expected predictive accuracy as a benchmark. In particular, LOCO-CV is recommended as it is less reliant on assumptions regarding the penalty term.

A stronger method of prediction would combine predictions from several models. The super learner \parencite{van2007super} accomplishes by assigning weights to the predictions from the candidate models, and asympotically such predictions are as good as those from the candidate model that minimizes the loss function with respect to the true probability distribution.
% (some times referred to as the oracle).
However, the application of the super learner to the kind of data considered in this chapter is not straightforward.
The super learner has been applied to propensity scores \parencite{pirracchio2015improving}, which like item response data involve a binary outcome variable, but the super learner would also need to extended to clustered data.

% \begin{itemize}
% 	\item \emph{Selection conditional on estimated model.} Holdout validation differs from AIC and LOCO-CV in that selection is based on the conditional prediction error rather than the expected prediction error (with the expectation taken over possible training datasets). Holdout validation with new items was more effective in selecting a model for prediction, even though it is not always more apt to choose the true model. However, the true model is not necessarily the best predictor in finite samples. For the purpose of prediction, the conditional prediction error seems to be a better benchmark for model selection. For the purpose of explanation, the expected prediction error may be more effective. Of course, the amount of available data may drive the choice a researcher makes in practice.
% 	\item \emph{Model averaging}. This chapter has focused on the selection of a single model based on predictive utility. If the goal is to obtain the best possible prediction, model averaging may be used instead. Model averaging involves aggregating the predictions from the set of candidates models, weighting the competing predictions according to the score function. The insights from the simulation results apply to this goal as well, and it is expected that model averaging using the LLTM-E2S would outperform the same using the LLTM.
% \end{itemize}

% \newpage
%
%
% \section{Notes}
%
% \subsection{Automatic item generation}
%
% 	\begin{itemize}
% 		\item \textcite{freund2008explaining}: AIG with figural matrix items.
% 		\item \textcite{holling2009automatic}: AIG with probability word problems.
% 		\item \textcite{cho2014additive}: ``In an item generation context, modeling the items can have two aims. First, given a dataset based on generated items, one may want to estimate the person traits (e.g., abilities) relying on a given statistical item model. Second, one may want to have an idea of the difficulty and discrimination of a generated item before it is generated, for example because one wants it to be optimally informative in an adaptive procedure. With such a purpose in mind, the item model needs to have a reasonable predictive value.''
% 		\item \textcite{arendasy2005automatic}: AIG with more figure rotation type things.
% 		\item \textcite{arendasy2007using}: AIG with algebra word problems
% 		\item \textcite{arendasy2010evaluating}: AIG with mental rotation problems.
% 		\item \textcite{arendasy2011using}: AIG with word fluency items.
% 	\end{itemize}
%
% \subsection{About AIC}
%
% The description below is taken from \textcite{burnham2003model}. The notation here is unrelated to that in the rest of the chapter.
%
% Kullback-Leibler information (or distance) is
% \begin{equation}
% 	I(f,g) = \int f(x) \log \left ( \frac{f(x)}{g(x|\theta)} \right ) ~dx
% \end{equation}
% where $f$ is the true distribution function, $g$ is the model distribution function, $x$ is a dataset, and $\theta$ is the model parameters. This is the ``information lost when $g$ is used to approximate $f$.'' The above is for continuous distributions, and the authors provide a separate equation for discrete distributions. Further, $I(f,g)$ may be rewritten as
% \begin{equation}
% 	I(f,g) = \int f(x) \log f(x) ~dx - \int f(x) \log g(x|\theta) ~dx
% \end{equation}
% and then
% \begin{equation}
% 	I(f,g) = \mathrm{E}_f[\log f(x)] - \mathrm{E}_f[\log g(x|\theta)]
% \end{equation}
% and then
% \begin{equation}
% 	I(f,g) = C - \mathrm{E}_f[\log g(x|\theta)]
% \end{equation}
% where $C$ is a constant, usually unknown or ignored, that does not depend on choice of model $g$. In this way, $\mathrm{E}_f[\log g(x|\theta)]$ is \emph{relative} distance.
%
% The target quantity for AIC is the ``relative expected K-L distance''
% \begin{equation}
% 	\mathrm{E}_y \mathrm{E}_x [\log( g(x | \hat \theta(y)) )]
% \end{equation}
% where $x$ and $y$ are independent datasets (not independent and dependent variables) drawn from an unknown true distribution $f$ and $\hat \theta(y)$ are MLE parameter estimates resulting from the fit of the model to $y$.
% (\cite{Kuha2004}, page 206, explains that the double expectation results from ``assuming the MLE is based on a separate, independent sample of data...'')
% Both expectations are effectively taken with respect to $f$ (page 60). $\hat \theta(y)$ differs from the psuedo-true parameter values $\theta_0$ for model $g$, and $g(x | \hat \theta_0)$ minimizes the K-L distance (in comparison to other values for $\theta$). The ``key result'' is
% \begin{equation}
% 	\mathrm{E}_y \mathrm{E}_x [\log( g(x | \hat \theta(y)) )] \approx
% 	\log( L(\hat \theta | data ) ) - K
% \end{equation}
% where $L(\hat \theta| data)$ is the log-likelihood of the fitted model and $K$ is the ``number of estimable parameters''. $K$ results from the use of $\hat \theta(y)$ instead of $\theta_0$, or in other words, due to the uncertainty in the estimated parameters. AIC is
% \begin{equation}
% 	\mathrm{AIC} = -2 \log(  L(\hat \theta|y)  ) + 2K
% .\end{equation}
% AIC is used to select the model that minimizes the ``expected value of this (conceptually) estimated K-L information'' (page 363):
% \begin{equation}
% 	\mathrm{E}_y \left [ I(f, g(\cdot | \hat \theta (y) )) \right ]
% .\end{equation}
%
% My interpretations: In the previous paragraph, there is an abrupt switch from
% 	$\log( L(\hat \theta | data ) )$
% to
% 	$\log( L(\hat \theta | y ) )$, which are presumably the same quantity.
% This is how it goes in the chapter (page 61), and the switch is confusing and unexplained.
%
% Also, because the expectations above are taken over data $y$ in
% 	$\mathrm{E}_y \mathrm{E}_x [\log( g(x | \hat \theta(y)) )]$,
% I believe AIC is not conditional on the parameter estimates from the available data. Adopting a cross-validation viewpoint, I would say that
% 	$\mathrm{E}_y \mathrm{E}_x [\log( g(x | \hat \theta(y)) )]$
% represents the expected cross-validation log-likelihood, where a model is trained on $y$ and evaluated on $x$. Further, AIC approximates this quantity (times $-2$).
%
% \textcite{stone1977asymptotic} is credited (for example, by \cite{Hastie2009}) with showing that AIC is asymptotically equivalent to leave-one-out cross-validation. The paper it self is short but difficult to follow; it's mainly a mathematical proof lacking discussion.
%
% \textcite{Kuha2004} describes the ``asymptotic efficiency'' of AIC: ``the expected mean squared error of predictions from models selected by it is the smallest possible in large samples.'' I interpret this to mean that AIC selects the best available approximation to the true model. BIC does not share this characteristic. BIC is consistent: ``the probability of selecting the true model from a set of candidates tends to 1 as $n$ increases if the true model is one of the models under consideration and the true parameter value $\theta*$ remains fixed'' (see references in \cite{Kuha2004}, page 217).
%
% \textcite{Kuha2004} motivates AIC in terms of the K-L distance and connects this to cross-validation. \textcite{burnham2003model} do not connect AIC to cross-validation, while \textcite{Hastie2009} present AIC as a correction to the in-sample likelihood due to overfitting.
%
% \textcite{Kuha2004} (page 208) lists two requirements for AIC to be a good estimate. First, the sample sized is assumed to be large. Corrections for small samples exist, but must be derived for every model type. Second, the models are true (or that the smaller model in a nested comparison is true). This is because the AIC penalty (the 2) is a property of the true distribution. For untrue models, AIC is biased but has zero variance. ``Other, less biased, estimates for the same quanityt exist, but their variances must also be larger. Thus, the constant estimate used in $\mathrm{AIC}_e$, besides being trivial to calculate, is likely to have a lower mean quared error thant alternatives in many models in which its assumptions are at least roughly satisfied.
%
% \textcite{fang2011asymptotic} shows, in the context of linear mixed effects models, that ``the marginal Akaike information criterion is asymptotically equivalent to the leave-one-cluster-out cross-validation and the conditional Akaike information criterion is asymptotically equivalent to the leave-one-observation-out cross-validation.''
%
%
% \subsection{Cross-validation}
%
% I should adopt the language ``holdout validation'' and drop ``holdout cross-validation'', as ``cross-validation'' seems to apply to $k$-fold, LOO, bootstrap CV, and the like. Nothing is ``crossed'' in holdout validation.
%
% \textcite{Hastie2009} suggest that there are two possible goals: model selection (choosing the best predictor) and model assessment (estimating the prediction error). In a data-rich situation, they say that a three part validation scheme is ideal: models are estimated in the ``training'' set, chosen based on loss in the ``validation'' set, and the prediction error for the final model is estimated on the ``test'' set. Single-dataset approaches (IC and CV) approximate the validation step.
%
% \emph{I think} that the error in the test dataset is an estimate of the conditional prediction error (conditional on the trained/fitted model), and that the error in the validation dataset is also a (potentially biased) estimate of the same. \emph{I think} that holdout validation is in this way different from single-dataset approaches like information criteria, $k$-fold CV, and leave-one-out CV. ``It does not seem possible to estimate conditional error effectively, given only the information in the same training set'' (\cite{Hastie2009}, page 220).
%
% \textcite{Hastie2009} say that CV approximates the expected test error. They provide a brief and not very clear discussion of whether cross-validation estimates the ``conditional test error''. They do a brief simulation comparing $k$-fold CV ($k=10$) and leave-one-out CV and conclude that neither estimates the conditional test error well. They also conclude that both ``are approximately unbiased for expected [test] error, but the variation in test error for different training sets is quite substantial.'' (In other words, the estimates have high variance, I think.)
%
% \textcite{arlot2010survey} mentions that the appeal of cross-validation methods is their universality, given that they are based on data splitting. They describe ``model selection for estimation'', which I think corresponds to estimating the expected error and choosing the model that minimizes the error. (The article relies heavily on stupid notation, so I may have to reread.) They mention efficiency and the oracle inequality (page 47). They alternative is ``model selection for identification'' (choosing the true model). The AIC-BIC dilemma (estimation versus identification) is mentioned with some probably useful references (page 48). Some papers on CV bias in the regression framework are mentioned (page 57).
%
% \textcite{arlot2010survey} discuss the merits of CV versus ``penalized criteria'' (page 71). ``The strongest argument for CV is its quasi-universality: Provided data are i.i.d., CV yields good model selection performances in (almost) any framework. Nevertheless, universality has a price: Compared to procedures designed to be optimal in a specific framework (like AIC), the model selection performances of CV can be less accurate, while its computational cost is higher.'' Later: ``More generally, because of its versatility, CV should be prefered to any model selection procedure relying on assumptions which are likely to be wrong.''


\printbibliography

\end{document}

