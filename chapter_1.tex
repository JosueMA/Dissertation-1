The first chapter will make the argument that cross-validation for clustered data requires consideration of what new data would be collected to support the desired inference, even when single dataset approximations like information criteria or $k$-fold cross-validation are used. 
In addition, it will introduce the ``doubly explanatory'' item response model as the motivating example to be used throughout the dissertation.
%Item response data by nature are always cross-classified; item responses are simultaneously nested both within persons and within items. 
This chapter lays further groundwork for the remaining chapters in a few other respects. First, it describes differing ways that cross-validation may be conducted and links these to
the inferences they support. Also, it frames the model in both frequentist and Bayesian terms and likewise describes the predictive quantities either paradigm employs in cross-validation.


\section{The ``doubly explanatory'' item response model}

The motivating example is the ``doubly explanatory'' \parencite{Wilson2004} item response model:
\begin{equation} \label{eq:eirm}
\Pr(y_{ip} = 1 | w_p, x_i, \zeta_p, \epsilon_i) =
\mathrm{logit}^{-1} \left [ (w_p'\gamma + \zeta_p) -
(x_i'\beta + \epsilon_i) \right ]
\end{equation}
\begin{equation}
\zeta_p \sim \mathrm{N}(0, \sigma^2)
\end{equation}
\begin{equation}
\epsilon_i \sim \mathrm{N}(0, \tau^2)
\end{equation}
where $y_{ip} = 1$ if person $p$ ($p = 1, \dotsc, P$) responded to item $i$ ($i = 1, \dotsc, I$) correctly and $y_{ip} = 0$ otherwise. The quantity $w_p'\gamma + \zeta_p$ is a latent regression of person ability; $w_p$ is a vector of person covariates, $\gamma$ is a vector of regression coefficients, and $\zeta_p$ is a residual. Similarly, $x_i'\beta + \epsilon_i$ is a latent regression of item difficulty in which $x_i$ is a vector of item covariates, $\beta$ is a vector of regression coefficients, and $\epsilon_i$ is a residual. The model may be understood as a generalization of the Rasch model \parencite{Rasch1960a} that decomposes the Rasch model ability and difficulty parameters into structural parts ($w_p'\gamma$ and $x_i'\beta$) and residual parts ($\zeta_p$ and $\epsilon_i$). The doubly explanatory explanatory model is quite general and encompasses many Rasch-family item response models.

Estimating the model using marginal maximum likelihood methods is infeasible, given the need to simultaneously integrate over $\zeta_p$ and $\epsilon_i$ when calculating the likelihood. The usual model for studying the effects of item predictors, the linear logistic test model \parencite{Fischer1973}, omits $\epsilon_i$, making the integration tractable. This approach is treated in depth in Chapter~2. On the other hand, the doubly explanatory model is easily estimated using Markov chain Monte Carlo (MCMC) methods, as is done in Chapter~3.


\section{Bayesian and frequentist approaches to the doubly explanatory model}

This section will compare Bayesian and frequentist modeling approaches, using the doubly explanatory model as an example. Discussion of the Bayesian approach will include:
\begin{itemize}
	\item differing types of parameters (``regular'' parameters, hyper-parameters, and arbitrary posterior quantities), 
	\item expressions for the posterior distribution, 
	\item prior distributions, and 
	\item hierarchical centering. 
\end{itemize}
Discussion of the frequentist approach will include:
\begin{itemize}
	\item the generalized linear mixed model framework,
	\item marginal maximum likelihood estimation,
	\item the distinction between ``fixed'' and ``random'' effects, and
	\item (approximate) means of estimating cross-mixed effects models.
\end{itemize}


\section{Holdout cross-validation schemes}

Performing holdout cross-validation requires a second dataset, perhaps from a separate data collection. The holdout dataset may involve a new sample of persons responding to the same set of items, which I term cross-validation \emph{over persons}. Alternatively, it may consist of a new set of items administered to the same persons, which I call cross-validation \emph{over items}. The former is useful for choosing among models with differing person covariates, and the latter is useful for choosing among models with differing item covariates.

Rather than collect a second dataset, a researcher may instead partition the available data, for example forming ``training'' and ``test'' halves or conducting $k$-fold cross-validation. In doing so, the researcher must consider whether cross-validation over persons or over items (or some other approach) is appropriate and partition the data in accordance with that choice.
Further, when constructing a likelihood for the purpose of using information criteria, the likelihood should be marginal in regards to whichever set of clusters cross-validation is to be performed over. Because of this, the ``default'' ways of calculating information criteria may not fit the desired inference. Chapters~2 and 3 are largely investigations of this issue.

\section{Predictive quantities}

Inferences regarding several facets of the model may be of interest:
\begin{itemize}
	\item residuals ($\zeta_p$ and $\epsilon_i$),
	\item clusters ($w_p'\gamma + \zeta_p$ and $x_i'\beta + \epsilon_i$),
	\item responses ($y_{ip}$), and
	\item likelihood.
\end{itemize}
Each of these may be considered for the existing dataset or for new (holdout) data. The statistical framework (frequentist or Bayesian) will determine the particular form for each of these quantities as well as whether they are thought of as estimates or predictions. Predictions for new responses are useful for model criticism (in particular for posterior predictive model checking), while predictions regarding the likelihood are useful for cross-validation. This portion of Chapter~1 may introduce the predictive pointwise density and marginal variation of it (though in this proposal they are presented as a part of Chapter~3).