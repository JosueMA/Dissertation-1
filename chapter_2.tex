This chapter investigates how well cross-validation performs within the frequentist framework using maximum likelihood estimation, specifically for the purpose of selecting among models with differing sets of item covariates. With this estimation method, the item residuals in Equation~\ref{eq:eirm} are commonly omitted from the model to make calculation of the marginal likelihood tractable. This chapter shows that AIC \parencite{akaike1974new} corresponds to cross-validation over persons and then goes on to shows that AIC and holdout cross-validation over persons both perform poorly in this instance.

\section{Model and methods}

Fitting the doubly explanatory model in Equation~\ref{eq:eirm} using marginal maximum likelihood is infeasible. Instead, the analysis models will be of the form:
\begin{equation} \label{eq:latent-reg-lltm}
	\Pr(y_{ip} = 1 | w_p, x_i, \zeta_p) =
	\mathrm{logit}^{-1} \left [ (w_p'\gamma + \zeta_p) - x_i'\beta \right ]
\end{equation}
\begin{equation}
	\zeta_p \sim \mathrm{N}(0, \sigma^2)
\end{equation}
Here the item residual for $\epsilon_i$ is omitted. This is a variation on the linear logistic test model, which is used to study the factors associated with the difficulty of items. The log-likelihood is
\begin{equation} \label{eq:lltm-likelihood}
	L(y ; \hat \sigma, \hat \gamma, \hat \beta) = 
	\sum_{p=1}^P \log
	\int \prod_{i=1}^I
		p(y_{ip} | \hat \gamma, \hat \beta, \zeta_p)
		p(\zeta_p | \hat \sigma)
	~d \zeta_p
\end{equation}
which is readily estimated as a generalized linear mixed model \parencite{Rijmen2003}.

Let $\tilde y$ represent response data from a holdout dataset, which may involve new persons or new items.
Then using the log-likelihood as the score function, $L(\tilde y ; \hat \sigma, \hat \gamma, \hat \beta)$ represents the fit of a model with parameter estimates from the original data evaluated in the holdout data. I will use the notation $\mathrm{CV}^{(P)}$ to represent $L(\tilde y ; \hat \sigma, \hat \gamma, \hat \beta)$ when $\tilde y$ arises from new persons and $\mathrm{CV}^{(I)}$ when it arises from new items. When a holdout dataset is available, $\mathrm{CV}^{(P)}$ or $\mathrm{CV}^{(I)}$ may be used to select among competing models.

AIC \parencite{akaike1974new} is an information criteria that estimates the expected log-likelihood of the fitted model when evaluated on new data. In the context of the model in Equation~\ref{eq:latent-reg-lltm}, it has the form
\begin{equation} 
	\mathrm{AIC} = -2 L(y ; \hat \sigma, \hat \gamma, \hat \beta) - 2k
,\end{equation}
where $k$ is the number of model parameters. Because the likelihood in Equation~\ref{eq:lltm-likelihood} is marginal over persons, AIC will estimate the expectation of $-2 \times \mathrm{CV}^{(P)}$.


\section{Simulation study 1: The effect of the quality of item predictors}

In each replication of this simulation, the doubly explanatory model is used to generate a training dataset and two holdout datasets: one containing new persons (new random draws of $\zeta_p$) and another containing new items (new random draws of $\epsilon_i$). 
Then competing models are fit to the training data, and AIC is calculated for each.  
Also, parameter estimates from fitted models are used to calculate $\mathrm{CV}^{(P)}$ and $\mathrm{CV}^{(I)}$ on the holdout datasets. The main quantity of interest in this simulation is the proportion of times each method selects each competing model. The generating values are as follows:
\begin{itemize}
	\item Number of persons is $P = 500$.
	\item Number of items is $I= 32$.
	\item Item covariate vectors $x_i$ are made of an intercept, three indicator variables, and one interaction.
	\item Item-related coefficients are $\beta = \{-.5, .5, .5, .5, -.5\}$.
	\item Item residuals have varying standard deviations, $\tau \in \{0, .1, .3, .5\}$.
	\item Person residuals have an SD of $\sigma = 1$.
\end{itemize}
The values of $\tau$ are chosen to represent cases where the item covariates are perfect, very strong, moderate, or weak predictors of item difficulty, and it is the only factor to vary between simulations.

The competing models are of the type in Equation~\ref{eq:latent-reg-lltm} and as such do not include item residuals. They differ from one another only in terms of the specification of $x_i'\beta$. For Model~1
\begin{equation} 
	x_i'\beta \equiv \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \beta_3 x_{3i}
,\end{equation}
for Model~2
\begin{equation} 
	x_i'\beta \equiv \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \beta_3 x_{3i} + 
		\beta_4 x_{1i} x_{2i}
,\end{equation}
and for Model~3
\begin{equation} 
	x_i'\beta \equiv \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \beta_3 x_{3i} + 
		\beta_4 x_{1i} x_{2i} + \beta_5 x_{2i} x_{3i}
.\end{equation}
Model~2 contains the correct choice of predictors, and in conditions where $\tau=0$ it matches the generating model. Otherwise, the competing models are only approximations to the true model.

Preliminary results indicate that AIC and cross-validation over persons perform similarly in terms of the proportion of times each model is selected. Further, comparing $\mathrm{CV}^{(P)}$ to $L(y ; \hat \sigma, \hat \gamma, \hat \beta)$ to obtain an empirical estimate of $k$ supports the accuracy of the penalty employed by AIC. Both approaches perform poorly when $\tau > .1$, however, indicating that neither is appropriate for selecting item covariates. Cross-validation over items is the only method to select Model~2 more often then Models~1 or 3 when $\tau > .1$, but it succeeded in doing so only in only 40 to $65\%$ of replications, depending on $\tau$.


\section{Simulation study 2: The effect of the number of items}

The second simulation is very similar to the first, the only differences being that $\tau$ is fixed at .3 and that the numbers of items vary, $I \in \{32, 64, 96, 128 \}$. The expected result was that cross-validation over items would select Model~2 more often as $I$ increases, while values of $I$ would have little effect on selection with AIC or cross-validation over persons.

Preliminary results suggest that cross-validation over items does perform better with greater numbers of items, but the percentage of times Model~2 is selected only grows from 55 to $65\%$. As expected, results for AIC and cross-validation over persons do not vary with the number of items.


\section{Notes}

\subsection{About the simulations}

I believe that there is a big problem with using the log-likelihood of a clearly misspecified model as a loss-function in CV. This is my intuition. The narrative should probably be that none of methods work with the misspecified LLTM, including the correct holdout validation scheme (over items). I don't think there is a lot to be learned from the simulations other than that methods aren't working, and so I don't think it is necessary to show all these differing conditions and make a lot of interpretations.

I think instead it makes more sense to demonstrate a reasonable approach, which is to fit the Rasch model and then perform CV on the Rasch difficulties using a meta-analysis regression. The meta-analysis part can be done with \texttt{sem} in Stata, I think, so it isn't too exotic to propose for people to actually do in practice.

I'm also considering changing the item predictors: $x_1$ would be continuous, and $x_2$ and $x_3$ would be dummy variables. Then the interactions $x_1 x_2$ and $x_1 x_3$ would be considered in the candidate models. This would make for a more informative set of covariates and may lead to  results that are more encouraging.


\subsection{About AIC}

The description below is taken from \textcite{burnham2003model}. The notation here is unrelated to that in the rest of the chapter.

Kullback-Leibler information (or distance) is
\begin{equation}
	I(f,g) = \int f(x) \log \left ( \frac{f(x)}{g(x|\theta)} \right ) ~dx
\end{equation}
where $f$ is the true distribution function, $g$ is the model distribution function, $x$ is a dataset, and $\theta$ is the model parameters. This is the ``information lost when $g$ is used to approximate $f$.'' The above is for continuous distributions, and the authors provide a separate equation for discrete distributions. Further, $I(f,g)$ may be rewritten as
\begin{equation}
	I(f,g) = \int f(x) \log f(x) ~dx - \int f(x) \log g(x|\theta) ~dx
\end{equation}
and then
\begin{equation}
	I(f,g) = \mathrm{E}_f[\log f(x)] - \mathrm{E}_f[\log g(x|\theta)]
\end{equation}
and then
\begin{equation}
	I(f,g) = C - \mathrm{E}_f[\log g(x|\theta)]
\end{equation}
where $C$ is a constant, usually unknown or ignored, that does not depend on choice of model $g$. In this way, $\mathrm{E}_f[\log g(x|\theta)]$ is \emph{relative} distance.

The target quantity for AIC is the ``relative expected K-L distance''
\begin{equation}
	\mathrm{E}_y \mathrm{E}_x [\log( g(x | \hat \theta(y)) )]
\end{equation}
where $x$ and $y$ are independent datasets (not independent and dependent variables) drawn from an unknown true distribution $f$ and $\hat \theta(y)$ are MLE parameter estimates resulting from the fit of the model to $y$. 
(\cite{Kuha2004}, page 206, explains that the double expectation results from ``assuming the MLE is based on a separate, independent sample of data...'')
Both expectations are effectively taken with respect to $f$ (page 60). $\hat \theta(y)$ differs from the psuedo-true parameter values $\theta_0$ for model $g$, and $g(x | \hat \theta_0)$ minimizes the K-L distance (in comparison to other values for $\theta$). The ``key result'' is
\begin{equation}
	\mathrm{E}_y \mathrm{E}_x [\log( g(x | \hat \theta(y)) )] \approx
	\log( L(\hat \theta | data ) ) - K
\end{equation}
where $L(\hat \theta| data)$ is the log-likelihood of the fitted model and $K$ is the ``number of estimable parameters''. $K$ results from the use of $\hat \theta(y)$ instead of $\theta_0$, or in other words, due to the uncertainty in the estimated parameters. AIC is
\begin{equation}
	\mathrm{AIC} = -2 \log(  L(\hat \theta|y)  ) + 2K
.\end{equation}
AIC is used to select the model that minimizes the ``expected value of this (conceptually) estimated K-L information'' (page 363):
\begin{equation}
	\mathrm{E}_y \left [ I(f, g(\cdot | \hat \theta (y) )) \right ]
.\end{equation}

My interpretations: In the previous paragraph, there is an abrupt switch from 
	$\log( L(\hat \theta | data ) )$  
to
	$\log( L(\hat \theta | y ) )$, which are presumably the same quantity.
This is how it goes in the chapter (page 61), and the switch is confusing and unexplained. 

Also, because the expectations above are taken over data $y$ in 
	$\mathrm{E}_y \mathrm{E}_x [\log( g(x | \hat \theta(y)) )]$, 
I believe AIC is not conditional on the parameter estimates from the available data. Adopting a cross-validation viewpoint, I would say that
	$\mathrm{E}_y \mathrm{E}_x [\log( g(x | \hat \theta(y)) )]$
represents the expected cross-validation log-likelihood, where a model is trained on $y$ and evaluated on $x$. Further, AIC approximates this quantity (times $-2$).

\textcite{stone1977asymptotic} is credited (for example, by \cite{Hastie2009}) with showing that AIC is asymptotically equivalent to leave-one-out cross-validation. The paper it self is short but difficult to follow; it's mainly a mathematical proof lacking discussion.

\textcite{Kuha2004} describes the ``asymptotic efficiency'' of AIC: ``the expected mean squared error of predictions from models selected by it is the smallest possible in large samples.'' I interpret this to mean that AIC selects the best available approximation to the true model. BIC does not share this characteristic. BIC is consistent: ``the probability of selecting the true model from a set of candidates tends to 1 as $n$ increases if the true model is one of the models under consideration and the true parameter value $\theta*$ remains fixed'' (see references in \cite{Kuha2004}, page 217).

\textcite{Kuha2004} motivates AIC in terms of the K-L distance and connects this to cross-validation. \textcite{burnham2003model} do not connect AIC to cross-validation, while \textcite{Hastie2009} present AIC as a correction to the in-sample likelihood due to overfitting.

\textcite{Kuha2004} (page 208) lists two requirements for AIC to be a good estimate. First, the sample sized is assumed to be large. Corrections for small samples exist, but must be derived for every model type. Second, the models are true (or that the smaller model in a nested comparison is true). This is because the AIC penalty (the 2) is a property of the true distribution. For untrue models, AIC is biased but has zero variance. ``Other, less biased, estimates for the same quanityt exist, but their variances must also be larger. Thus, the constant estimate used in $\mathrm{AIC}_e$, besides being trivial to calculate, is likely to have a lower mean quared error thant alternatives in many models in which its assumptions are at least roughly satisfied.

\textcite{fang2011asymptotic} shows, in the context of linear mixed effects models, that ``the marginal Akaike information criterion is asymptotically equivalent to the leave-one-cluster-out cross-validation and the conditional Akaike information criterion is asymptotically equivalent to the leave-one-observation-out cross-validation.''


\subsection{Cross-validation}

I should adopt the language ``holdout validation'' and drop ``holdout cross-validation'', as ``cross-validation'' seems to apply to $k$-fold, LOO, bootstrap CV, and the like. Nothing is ``crossed'' in holdout validation.

\textcite{Hastie2009} suggest that there are two possible goals: model selection (choosing the best predictor) and model assessment (estimating the prediction error). In a data-rich situation, they say that a three part validation scheme is ideal: models are estimated in the ``training'' set, chosen based on loss in the ``validation'' set, and the prediction error for the final model is estimated on the ``test'' set. Single-dataset approaches (IC and CV) approximate the validation step.

\emph{I think} that the error in the test dataset is an estimate of the conditional prediction error (conditional on the trained/fitted model), and that the error in the validation dataset is also a (potentially biased) estimate of the same. \emph{I think} that holdout validation is in this way different from single-dataset approaches like information criteria, $k$-fold CV, and leave-one-out CV. ``It does not seem possible to estimate conditional error effectively, given only the information in the same training set'' (\cite{Hastie2009}, page 220).

\textcite{Hastie2009} say that CV approximates the expected test error. They provide a brief and not very clear discussion of whether cross-validation estimates the ``conditional test error''. They do a brief simulation comparing $k$-fold CV ($k=10$) and leave-one-out CV and conclude that neither estimates the conditional test error well. They also conclude that both ``are approximately unbiased for expected [test] error, but the variation in test error for different training sets is quite substantial.'' (In other words, the estimates have high variance, I think.)

\textcite{arlot2010survey} mentions that the appeal of cross-validation methods is their universality, given that they are based on data splitting. They describe ``model selection for estimation'', which I think corresponds to estimating the expected error and choosing the model that minimizes the error. (The article relies heavily on stupid notation, so I may have to reread.) They mention efficiency and the oracle inequality (page 47). They alternative is ``model selection for identification'' (choosing the true model). The AIC-BIC dilemma (estimation versus identification) is mentioned with some probably useful references (page 48). Some papers on CV bias in the regression framework are mentioned (page 57).

\textcite{arlot2010survey} discuss the merits of CV versus ``penalized criteria'' (page 71). ``The strongest argument for CV is its quasi-universality: Provided data are i.i.d., CV yields good model selection performances in (almost) any framework. Nevertheless, universality has a price: Compared to procedures designed to be optimal in a specific framework (like AIC), the model selection performances of CV can be less accurate, while its computational cost is higher.'' Later: ``More generally, because of its versatility, CV should be prefered to any model selection procedure relying on assumptions which are likely to be wrong.''

