\section{Introduction}

% Item explanatory models and prediction
Several models have been developed that account for the factors associated with item difficulty: the linear logistic test model \parencite{Fischer1973}, the linear logistic test model with error \parencite{Janssen2004}, the 2PL-constrained model \parencite{embretson1999generating}, the additive multilevel item structure model \parencite{cho2014additive}, and others. Such models are described as item explanatory models by \textcite{Wilson2004}, and these models lend themselves to the prediction of item difficulties (and in some cases other item parameters) for new items. Predictions regarding new items are useful in automatic item generation, especially in regards to adaptive testing when the goal is to generate an optimally informative item during administration \parencite[see for example,][]{embretson1999generating}. Such a prediction is sometimes referred to as ``precalibration'' \parencite[see for example,][]{gierl2012automatic}. Even when item generation is not automatic, the ability to anticipate the difficulty of new items may be useful in the development of new test forms.

% Prediction-based model selection
The relevant factors and interactions related to item difficulty may not be known a priori, and in this case a prediction-based approach to model selection may be employed to select a best model for the prediction of new item difficulties. A model selection scheme requires the choice of a score function to evaluate models, and the deviance ($-2$ times the log-likelihood) is a natural choice for item response models. Holdout validation, cross-validation, or information criteria may be used to select a best model from a set of candidate models on the basis of observed or expected prediction utility. In holdout validation, prediction error is estimated by training a model on one dataset and evaluating its fit (deviance) in a second dataset. Cross-validation is similar but involves splitting the data multiple times and aggregating the results. Akaike information criterion \parencite[AIC;][]{akaike1974new} is asymptotically equivalent to cross-validation \parencite{stone1977asymptotic} but relies on only a single fit of the model. In holdout validation, the estimated prediction error is conditional on the fitted model, whereas in cross-validation and AIC it approximates the expectation over possible training datasets \parencite{Hastie2009}.

% Prediction is a good basis of model selection in general
Predictive utility is a good basis of model selection in general, even if the goal is not actually the prediction of new data. In particular, if it is believed that none of the candidate models are true, then the model that best predicts new (or holdout) data may be justified as the best available approximation to the true model. 
%One way of formalizing the loss of information that results from using such an approximation is the Kullback-Leibler distance, which forms the basis for AIC.
For example a researcher may be interested in studying the factors associated with item difficulty in order to learn about the cognitive or psychological theory pertaining to the latent construct. \emph{((Give some examples.))} In short, the purpose of modeling item difficulty may be explanation rather than prediction, and predictive utility remains a strong basis for selecting a preferred model.

% Prediction and clustered data
With models for clustered data, predictions may be for new responses or for (the latent means of) new clusters. Predictions for responses may be for new responses either from the same clusters or from new clusters. Item response models are more complicated given that the responses are nested simultaneously within both persons and items. For such data, predictions could be made regarding the latent trait for new persons (based on person-related covariates) or for the difficulty of new items (based on item-related covariates). Prediction of new responses may arise from any combination of the same or new persons and the same or new items, though this chapter focuses only on the prediction of new item clusters.

% Model and likelihood choices
The choice of model and the associated likelihood imply what the features new (or holdout) data would have. In the generalized linear mixed model (GLMM) framework, models are built from ``fixed'' and ``random'' effects. Fixed effects do not (generally) vary across clusters, much like standard regression coefficients. Random effects, such as random intercepts or random coefficients, do vary across clusters. The random effects are not modeled directly, but instead the parameters of their assumed joint distribution are estimated. These models are commonly fit using marginal maximum likelihood estimation, in which the random effects are integrated over their estimated distributions, and this marginalization over implies that a new data collection would involve a new set of clusters. Alternatively, cluster-specific parameters could be included as fixed effects, implying that new data would involve new observations from the same set of clusters.

% Model and likelihood choices for IRT
Rasch family item response models, including the LLTM but not the LLTM-E, are readily specified as GLMMs \parencite{Rijmen2003} and are commonly estimated using marginal maximum likelihood \parencite{Bock1981}. Items are customarily modeled as fixed effects, perhaps as item-specific parameters for the Rasch model or as a weighted sum of covariates for the LLTM, and persons are modeled as random effects, perhaps with fixed effects for the mean structure of the person ability distribution \parencite[for example,][]{Adams1997b}. In this way, such models and their associated marginal likelihoods imply that persons are exchangeable, and so will be different in new data, and that items are constant. Using this marginal likelihood as a score function is well-suited to the selection of models predicting person ability but poorly suited to the selection of models predicting item difficulty. The disconnect between score function and the desired prediction inference may yield misleading results in prediction-based model selection, whether holdout validation, cross-validation, or information criteria are used.

% The LLTM-E2S as a solution
The LLTM-E is associated with a likelihood that is marginal in regards to the items. Specifically it is marginal over the residuals for the predicted item difficulties and so should be well-suited for selecting models that predict item difficulty. However, given that the model is simultaneously marginal in regard to persons, it is infeasible to estimate using marginal maximum likelihood. I propose a two-stage approximation to the LLTM-E, called the LLTM-E2S, that yields an appropriate likelihood that is marginal in regards to the items.

% This chapter
In this chapter, simulation study results for several model selection strategies are reported for the LLTM and LLTM-E2E.  It is expected that holdout validation using a new set of items yields better item prediction results than repeating the same set of items. That may seem forgone, but a more subtle matter is that the usual application of AIC will correspond to a prediction inference involving new items for the LLTM-E2S but involving the same items the LLTM, owing to differences in the construction of the likelihood. Likewise, other selection strategies, including likelihood ratio tests and BIC, yield differing results for the two modeling strategies. In short, common selection strategies fail for the LLTM but behave reasonably for the LLTM-E2S.


%\newpage
%
%\begin{itemize}
%	\item Purpose
%		\begin{itemize}
%			\item Model selection
%			\item Predictive accuracy (Kullback-Liebler distance, ``error of approximation'')			
%		\end{itemize}		
%	\item Motivation and LLTM-E
%			\begin{itemize}
%				\item Fixed effects selected by LR-test $\rightarrow$ AIC
%				\item Wrong criterion / wrong model
%				\item Purpose of LLTM can be prediction (refs)		
%				\item Model needs to include error (Johnson and Sinharay)
%				\item Cross-validation
%				\item New items
%				\item Conditional versus marginal. Marginal $\rightarrow$ random effects $\rightarrow$ new items/clusters. Conditional $\rightarrow$ fixed effects $\rightarrow$ same items/clusters. IRT models are almost always conditional in regards to items.
%			\end{itemize}	
%\end{itemize}


\section{Models}


\subsection{The linear logistic test model with error}

The data generating model in the simulation study is the linear logistic test model with error \parencite[LLTM-E;][]{DeBoeck2008}:
\begin{equation}
	\Pr(y_{ij} = 1 | x_i, \theta_j) =
	\mathrm{logit}^{-1} \left [ \theta_j - (x_i'\beta + \epsilon_i) \right ]
\end{equation}
\begin{equation}
	\theta_j \sim \mathrm{N}(0, \sigma^2)
\end{equation}
\begin{equation}
	\epsilon_i \sim \mathrm{N}(0, \tau^2)
,\end{equation}
where $y_{ij} = 1$ if person $j$ ($j = 1, \dotsc, J$) responded to item $i$ ($i = 1, \dotsc, I$) correctly and $y_{ij} = 0$ otherwise. 
Latent ability is denoted by $\theta_j$, which follows a normal distribution. 
The quantity $x_i'\beta + \epsilon_i$ is a latent regression of item difficulty in which $x_i$ is a vector of item covariates, $\beta$ is a vector of regression coefficients, and $\epsilon_i$ is a residual. 
Further, $x_i$ is a row from a matrix of item covariates $X$, which will in general include an intercept term as one of the columns.
The model may be understood as a generalization of the Rasch model \parencite{Rasch1960a} that decomposes the Rasch model difficulty parameters into a structural part ($x_i'\beta$) and residual part ($\epsilon_i$).
Omitting the residual part from the LLTM-E yields the standard linear logistic test model \parencite[LLTM;][]{Fischer1973}.
\emph{((``choice of scoring function p. 998 Gelman Hwang Vehtari''))}

Fitting the model using marginal maximum likelihood estimation is infeasible, given the need to integrate over the vectors $\theta$ and $\epsilon$ simultaneously when calculating the marginal (log) likelihood (at parameter estimates):
\begin{equation} \label{eq:lltme-likelihood}
	L(y | \hat \omega_m(y)) = \log 
		\int \cdots \int \left [
			\prod_{i=1}^I \prod_{j=1}^J
			\Pr(y_{ij} | \hat \beta, \epsilon_i, \theta_j)
			g(\epsilon_i ; 0, \hat \tau^2)
			g(\theta_j ; 0, \hat \sigma^2)
		\right ] ~d \epsilon d \theta
.\end{equation}
This marginal likelihood does not simplify and at best may be reduced from $I+J$ to $I+1$ dimensional integrals \parencite{goldstein1987multilevel, rasbash1994efficient}.
$\hat \omega_m(y)$ is shorthand for all estimated parameters ($\hat \beta$, $\hat \sigma$, and $\hat \tau$) for model $m$, which are estimated from data $y$, and the hats on parameters denote marginal maximum likelihood estimates. For the moment it may seem redundant to indicate that the parameter estimates arise from $y$ in the notation $\hat \omega_m(y)$, but this notation will become useful later.
%In contrast, Markov chain Monte Carlo (MCMC) methods do not rely on the marginal likelihood and so readily estimate the model, as featured in Chapter~3.


\subsection{The linear logistic test model}

A common model for studying the effects of item covariates, the LLTM \parencite[][]{Fischer1973}, omits the item residual $\epsilon_i$:
\begin{equation}
	\Pr(y_{ip} = 1 | x_i, \theta_j) =
	\mathrm{logit}^{-1} \left [ \theta_j - x_i'\beta \right ]
\end{equation}
\begin{equation}
	\theta_j \sim \mathrm{N}(0, \sigma^2)
.\end{equation}
Otherwise, the model is the same as the LLTM-E.
The marginal log-likelihood for the LLTM (at the parameter estimates) is
\begin{equation} \label{eq:lltm-likelihood}
	L(y | \hat \omega_m(y)) = 
	\sum_{j=1}^J \log
	\int 
		\left [ \prod_{i=1}^I \Pr(y_{ij} | \hat \beta, \theta_j) \right ]
		g(\theta_j ; 0, \hat \sigma^2)
	~d \theta_j
,\end{equation}
where $\hat \omega_m(y)$ again represents all estimated parameters, this time only $\hat \beta$ and $\hat \sigma$.
While no closed-form solution exists for the single integration due to the logit link function, it is easily approximated, for example, by using adaptive quadrature \emph{((ref))}. Indeed, the LLTM may be expressed as a generalized linear mixed model and is readily fit in standard software in addition to more specialized software for item response theory models.

The LLTM is a fundamentally incomplete model owing to the omission of $\epsilon_i$. In real applications, in which the item covariates are invariably less than perfect predictors for item difficulty, $x_i' \beta$ alone cannot hope to replicate the ``complete'' item difficulties $x_i' \beta + \epsilon_i$. Critically, the misspecification of the model in this regard results in inappropriately small standard errors for $\hat \beta$. This problem directly parallels the situation in multilevel modeling in which a ``flat'' (generalized) linear model is fit to clustered data. 

It is worth noting that \textcite{Fischer1973} recommended ensuring against this misspecification in the LLTM by conducting a likelihood ratio test comparing the LLTM to the Rasch model. Results from the LLTM were only to be interpreted if the likelihood ratio test did not reject it. However, the LLTM will generally be rejected, leaving the researcher with two options: either refrain from studying the sources of item difficulty or ignore advice and interpret a misleading model. 
\emph{((Reread Fischer to ensure my summary is accurate.))}


\subsection{Two-stage estimation}

To avoid the high-dimensional integral in Equation~\ref{eq:lltme-likelihood}, I propose a two-stage estimation of the LLTM-E, which I will refer to as the LLTM-E2S.
In the first stage, the Rasch model is fit to the data. The Rasch model is
\begin{equation}
	\Pr(y_{ij} = 1 | \delta_i, \theta_j) =
	\mathrm{logit}^{-1} \left [ \theta_j - \delta_i \right ]
\end{equation}
\begin{equation}
	\theta_j \sim \mathrm{N}(0, \sigma^2)
,\end{equation} 
where $\delta_i$ is an item-specific difficulty parameter.  Point estimates and standard errors for each $\hat \delta_i$ are obtained by marginal maximum likelihood estimation, using a likelihood similar to that in Equation~\ref{eq:lltm-likelihood}.

In the second stage, the $\hat \delta_i$ are regressed on the item covariates:
\begin{equation}
	\hat \delta_i = x_i'\beta + u_i + \epsilon_i
\end{equation}
\begin{equation}
	u_i \sim \mathrm{N}(0, \widehat \mathrm{var}(\hat \delta_i))
\end{equation}
\begin{equation}
	\epsilon_i \sim \mathrm{N}(0, \tau^2)
,\end{equation}
where $u_i$ is a residual related to uncertainty in the estimated $\hat \delta_i$, and $\epsilon_i$ is the usual residual in linear regression. 
The residual $u_i$ has known variance $\widehat \mathrm{var}(\hat \delta_i)$, which is the estimated squared standard error for $\hat \delta_i$ obtained in the first stage. 
In contrast, the variance for $\epsilon_i$, $\tau^2$, is estimated.
This is a random-effects meta-regression model \emph{((ref, including Raudenbush))}.
Once again, let $\hat \omega_m(y)$ represent the set of estimated parameters, here $\hat \beta$ and $\hat \tau$. 
Then the second-stage log-likelihood is
\begin{equation}
	L(y | \hat \omega_m(y)) = \sum_{i=1}^I \log 
	[ g(\hat \delta_i ; x_i'\beta, \widehat \mathrm{var}(\hat \delta_i) + \tau^2) ]
\end{equation}
and is suitable only for the selection of an item difficulty model.
\emph{((Provide examples of other uses of two-stage estimation.))}


\section{Model selection strategies}


\subsection{Holdout validation}

In holdout validation a large dataset is split into three parts: the \emph{training}, \emph{validation}, and \emph{evaluation} subsets. 
(The evaluation subset is also referred as the test subset.)
For consistency, the fit of models will be evaluated in terms of deviance, which is minus twice the log likelihood, and which may also be referred to as prediction error.
In brief, parameter estimates are obtained from fitting the model to the training subset, and then the deviance of the fitted model applied to the validation subset is calculated. 
The model with the lowest deviance in the validation subset is selected and then evaluated a second time in the evaluation subset.
The use of a validation subset addresses the bias that would arise from both fitting and evaluating the model using only a training subset.
The use of an evaluation subset addresses the bias that would arise from selecting and evaluating a model using the validation subset alone.

This chapter extends the usual holdout validation scheme by considering what elements differ or persist between the subsets. 
For the case of selecting a model that predicts item difficulties, the relevant detail is whether the subsets include the same or different items. 
Let $y^{(\mathrm{t})}$ be the training subset. 
A validation subset may include the same items as $y^{(\mathrm{t})}$, and such a validation subset will be denoted $y^{(\mathrm{s})}$. 
Alternatively, a validation subset might include a new set of items, and that training subset will be denoted $y^{(\mathrm{n})}$. 
A model is selected based on 
	$\mathrm{dev}(y^{(\mathrm{n})} | \hat \omega_m(y^{(\mathrm{t})}))$ or
	$\mathrm{dev}(y^{(\mathrm{s})} | \hat \omega_m(y^{(\mathrm{t})}))$, 
	depending on the form of the training subset.
Let $m^*$ represent the model selected from this process.
It is evaluated in the evaluation subset, $y^{(\mathrm{e})}$, by 
	$\mathrm{dev}(y^{(\mathrm{e})} | \hat \omega_{m^*}(y^{(\mathrm{t})}))$,
where $m^*$ may differ depending on which type of validation subset was used.
In this chapter, the evaluation subset always contains new items; that is, items that are different from those featured in the training and validation subsets.

Other elements of the data may differ or persist between the subsets. For example, the subsets may include the same or different persons. If the number of persons is small, there may be some advantage in having the same persons in each subset, but in general this is unlikely to be an important factor for the purpose of selecting among models for item prediction. In the simulations that follow, all subsets have different groups of persons. Another element of the data that may or may not persist is $X$; the items may or may not have the same covariate values as in the training subset. In other words, the items may or may not follow exactly the same design. For simplicity, the simulations assume the same $X$ in all subsets, but this need not be the case in general.

The estimated prediction error (deviance) in holdout validation is conditional on the particular training data used. This is clear in the notation
	$\mathrm{dev}(y^{(\mathrm{e})} | \hat \omega_{m^*}(y^{(\mathrm{t})}))$, 
in which the deviance of the chosen model in the test subset is conditional on parameter estimates obtained from the training subset.
This fact distinguishes holdout validation from the cross-validation methods discussed in the next section.
		
In summary, two approaches to holdout validation for models with item covariates are considered: one in which the validation subset features the same items as the training subset and one in which the validation subset features new items. Either one may be used with the LLTM and the LLTM-E2S, though differences in how the likelihood is constructed for those methods may help or hinder the sensitivity of holdout validation. As the likelihood for the LLTM-E2S is marginal over items, it is expected to perform well in model selection when paired with holdout validation with new items. (The same should be true for the LLTM-E, if the marginal likelihood for it could be evaluated.) I expect the LLTM with holdout validation with new items to perform less well, as the likelihood for it does not generalize over items. Further, I expect holdout validation with the same items will perform poorly in all cases.


\subsection{Cross-validation and AIC}

If data are not abundant, single dataset methods for model selection may be considered. In $k$-fold cross-validation, the data are split into $K$ (approximately) equally sized partitions, most often $K=5$ or 10. 
A model is fit to all data \emph{not} in fold $k$, and then the fitted model is evaluated on the data in fold $k$. 
This process is performed for every fold and the resulting deviances are summed. For item response data, the data may be partitioned into folds defined by persons or by items.
To compare models that explain the difficulty of items, constructing folds based on items is needed.
On the other hand, comparing models with differing sets of person covariates \parencite[see for example][]{Adams1997b} would require folds based on persons.
%A model is fit to all data \emph{not} in fold $k$, denoted $y^{(-k)}$, and the fitted model is evaluated on the data in fold $k$, $y^{(k)}$. 
%This process is performed for every fold and the resulting deviances are summed, yielding 
%$\mathrm{dev}(y^{(k)}, \hat \omega_m | y^{(-k)})$.
%\emph{((Have some doubts about that notation.))}

For the LLTM, a possibility is to assign each item to its own fold, which may be referred to as leave-one-cluster-out cross-validation (LOCO-CV). Then
\begin{equation}
	\mathrm{LOCO\textnormal{-}CV}_m = \sum_{i=1}^{I} \mathrm{dev}(y^{(i)} | \hat \omega_m( y^{(-i)}))
\end{equation}
where $y^{(i)}$ indicates all data for item $i$, and $y^{(-i)}$ indicates all data \emph{not} for item $i$. LOCO-CV may also be calculated using LLTM-E2S. In this case, the Rasch model is first fit to all data, just as proposed earlier, and then the regression in the second stage is repeated $I$ times, leaving one item out each time. From this, LOCO-CV is calculated as in the above equation. The simulations in this chapter focus on LOCO-CV using the LLTM-E2S.

%The LLTM-E2S is particularly well-adapted to this approach, and in this chapter the special case where $k=I$ is considered, in which each item is left out in turn. 
%This is a form of leave-one-out cross-validation (LOO-CV).

The Akaike information criterion \parencite[AIC;][]{akaike1974new} requires only a single fit of the model and has the form
\begin{equation} \label{eq:aic}
	\mathrm{AIC}_m = \mathrm{dev}(y | \hat \omega_m(y)) + 2p_m
,\end{equation}
where $p_m$ is the count of parameters in model $m$. 
AIC is an approximation related to the Kullback-Leibler distance, which is a measure of the information lost when a model is used to approximate the true data generating distribution. Calculating the Kullback-Leibler distance would require knowing the true data generating distribution, but approximating the expected \emph{relative} distance 
\begin{equation}
	\mathrm{ERD}_m = \mathrm{E}_{y^{(e)}} \mathrm{E}_{y^{(t)}} [L(y^{(e)} |\hat \omega_m(y^{(t)}))]
\end{equation}
is tractable.
In the above equation, $y^{(e)}$ and $y^{(t)}$ are (hypothetical) independent datasets and the expectations are taken over the true data generating distribution. The difference between the Kullback-Leibler distance and expected relative distance is an unknown constant that is a function only of the true data generating distribution. This constant will be the same for all competing models, given that it does not depend on the models. 
%It is in this way that the expected relative distance is relative.
AIC is an approximation to the expected relative distance multiplied by negative two. For models without mixed effects, AIC is asymptotically equivalent to  ``leave-one-observation-out'' cross-validation \parencite{stone1977asymptotic}, and for models with mixed effects it is asymptotically equivalent to LOCO-CV \parencite{fang2011asymptotic}, at least for linear mixed effects models.

\textcite{Kuha2004} describes two requirements for AIC to be a good estimate of the expected relative distance. First, the sample size is assumed to be large. Corrections for small samples exist but must be derived for every model type. Second, the candidate models are assumed to be true. This is a result of the derivation of AIC; the AIC penalty (two times the number of parameters) is a property of the true distribution. For untrue models, the penalty is biased but has zero variance. ``Other, less biased, estimates for the same quantity exist, but their variances must also be larger. Thus, the constant estimate used in [AIC], besides being trivial to calculate, is likely to have a lower mean squared error than alternatives in many models in which its assumptions are at least roughly satisfied.''

\textcite{Vaida2005} demonstrate that, for linear mixed effects models, ``marginal'' AIC (as in Equation~\ref{eq:aic}) assumes that (hypothetical) new datasets would entail a different set of clusters than the original data. Also in the context of linear mixed effects models, \textcite{Greven2010} show that marginal AIC is not asymptotically unbiased, favoring models with fewer random effects, but suggest this may not be a problem in choosing between models that merely have differing fixed effects. In addition, \textcite{Vaida2005} develop a conditional AIC for inferences pertaining to new datasets that would have the same, fixed set of clusters, and this work has been extended by others \parencite{Liang2008, Greven2010, yu2012conditional, yu2013information, saefken2014unifying}, though this conditional AIC is not suitable for this application.

The appropriateness of AIC for both the LLTM and LLTM-E2S is unclear. The likelihood for the LLTM is (in general) badly misspecified due to the omission of the item residuals, and so the assumption that the model be close to the true model will not be met. Meanwhile, the likelihood for the LLTM-E2S results from an unusual regression model in which part of the error variance is known, and the accuracy of AIC for such models is unknown. \emph{((Check that this really is unknown.))} Further, the usual form of AIC given above applies the same penalty for either approach, even though the magnitude of the deviances differ dramatically between the two.

Some predictions may be made from these facts. LOO-CV with the LLTM-E2S is expected to perform well. If AIC performs similarly to LOO-CV for the LLTM-E2S, then the appropriateness of AIC in that context would be supported, but this is not a given. AIC with the LLTM is expected to perform poorly, partly because the likelihood is misspecified and partly because the likelihood is not marginal over items.


\subsection{Other model selection strategies}

The Bayesian information criterion \parencite[BIC;][]{schwarz1978estimating} is also considered, which is given by
\begin{equation}
	\mathrm{BIC}_m = \mathrm{dev}(y | \hat \omega_m(y)) + p_m \log N
,\end{equation}
where $N$ is the count of observations. For the LLTM approach $N = I \times P$, while for the two stage approach $N = I$.

\emph{((Likelihood ratio testing, etc.))}


\section{Simulation}


\subsection{Simulation study design}

In each replication of this simulation, the LLTM-E is used to generate the training data subset, $y^{(t)}$, the validation subsets, $y^{(s)}$ and $y^{(n)}$, and an evaluation subset, $y^{(e)}$. The two forms of holdout validation are performed for competing models using both the LLTM and LLTM-E2S. In this way the simulation is a $2 \times 2$ (holdout validation type by method) design. 
The simulation replications track which model is selected and the estimated prediction errors, 
	$\mathrm{dev}(y^{(\mathrm{e})} | \hat \omega_{m}(y^{(\mathrm{t})}))$,
for both types of holdout validation and for all models.
In addition, model selection is also preformed using AIC, BIC, leave-one-out cross-validation, and likelihood ratio tests.

The item predictors for the data generating model are
\begin{equation} 
	x_i'\beta \equiv \beta_1 x_{1i} + \beta_2 x_{2i} + \beta_3 x_{3i} + \beta_4 x_{4i} + 
		\beta_5 x_{2i} x_{3i}
,\end{equation}
which includes an intercept ($x_{1i} = 1$), a continuous covariate ($x_{2i}$), two indicator variables ($x_{3i}$ and $x_{4i}$), and an interaction ($x_{2i} x_{3i}$). The coefficients are 
$\beta = \{-.5, 1, .5, .5, -.5\}$. Three competing models are subjected to the various model selection schemes. Model~1 omits the interaction:
\begin{equation} 
	x_i'\beta \equiv \beta_1 x_{1i} + \beta_2 x_{2i} + \beta_3 x_{3i} + \beta_4 x_{4i}
.\end{equation}
Model~2 matches the data generating model.
Model~3 includes an extra, unwarranted interaction:
\begin{equation} 
	x_i'\beta \equiv \beta_1 x_{1i} + \beta_2 x_{2i} + \beta_3 x_{3i} + \beta_4 x_{4i} + 
		\beta_5 x_{2i} x_{3i} + \beta_6 x_{2i} x_{4i}
.\end{equation}
Using the two stage method with Model~2 matches the data generating model, while using the LLTM with Model~2 only approximates it because the LLTM does not include item residuals.

Two factors are varied across simulation conditions: the standard deviation of the item residuals $\tau$ and the number of items $I$. In one set of conditions, 
$\tau \in \{.2, .5, 1\}$ while $I$ is fixed at 32, 
and in another set 
$I \in \{16, 32, 64\}$ while $\tau$ is fixed at .5.
Due to overlap, the design has a total of five conditions. Table~\ref{tab:2-X} provides the item covariate matrix when $I=32$. When $I \ne 32$, $x_{2i}$ takes a different set of values with the same range and the fully crossed design of $x_{2i}$, $x_{3i}$, and $x_{4i}$ is maintained. Due to the design, the item covariates are uncorrelated. In all simulation conditions, the multiple datasets are generated with 500 persons and with ability standard deviation $\sigma = 1$.

\begin{table}
	\label{tab:2-X}
	\centering
	\input{figs/2-table-X.tex}
	\caption{Item covariate matrix ($X$) for simulated datasets with $I = 32$ items. When $I \ne 32$, $x_2$ is a modified set of similarly blocked equidistant numbers between zero and one.}
\end{table}

A key feature of the generated datasets (and data of this type more generally) is the
extent to which the item covariates account for the item difficulties. Let 
$\upsilon^2 = \mathrm{var}(X\beta)$ 
represent the variance of the structural part of item difficulty. Because of the item design, $\upsilon^2 = .39$ for all simulated datasets, even if they have differing numbers of items. The total item variance is 
$\upsilon^2 + \tau^2$. Then
\begin{equation} 
	R^2 = \frac{\upsilon^2}{\upsilon^2 + \tau^2}
\end{equation}
represents the proportion of item variance accounted for by the item predictors. Figure~\ref{fig:2-rsq-vs-tau} displays $R^2$ as a function of $\tau$ with $\upsilon^2$ fixed to this value. The points marked indicate the generating values of $\tau$ and correspond approximately to conditions in which 90\%, 60\%, and 30\% of item variance is accounted for.

\begin{figure}[tbp]
	\label{fig:2-rsq-vs-tau}
	\centering
	\includegraphics{figs/2-rsq-vs-tau}
	\caption{Proportion of explained item variance ($R^2$) plotted against the residual item standard deviation ($\tau$). The dots represent chosen values for $\tau$ in the simulations, which correspond approximately to $R^2 = .9$, .6, and .3.}
\end{figure}


\subsection{Parameter recovery}

Parameter recovery is investigated for both the LLTM and LLTM-E2S. Results for the LLTM-E2S are of interest as confirmation that the method works, while results for the LLTM are of interest because the misspecification of the LLTM may lead to bias. 
Bias in parameter estimates is determined from the difference between the estimated and generating parameters across simulation replications. 
For this purpose, I focus on Model~2 because it matches the correct model, and I use estimation results $\hat \omega(y^{(t)})$. 
The results are based on 500 replications per condition. 
Figure~\ref{fig:2-bias} presents estimates of bias (the mean difference) with 95\% confidence intervals ($\pm 1.96 \frac{\mathrm{sd}}{\sqrt{500}}$) separately for the LLTM and LLTM-E2S. 

For the LLTM, there is evidence of attenuation in $\hat \beta$ in most of the simulation conditions, though with the exception of the $\tau=1$ condition, the effect is small.
(The generating values of $\beta_1$ and $\beta_5$ are negative and overestimated, while the generating values for the others are positive and underestimated.)
With the absence of item residuals, the LLTM is like a population-average model in regards to the items, which is known to exhibit attenuated coefficients for the logistic case \parencite{ritz2004equivalence}.
$\hat \sigma$ also exhibits a downward bias that depends on $\tau$.

For the LLTM-E2S, there is no systematic evidence for bias in $\hat \beta$, though there is a downward bias in $\hat \tau$ that is mitigated with smaller generating values of $\tau$ and larger numbers of items. The bias in $\hat \tau$ may be alleviated by using restricted maximum likelihood estimation, but for simplicity and speed maximum likelihood estimation is used in this simulation.
%The LLTM does not include estimation of $\tau$, and the LLTM-E2S does not include estimation of $\sigma$ in the second stage.

\begin{figure}
	\centering
	\includegraphics{figs/2-bias}
	\caption{Mean bias (parameter estimates minus generating values) with 95\% confidence intervals for Model~2. Results are for 500 simulation replications per condition. At the top are results for conditions in which the residual item standard deviation ($\tau$) is varied while the number of items is held at 32, and at the bottom are conditions in which the number of items is varied while $\tau$ is held at .5. The LLTM does not include estimation of $\tau$, and the LLTM-E2S does not include estimation of $\sigma$ in the second stage.}
	\label{fig:2-bias}
\end{figure}


The two approaches provide very different standard error estimates. To illustrate, I focus on $\beta_6$ for Model~3. Because $\beta_6 = 0$ in data generation,
$\frac{\hat \beta_6}{\mathrm{se}(\hat \beta_6)}$ 
should asymptotically follow a standard normal distribution across simulation iterations if the standard errors are correct. 
Figure~\ref{fig:2-qqplot} presents Q-Q plots of the observed 
$\frac{\hat \beta_6}{\mathrm{se}(\hat \beta_6)}$ 
against the quantiles of the standard normal distribution. In all conditions, 
$\frac{\hat \beta_6}{\mathrm{se}(\hat \beta_6)}$ 
for the LLTM deviate greatly from the expected results from a standard normal distribution, indicating that the estimated standard errors are too small.
The LLTM-E2S fairs much better, excepting the condition with only $I=16$ items which shows somewhat longer than expected tails for the standard errors.

Clearly the LLTM yields inappropriate standard errors. If $\tau = 0$, it may be that the LLTM would yield approximately correct standard errors, but this is an improbable scenario. The problem is a result of the omission of item residuals from the model. As an aside, a general rule of using the LLTM is to conduct a likelihood ratio test against the Rasch model before interpreting results. Doing so may guard against inappropriate standard errors, as the standard errors may be reasonable if the LLTM is an approximately correct model. However, in every replication of this simulation the likelihood ratio test rejected Model~2 when compared to the Rasch model, which includes replications in which the item covariates account for about 90\% of item variance.

\begin{figure}
	\centering
	\includegraphics{figs/2-qqplot}
	\caption{Q-Q plots for the observed $\frac{\hat \beta_6}{\mathrm{se}(\hat \beta_6)}$ for Model~3 across simulation iterations versus standard normal quantiles. On the left are results for simulation replications in which the residual item standard deviation ($\tau$) varies with the number of items held at 32, and on the right are results for replications in which the number of items vary with $\tau$ held at .5. Results are shown separately for the LLTM and LLTM-E2S. Because $\beta_6 = 0$ in data generation, $\frac{\hat \beta_6}{\mathrm{se}(\hat \beta_6)}$ should follow a standard normal distribution across simulation iterations if the standard errors are correct. The lines have an intercept of zero and slope of one, indicating where the points should lay if $\frac{\hat \beta_6}{\mathrm{se}(\hat \beta_6)}$ follows a standard normal distribution.}
	\label{fig:2-qqplot}
\end{figure}


\subsection{Model selection}

As described in the previous section, when $\tau$ is low the item covariates are strong predictors, so $\tau = .2$ is a (relatively) high information condition. Conversely, $\tau = 1$ is (relatively) low information. 
Using prediction as a basis for model selection, it is expected that the true model should be selected in conditions of high information, given that the true model will yield that best predictions asymptotically.
However, with low information some parameters for the true model may be poorly estimated, or in other words have high variance, and so a simpler model with more stable estimates may provide more accurate predictions.
Either case is correct behavior for holdout validation, cross-validation, and AIC, which all seek to identify the most predictive model rather than a true model.

% HV is similar for LLTM and LLTM-E2S. HV with new items is better than HV with same items. Performance of HV.
Figure~\ref{fig:2-selection-tau} provides the model selection results for the simulation conditions in which $\tau$ is varied, and Figure~\ref{fig:2-selection-nitems} shows the same for conditions in which the number of items is varied. Holdout validation using new items performs similarly between the LLTM (left side of both figures) and the LLTM-E2S (right side). It also adheres to the anticipated results; Model~2 is selected the majority of times in high information conditions, but Model~1 is selected with increasing frequency as the amount of information decreases ($\tau$ increases or $I$ decreases). Holdout validation using the same items also behaves similarly between the LLTM and LLTM-E2S, but the anticipated result is clearly contradicted. Model~3 is chosen the majority of times in all simulation conditions. This result is not surprising; idiosyncrasies that arise from a particular set of item residuals in the training subset are repeated again in the validation subset, and so the two subsets are very similar. In this way an inappropriately complex model is provided an opportunity to capitalize on chance. Clearly, holdout validation for item prediction must feature datasets with differing sets of items in order to be effective.

\begin{figure}
	\centering
	\includegraphics{figs/2-selection-tau}
	\caption{Percentages of times each model was chosen by the various selection criteria for simulation replications in which the residual item standard deviation ($\tau$) is varied while the number of items is held at 32. Results are shown separately for the LLTM and LLTM-E2S. The criteria include the likelihood ratio test (``LR test''), AIC, BIC, holdout validation using the same set of items (``HV same items''), holdout validation with a new set of items (``HV new items''), and leave-one-cluster-out cross-validation (``LOCO-CV''). The last criterion was applied only to the LLTM-E2S and involves leaving one item out at a time.}	
	\label{fig:2-selection-tau}
\end{figure}

\begin{figure}
	\centering
	\includegraphics{figs/2-selection-nitems}
	\caption{Percentages of times each model was chosen by the various selection criteria for simulation replications in which the number of items is varied while the residual item standard deviation ($\tau$) is held at .5. Results are shown separately for the LLTM and LLTM-E2S. The criteria include the likelihood ratio test (``LR test''), AIC, BIC, holdout validation using the same set of items (``HV same items''), holdout validation with a new set of items (``HV new items''), and leave-one-cluster-out cross-validation (``LOCO-CV''). The last criterion was applied only to the LLTM-E2S and involves leaving one item out at a time.}
	\label{fig:2-selection-nitems}
\end{figure}

% For LLTM, AIC corresponds to HV with same items. Also, LR and BIC.
Focusing now on the LLTM, AIC performs similarly to holdout validation with the same items in terms of model selection (Figures~\ref{fig:2-selection-tau} and \ref{fig:2-selection-nitems} again) and quite differently from holdout validation with new items. This demonstrates that using AIC with the deviance from the LLTM corresponds to an inference involving the same set of items. The likelihood ratio test is more conservative, that is, tends to prefer simpler models, when compared to AIC. Assuming an alpha level .05, a model would have to have a deviance 3.8 lower than a competing model with one fewer parameter in order to reject the simpler model, compared to a difference in deviance of 2 for AIC. BIC is more conservative still with a penalty of 9.7 per parameter for the case of 32 items and 500 persons. The relative conservatism of the likelihood ratio test and BIC are noticeable in the two figures. However, AIC, BIC, and the likelihood ratio test all bear resemblance to holdout validation with the same items and too frequently select too complex a model.

% For LLTM-E2S, AIC corresponds to HV with new items. Also, LR and BIC.
In contrast, AIC paired with the LLTM-E2S performs similarly to holdout validation with new items in terms of model selection (Figures~\ref{fig:2-selection-tau} and \ref{fig:2-selection-nitems} again) and is generally more apt to select Model~2 than holdout validation with new items. Again, the likelihood ratio test and BIC are more conservative than AIC for the LLTM-E2S, with the LR test implying a penalty of 3.8 again (assuming the models differ by one parameter) and BIC implying a softer than before penalty of 3.5 (assuming 32 items). The relative harshness of the penalties are reflected in the selection results presented in the figures. Unlike for the LLTM, AIC, BIC, and the likelihood ratio test appear more like holdout validation with new items, and the three perform as expected in model selection.

% LOCO-CV.
LOCO-CV was performed using the LLTM-E2S. LOCO-CV could also be performed with the LLTM, but it is not included in the simulation study due to the time requirements. LOCO-CV performed similarly to AIC, as would be expected, though it was more conservative. This is at least partly due to LOCO-CV relying on one less observation than AIC when fitting models. In fact, when $I = 16$ (Figure~\ref{fig:2-selection-nitems}) LOCO-CV exhibits strong conservatism relative to AIC, but when $I=64$ the two methods behave quite similarly.

% Selecting true model is not really the criteria.
It is important to note that judging the selection strategies simply by how often they select the a ``true'' model is somewhat inappropriate for holdout validation, cross-validation, and AIC. The reason is that these methods are designed to select the model that best predicts new data, and the true model is only guaranteed to be the best predictor asymptotically (as sample size approaches infinity). An overly simple model may often yield better predictions in finite data, especially when some aspects of the true model are poorly estimated. This is referred to as the bias variance trade off. On the other hand, selecting an overly complex model should be a chance occurrence. For these reasons, the selection results do not (directly) indicate which approaches yield the best predictions. 

% Conditional versus expected prediction error.
For the LLTM-E2S, holdout validation with new items was less effective than AIC or LOCO-CV at selecting the generating model, but these selection strategies differ in an important regard; holdout validation selects a model based on the conditional test error while AIC and LOCO-CV select a model based on the expected test error. Holdout validation indicates the prediction error of a particular fitted model in a new dataset. 
% which may be a highly variable result depending the particular datasets used. 
AIC and LOCO-CV, on the other hand, estimate how well a model will predict new data in expectation, over possible training datasets. In this way, AIC and LOCO-CV do not directly inform on the prediction error of particular fitted models. For these reasons, holdout validation may be preferred when the purpose is prediction per se, while AIC and LOCO-CV may be preferred for selecting a preferred model for explanation.

%Figure~\ref{fig:2-selection-tau} provides the model selection results for the simulation conditions in which $\tau$ is varied. Considering first the LLTM method, the likelihood ratio test, AIC, and BIC tend to select Model~2 when $\tau = .2$; BIC is particularly successful in this regard while AIC is not. However, all three show a strong inclination towards selecting the more complicated model when $\tau = 1$. Holdout validation with the same items exhibits the same tendencies, and selection results for it are similar to those for AIC. In summation, these selection approaches with the LLTM do not adhere to the expectation that with less information a simpler model should be preferred, with the important exception of holdout validation with new items. \emph{((Explain why: item residuals better fit by model 3 because more parameters and these residuals are constant over replications.))}
%
%Selection results for the LLTM-E2S are also provided in Figure~\ref{fig:2-selection-tau}. All three of the likelihood ratio test, AIC, and BIC strongly tend to select Model~2 when $\tau = .2$ and Model~1 when $\tau = 1$, conforming to expectation. Further, results for holdout validation with new items resembles those for AIC and leave-one-out cross-validation, though holdout validation with new items does yield notably more diffuse selection. Leave-one-out cross-validation appears to function like AIC with a stronger tendency towards simpler models.
%
%Figure~\ref{fig:2-selection-nitems} shows selection results for the simulation conditions in which the number of items is varied. With the LLTM, all the selection criteria, except for holdout validation with new items, tend to select Model~3, and the results do not very much by $I$. As expected, results for holdout validation with the same items resembles those for AIC. With the two stage method, all the selection criteria, except for holdout validation with the same items, tend to select Model~2 in the high information condition ($I=64$) and Model~1 in the low information condition ($I=16$). As before, results for holdout validation with new items loosely resembles those for AIC, and leave-on-out cross-validation exhibits a greater tendency towards simpler models compared to AIC.


\subsection{Implied penalties associated with holdout validation}

AIC features a penalty to the insample deviance that is a function of the number of model parameters. Let this be $d_\mathrm{AIC}$. In this context, $d_\mathrm{AIC}$ is an estimate of
\begin{equation}
	d_{\mathrm{HV}} = \mathrm{E}_{y^{(\mathrm{e})}} \mathrm{E}_{y^{(\mathrm{t})}} 
	[
		\mathrm{dev}(y^{(\mathrm{e})} | \hat \omega_m(y^{(\mathrm{t})})) -
		\mathrm{dev}(y^{(\mathrm{e})} | \hat \omega_m(y^{(\mathrm{e})}))
	]
,\end{equation}
which is the expected difference between the holdout validation deviance and the insample deviance. Then $\hat d_{\mathrm{HV}}$, an empirically determined estimate for the appropriate penalty, may be obtained from the simulation as
\begin{equation}
	\hat d_{\mathrm{HV}} = \frac{1}{S} \sum_{s=1}^{S} 
	\left [
		\mathrm{dev}(y^{(\mathrm{e})}_s | \hat \omega_m(y^{(\mathrm{t})}_s)) -
		\mathrm{dev}(y^{(\mathrm{e})}_s | \hat \omega_m(y^{(\mathrm{e})}_s))
	\right ]
,\end{equation}
where $s$ indexes the $S$ simulation conditions.
Because $\hat d_{\mathrm{HV}}$ will vary by model, simulation condition, and method (LLTM versus LLTM-E2S), it is calculated separately for each combination. These are presented in the top four panels of Figure~\ref{fig:2-penalty}. 
% Similar estimates, $\hat d_{\mathrm{LOO}}$, may are calculated for leave-one-out cross-validation, shown in the bottom two panels.

For the LLTM, the estimated penalties ($\hat d_{\mathrm{HV}}$) are much greater than those associated with AIC ($d_\mathrm{AIC}$), as shown in Figure~\ref{fig:2-penalty}. \emph{((Add description of SD plots.))} Model~2 has one parameter more than Model~1, and likewise Model~3 has one more parameter than Model~2, and so AIC implies a \emph{relative} penalty of 2 between the models. The differences for those model pairs in $\hat d_{\mathrm{HV}}$ are about 50 or more for most simulation conditions, the exception being when $\tau$ is small. \emph{((Why?))}

For the LLTM-E2S, $d_\mathrm{AIC}$ remains lower than $\hat d_{\mathrm{HV}}$, though the discrepancy is much less. While $d_\mathrm{AIC}$ does not vary with the number of items (sample size), $\hat d_{\mathrm{HV}}$ decreases as the number of items increases. With $I=64$, the relative differences in $\hat d_{\mathrm{HV}}$ between models is similar to that for $d_\mathrm{AIC}$, which is related to the large sample assumption of AIC. In addition, for Models~2 and 3 $\hat d_{\mathrm{HV}}$ decreases as $\tau$ increases, while it does not vary much for Model~1. Even though $d_\mathrm{AIC}$ does not approximate $\hat d_{\mathrm{HV}}$ particularly well in the LLTM-E2S, AIC does a better job of selecting the correct model than holdout validation with new items. This happens partly because $d_{\mathrm{HV}}$ has high variance, while $d_\mathrm{AIC}$ has zero variance.

%Leave-one-out cross-validation bears a similar relationship to AIC (Figure~\ref{fig:2-penalty}). $\hat d_{\mathrm{LOO}}$ is greater than $d_\mathrm{AIC}$. This trend is affected strongly by the number of items but not substantially by $\tau$. Leave-one-out cross-validation out performs holdout validation probably by virtue of having lower variance.

\begin{figure}
	\centering
	\includegraphics{figs/2-penalty}
	\caption{Appropriate penalties for in-sample deviance as estimated by holdout validation with new items ($\hat d_{\mathrm{HV}}$). The left column shows estimated penalties for each model, while the right column shows the estimated penalty $\pm 1$ one standard deviation for Model~2 only. The top set of plots shows simulation replications in which the residual item standard deviation ($\tau$) varies while the number of items is held at 32, and the bottom set shows replications in which the number of items vary while $\tau$ is held at .5.
	% or leave-one-out cross-validation ($\hat d_{\mathrm{LOO}}$, bottom row) across simulation conditions. 
	The dashed lines show the penalties associated with AIC ($d_\mathrm{AIC}$).}
	\label{fig:2-penalty}
\end{figure}


\subsection{Prediction}

In holdout validation, a model $m$ is evaluated by the holdout deviance,
$\mathrm{dev}(y^{(\mathrm{e})} | \hat \omega_{m^*}(y^{(\mathrm{t})}))$,
which is the deviance of the trained model when applied to the evaluation subset. For the purpose of comparing simulation results, it is useful to instead consider what I will call the relative holdout deviance,
\begin{equation}
	\mathrm{RHD}_m = \mathrm{dev}(y^{(\mathrm{e})} | \hat \omega_{m}(y^{(\mathrm{t})})) -
	                 \frac{1}{M} \sum_{q=1}^M 
	                   \mathrm{dev}(y^{(\mathrm{e})} | \hat \omega_{q}(y^{(\mathrm{t})}))
\end{equation}
which is the holdout deviance for model $m$ minus the average holdout deviance for all $M$ models under consideration. The deviance for the same model across simulation replications is highly variable, and the RHD stabilizes this variation while maintaining the relative differences in deviance between competing models.

To assess the differences in prediction accuracy between different selection criteria, I define $\mathrm{RHD}_m^*$ as the relative holdout deviance for the model chosen by a given selection criteria. In this way the usefulness of differing selection criteria may be evaluated by the values they obtain for $\mathrm{RHD}_m^*$. Though a researcher would not ordinarily have an evaluation subset when employing single-dataset methods like cross-validation or information criteria, the simulation study does have an evaluation subset that may be used to calculated $\mathrm{RHD}_m^*$ even for these selection approaches. Figure~\ref{fig:2-prediction} presents this result for the simulation. Across the board, holdout validation with new items performs better than the other methods, though results are sometimes close with the LLTM. 

I focus on the results for the LLTM-E2S. It should not be surprising that predictions from holdout validation with new items outperforms those from holdout validation with the same items. However, holdout validation with new items also provided better predictions than AIC and LOCO-CV in all simulation conditions, even though these were found to select the true model more often for some conditions (as in Figures~\ref{fig:2-selection-tau} and \ref{fig:2-selection-nitems}). All three of these selection methods should reliably select the most predictive model (not necessarily the true model) as sample size becomes large. In the bottom right panel of Figure~\ref{fig:2-prediction}, it may be seen that both AIC and LOCO-CV approach the performance of holdout validation with new items as the number of items increases.
%The reason for this apparent contradiction is that predictions for new data from holdout validation are conditional on the estimated parameters

\begin{figure}
	\centering
	\includegraphics{figs/2-prediction}
	\caption{The mean of the relative holdout deviances ($\mathrm{RHD}_m$) across simulation replications for the selected model by selection criterion. The left column shows simulation replications in which the residual item standard deviation ($\tau$) varies while the number of items is held at 32, and the right column shows replications in which the number of items vary while $\tau$ is held at .5. The criteria include the likelihood ratio test (``LR test''), AIC, BIC, holdout validation using the same set of items (``HV same items''), holdout validation with a new set of items (``HV new items''), and leave-one-cluster-out cross-validation (``LOCO-CV''). The last criterion was applied only to the LLTM-E2S and involves leaving one item out at a time. Results are shown separately for the LLTM and LLTM-E2S.
	}
	\label{fig:2-prediction}
\end{figure}


%Given that holdout validation with new items tends to select the correct model more often than holdout validation with the same items, we may be interested in comparing how well the two perform in predicting the difficulty of new items. 
%For this purpose, the difference
%	$\mathrm{dev}(y^{(\mathrm{t})} | \hat \omega_{m^*}(y^{(\mathrm{n})})) -
%	\mathrm{dev}(y^{(\mathrm{t})} | \hat \omega_{m^*}(y^{(\mathrm{s})}))$
%	for the two stage method is calculated and then summarized across simulation replications in Figure~\ref{fig:2-hvpred-twostage}. 
%(The same may be considered for the LLTM, but the results are similar and not shown.) In other words, the out-of-sample deviance for the fitted model chosen by same item holdout validation is subtracted from the that chosen by new item holdout validation. The figure shows that for all simulation conditions, the difference is concentrated near zero and has a high variance. Paradoxically, hold out validation with new items does demonstrate a tendency towards better prediction despite its greater success in selecting the generating model.
%


\section{Discussion}

\begin{itemize}
	\item \emph{Selection conditional on estimated model.} Holdout validation differs from AIC and LOCO-CV in that selection is based on the conditional prediction error rather than the expected prediction error (with the expectation taken over possible training datasets). Holdout validation with new items was more effective in selecting a model for prediction, even though it is not always more apt to choose the true model. However, the true model is not necessarily the best predictor in finite samples. For the purpose of prediction, the conditional prediction error seems to be a better benchmark for model selection. For the purpose of explanation, the expected prediction error may be more effective. Of course, the amount of available data may drive the choice a researcher makes in practice.
	\item \emph{Model averaging}. This chapter has focused on the selection of a single model based on predictive utility. If the goal is to obtain the best possible prediction, model averaging may be used instead. Model averaging involves aggregating the predictions from the set of candidates models, weighting the competing predictions according to the score function. The insights from the simulation results apply to this goal as well, and it is expected that model averaging using the LLTM-E2S would outperform the same using the LLTM.
\end{itemize}



% Old content follows
\newpage


%
%This chapter investigates how well cross-validation performs within the frequentist framework using maximum likelihood estimation, specifically for the purpose of selecting among models with differing sets of item covariates. With this estimation method, the item residuals in Equation~\ref{eq:eirm} are commonly omitted from the model to make calculation of the marginal likelihood tractable. This chapter shows that AIC \parencite{akaike1974new} corresponds to cross-validation over persons and then goes on to shows that AIC and holdout cross-validation over persons both perform poorly in this instance.
%
%\section{Model and methods}
%
%Fitting the doubly explanatory model in Equation~\ref{eq:eirm} using marginal maximum likelihood is infeasible. Instead, the analysis models will be of the form:
%\begin{equation} \label{eq:latent-reg-lltm}
%	\Pr(y_{ip} = 1 | w_p, x_i, \zeta_p) =
%	\mathrm{logit}^{-1} \left [ (w_p'\gamma + \zeta_p) - x_i'\beta \right ]
%\end{equation}
%\begin{equation}
%	\zeta_p \sim \mathrm{N}(0, \sigma^2)
%\end{equation}
%Here the item residual for $\epsilon_i$ is omitted. This is a variation on the linear logistic test model, which is used to study the factors associated with the difficulty of items. The log-likelihood is
%\begin{equation}
%	L(y ; \hat \sigma, \hat \gamma, \hat \beta) = 
%	\sum_{p=1}^P \log
%	\int \prod_{i=1}^I
%		p(y_{ip} | \hat \gamma, \hat \beta, \zeta_p)
%		p(\zeta_p | \hat \sigma)
%	~d \zeta_p
%\end{equation}
%which is readily estimated as a generalized linear mixed model \parencite{Rijmen2003}.
%
%Let $\tilde y$ represent response data from a holdout dataset, which may involve new persons or new items.
%Then using the log-likelihood as the score function, $L(\tilde y ; \hat \sigma, \hat \gamma, \hat \beta)$ represents the fit of a model with parameter estimates from the original data evaluated in the holdout data. I will use the notation $\mathrm{CV}^{(P)}$ to represent $L(\tilde y ; \hat \sigma, \hat \gamma, \hat \beta)$ when $\tilde y$ arises from new persons and $\mathrm{CV}^{(I)}$ when it arises from new items. When a holdout dataset is available, $\mathrm{CV}^{(P)}$ or $\mathrm{CV}^{(I)}$ may be used to select among competing models.
%
%AIC \parencite{akaike1974new} is an information criteria that estimates the expected log-likelihood of the fitted model when evaluated on new data. In the context of the model in Equation~\ref{eq:latent-reg-lltm}, it has the form
%\begin{equation} 
%	\mathrm{AIC} = -2 L(y ; \hat \sigma, \hat \gamma, \hat \beta) - 2k
%,\end{equation}
%where $k$ is the number of model parameters. Because the likelihood in Equation~\ref{eq:lltm-likelihood} is marginal over persons, AIC will estimate the expectation of $-2 \times \mathrm{CV}^{(P)}$.


\section{Notes}

\subsection{Automatic item generation}

	\begin{itemize}
		\item \textcite{freund2008explaining}: AIG with figural matrix items.
		\item \textcite{holling2009automatic}: AIG with probability word problems.
		\item \textcite{cho2014additive}: ``In an item generation context, modeling the items can have two aims. First, given a dataset based on generated items, one may want to estimate the person traits (e.g., abilities) relying on a given statistical item model. Second, one may want to have an idea of the difficulty and discrimination of a generated item before it is generated, for example because one wants it to be optimally informative in an adaptive procedure. With such a purpose in mind, the item model needs to have a reasonable predictive value.''
		\item \textcite{arendasy2005automatic}: AIG with more figure rotation type things.
		\item \textcite{arendasy2007using}: AIG with algebra word problems
		\item \textcite{arendasy2010evaluating}: AIG with mental rotation problems.
		\item \textcite{arendasy2011using}: AIG with word fluency items.
	\end{itemize}

\subsection{About AIC}

The description below is taken from \textcite{burnham2003model}. The notation here is unrelated to that in the rest of the chapter.

Kullback-Leibler information (or distance) is
\begin{equation}
	I(f,g) = \int f(x) \log \left ( \frac{f(x)}{g(x|\theta)} \right ) ~dx
\end{equation}
where $f$ is the true distribution function, $g$ is the model distribution function, $x$ is a dataset, and $\theta$ is the model parameters. This is the ``information lost when $g$ is used to approximate $f$.'' The above is for continuous distributions, and the authors provide a separate equation for discrete distributions. Further, $I(f,g)$ may be rewritten as
\begin{equation}
	I(f,g) = \int f(x) \log f(x) ~dx - \int f(x) \log g(x|\theta) ~dx
\end{equation}
and then
\begin{equation}
	I(f,g) = \mathrm{E}_f[\log f(x)] - \mathrm{E}_f[\log g(x|\theta)]
\end{equation}
and then
\begin{equation}
	I(f,g) = C - \mathrm{E}_f[\log g(x|\theta)]
\end{equation}
where $C$ is a constant, usually unknown or ignored, that does not depend on choice of model $g$. In this way, $\mathrm{E}_f[\log g(x|\theta)]$ is \emph{relative} distance.

The target quantity for AIC is the ``relative expected K-L distance''
\begin{equation}
	\mathrm{E}_y \mathrm{E}_x [\log( g(x | \hat \theta(y)) )]
\end{equation}
where $x$ and $y$ are independent datasets (not independent and dependent variables) drawn from an unknown true distribution $f$ and $\hat \theta(y)$ are MLE parameter estimates resulting from the fit of the model to $y$. 
(\cite{Kuha2004}, page 206, explains that the double expectation results from ``assuming the MLE is based on a separate, independent sample of data...'')
Both expectations are effectively taken with respect to $f$ (page 60). $\hat \theta(y)$ differs from the psuedo-true parameter values $\theta_0$ for model $g$, and $g(x | \hat \theta_0)$ minimizes the K-L distance (in comparison to other values for $\theta$). The ``key result'' is
\begin{equation}
	\mathrm{E}_y \mathrm{E}_x [\log( g(x | \hat \theta(y)) )] \approx
	\log( L(\hat \theta | data ) ) - K
\end{equation}
where $L(\hat \theta| data)$ is the log-likelihood of the fitted model and $K$ is the ``number of estimable parameters''. $K$ results from the use of $\hat \theta(y)$ instead of $\theta_0$, or in other words, due to the uncertainty in the estimated parameters. AIC is
\begin{equation}
	\mathrm{AIC} = -2 \log(  L(\hat \theta|y)  ) + 2K
.\end{equation}
AIC is used to select the model that minimizes the ``expected value of this (conceptually) estimated K-L information'' (page 363):
\begin{equation}
	\mathrm{E}_y \left [ I(f, g(\cdot | \hat \theta (y) )) \right ]
.\end{equation}

My interpretations: In the previous paragraph, there is an abrupt switch from 
	$\log( L(\hat \theta | data ) )$  
to
	$\log( L(\hat \theta | y ) )$, which are presumably the same quantity.
This is how it goes in the chapter (page 61), and the switch is confusing and unexplained. 

Also, because the expectations above are taken over data $y$ in 
	$\mathrm{E}_y \mathrm{E}_x [\log( g(x | \hat \theta(y)) )]$, 
I believe AIC is not conditional on the parameter estimates from the available data. Adopting a cross-validation viewpoint, I would say that
	$\mathrm{E}_y \mathrm{E}_x [\log( g(x | \hat \theta(y)) )]$
represents the expected cross-validation log-likelihood, where a model is trained on $y$ and evaluated on $x$. Further, AIC approximates this quantity (times $-2$).

\textcite{stone1977asymptotic} is credited (for example, by \cite{Hastie2009}) with showing that AIC is asymptotically equivalent to leave-one-out cross-validation. The paper it self is short but difficult to follow; it's mainly a mathematical proof lacking discussion.

\textcite{Kuha2004} describes the ``asymptotic efficiency'' of AIC: ``the expected mean squared error of predictions from models selected by it is the smallest possible in large samples.'' I interpret this to mean that AIC selects the best available approximation to the true model. BIC does not share this characteristic. BIC is consistent: ``the probability of selecting the true model from a set of candidates tends to 1 as $n$ increases if the true model is one of the models under consideration and the true parameter value $\theta*$ remains fixed'' (see references in \cite{Kuha2004}, page 217).

\textcite{Kuha2004} motivates AIC in terms of the K-L distance and connects this to cross-validation. \textcite{burnham2003model} do not connect AIC to cross-validation, while \textcite{Hastie2009} present AIC as a correction to the in-sample likelihood due to overfitting.

\textcite{Kuha2004} (page 208) lists two requirements for AIC to be a good estimate. First, the sample sized is assumed to be large. Corrections for small samples exist, but must be derived for every model type. Second, the models are true (or that the smaller model in a nested comparison is true). This is because the AIC penalty (the 2) is a property of the true distribution. For untrue models, AIC is biased but has zero variance. ``Other, less biased, estimates for the same quanityt exist, but their variances must also be larger. Thus, the constant estimate used in $\mathrm{AIC}_e$, besides being trivial to calculate, is likely to have a lower mean quared error thant alternatives in many models in which its assumptions are at least roughly satisfied.

\textcite{fang2011asymptotic} shows, in the context of linear mixed effects models, that ``the marginal Akaike information criterion is asymptotically equivalent to the leave-one-cluster-out cross-validation and the conditional Akaike information criterion is asymptotically equivalent to the leave-one-observation-out cross-validation.''


\subsection{Cross-validation}

I should adopt the language ``holdout validation'' and drop ``holdout cross-validation'', as ``cross-validation'' seems to apply to $k$-fold, LOO, bootstrap CV, and the like. Nothing is ``crossed'' in holdout validation.

\textcite{Hastie2009} suggest that there are two possible goals: model selection (choosing the best predictor) and model assessment (estimating the prediction error). In a data-rich situation, they say that a three part validation scheme is ideal: models are estimated in the ``training'' set, chosen based on loss in the ``validation'' set, and the prediction error for the final model is estimated on the ``test'' set. Single-dataset approaches (IC and CV) approximate the validation step.

\emph{I think} that the error in the test dataset is an estimate of the conditional prediction error (conditional on the trained/fitted model), and that the error in the validation dataset is also a (potentially biased) estimate of the same. \emph{I think} that holdout validation is in this way different from single-dataset approaches like information criteria, $k$-fold CV, and leave-one-out CV. ``It does not seem possible to estimate conditional error effectively, given only the information in the same training set'' (\cite{Hastie2009}, page 220).

\textcite{Hastie2009} say that CV approximates the expected test error. They provide a brief and not very clear discussion of whether cross-validation estimates the ``conditional test error''. They do a brief simulation comparing $k$-fold CV ($k=10$) and leave-one-out CV and conclude that neither estimates the conditional test error well. They also conclude that both ``are approximately unbiased for expected [test] error, but the variation in test error for different training sets is quite substantial.'' (In other words, the estimates have high variance, I think.)

\textcite{arlot2010survey} mentions that the appeal of cross-validation methods is their universality, given that they are based on data splitting. They describe ``model selection for estimation'', which I think corresponds to estimating the expected error and choosing the model that minimizes the error. (The article relies heavily on stupid notation, so I may have to reread.) They mention efficiency and the oracle inequality (page 47). They alternative is ``model selection for identification'' (choosing the true model). The AIC-BIC dilemma (estimation versus identification) is mentioned with some probably useful references (page 48). Some papers on CV bias in the regression framework are mentioned (page 57).

\textcite{arlot2010survey} discuss the merits of CV versus ``penalized criteria'' (page 71). ``The strongest argument for CV is its quasi-universality: Provided data are i.i.d., CV yields good model selection performances in (almost) any framework. Nevertheless, universality has a price: Compared to procedures designed to be optimal in a specific framework (like AIC), the model selection performances of CV can be less accurate, while its computational cost is higher.'' Later: ``More generally, because of its versatility, CV should be prefered to any model selection procedure relying on assumptions which are likely to be wrong.''

